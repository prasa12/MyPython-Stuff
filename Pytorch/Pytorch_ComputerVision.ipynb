{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFDEBOAxZTDhxZrm5X1LpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4bcc2fe5fcb24e7783c2884ef8782148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32417f9086484c00983acffc5dad074d",
              "IPY_MODEL_170c57c7aaf84f38843e2a130a7d9551",
              "IPY_MODEL_387c5ce60a8341e0938dcdae5c6c0ae0"
            ],
            "layout": "IPY_MODEL_8f5d49bb81d8458fa459066c5bc60fab"
          }
        },
        "32417f9086484c00983acffc5dad074d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2126b7fa9324371920d7e29dbb78d87",
            "placeholder": "​",
            "style": "IPY_MODEL_ba4a2845f2614758a57dd36fe213810f",
            "value": "100%"
          }
        },
        "170c57c7aaf84f38843e2a130a7d9551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db75168f93b54e5ba2779d5d1bde5349",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3228e763532e4528bee92c55ebd06fc3",
            "value": 4
          }
        },
        "387c5ce60a8341e0938dcdae5c6c0ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92e13007e5f24f89af5b771bab06e264",
            "placeholder": "​",
            "style": "IPY_MODEL_ed4139fd8bc84e9ca895d47674bc1d46",
            "value": " 4/4 [00:37&lt;00:00,  9.27s/it]"
          }
        },
        "8f5d49bb81d8458fa459066c5bc60fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2126b7fa9324371920d7e29dbb78d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4a2845f2614758a57dd36fe213810f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db75168f93b54e5ba2779d5d1bde5349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3228e763532e4528bee92c55ebd06fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "92e13007e5f24f89af5b771bab06e264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4139fd8bc84e9ca895d47674bc1d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20b6cd4eb74f4f52a10af2f4c1f269ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03818ebefcf148c3833402c3867eb014",
              "IPY_MODEL_a03087d5b7dd4f76b600593862195364",
              "IPY_MODEL_3f14cd2e79914cafad1b73604bb7a12c"
            ],
            "layout": "IPY_MODEL_07f2202217f345ab8f862a3480f0ee27"
          }
        },
        "03818ebefcf148c3833402c3867eb014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea432312e27741b6b0a4f1c426b2a312",
            "placeholder": "​",
            "style": "IPY_MODEL_1fda17e0c65a400ab477da4ff7290605",
            "value": " 90%"
          }
        },
        "a03087d5b7dd4f76b600593862195364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_894ecad95bf44fa1ace25b84531bcf63",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0fbfcf3bb0b428fbb5c8325b289e807",
            "value": 9
          }
        },
        "3f14cd2e79914cafad1b73604bb7a12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98a3c87bcabd4ad4ab9002cc33f97cb9",
            "placeholder": "​",
            "style": "IPY_MODEL_e5a9555a8e6845ffbd67936b58c24f03",
            "value": " 9/10 [01:41&lt;00:11, 11.30s/it]"
          }
        },
        "07f2202217f345ab8f862a3480f0ee27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea432312e27741b6b0a4f1c426b2a312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fda17e0c65a400ab477da4ff7290605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "894ecad95bf44fa1ace25b84531bcf63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0fbfcf3bb0b428fbb5c8325b289e807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98a3c87bcabd4ad4ab9002cc33f97cb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a9555a8e6845ffbd67936b58c24f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasa12/MyPython-Stuff/blob/master/Pytorch/Pytorch_ComputerVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Computer vision\n",
        "\n",
        "## 0. Computer Vision libraries for Pytorch\n",
        "\n",
        "* [' torchvision'] - base domain library for Pytorch computer Vision\n",
        "* ' torchvision.datasets' - get datasets and data loading functions for computer vision for your own problems\n",
        "* 'torchvision.transforms' - functions for manipulating your vision data (images) to be suitable for use with ML model\n",
        "* ' torch.utils.data.DataLoader'- Creates a Python iterable over a dataset\n",
        "* ' torch.utils.data.Dataset' - Base dataset class for Pytorch"
      ],
      "metadata": {
        "id": "bwvCQxWiGvHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torch vision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check versions\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxtkLXs_BnbU",
        "outputId": "1734b5cc-dd9a-4442-efe1-6efb17364e7f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n",
            "0.23.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting datasets\n",
        "The dataset we will be using is Fashion MNIST"
      ],
      "metadata": {
        "id": "DXnS5KV-D5pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")"
      ],
      "metadata": {
        "id": "gHDamhIUD2jK",
        "outputId": "31ba8961-e662-42b5-d9f6-e49b107b1008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.8MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 175kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.32MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NyeounUFYU7",
        "outputId": "80349710-2fe9-4228-a465-a4288eac7c76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training example\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uCY7pgjFmAu",
        "outputId": "42b0742d-f809-47d4-b750-60d52a217651"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "           0.0157, 0.0000, 0.0000, 0.0118],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0471, 0.0392, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "           0.3020, 0.5098, 0.2824, 0.0588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "           0.5529, 0.3451, 0.6745, 0.2588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "           0.4824, 0.7686, 0.8980, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "           0.8745, 0.9608, 0.6784, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "           0.8627, 0.9529, 0.7922, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "           0.8863, 0.7725, 0.8196, 0.2039],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "           0.9608, 0.4667, 0.6549, 0.2196],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "           0.8510, 0.8196, 0.3608, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "           0.8549, 1.0000, 0.3020, 0.0000],\n",
              "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "           0.8784, 0.9569, 0.6235, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "           0.9137, 0.9333, 0.8431, 0.0000],\n",
              "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "           0.8627, 0.9098, 0.9647, 0.0000],\n",
              "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "           0.8706, 0.8941, 0.8824, 0.0000],\n",
              "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "           0.8745, 0.8784, 0.8980, 0.1137],\n",
              "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "           0.8627, 0.8667, 0.9020, 0.2627],\n",
              "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "           0.7098, 0.8039, 0.8078, 0.4510],\n",
              "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "           0.6549, 0.6941, 0.8235, 0.3608],\n",
              "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "           0.7529, 0.8471, 0.6667, 0.0000],\n",
              "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "           0.3882, 0.2275, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYzMYLxcGLpz",
        "outputId": "cada895e-3c41-4dce-e451-be356bf76428"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2llE3Wv3GP8W",
        "outputId": "a1862bfc-f1f5-49c9-c794-8ea4f54a9410"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'T-shirt/top': 0,\n",
              " 'Trouser': 1,\n",
              " 'Pullover': 2,\n",
              " 'Dress': 3,\n",
              " 'Coat': 4,\n",
              " 'Sandal': 5,\n",
              " 'Shirt': 6,\n",
              " 'Sneaker': 7,\n",
              " 'Bag': 8,\n",
              " 'Ankle boot': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW_ZhQiMGTWy",
        "outputId": "0c081298-f5ed-497a-b071-4f33050ba189"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 0, 0,  ..., 3, 0, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape, label\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB3Pn5eRGYFc",
        "outputId": "ee1880a3-a97c-460b-cc3a-97afa0151c43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Visualizing our data"
      ],
      "metadata": {
        "id": "ZtI8ugm7Ig26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image.squeeze())\n",
        "print(f'Label: {label}, Class: {class_names[label]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "AE49mGgfGb3W",
        "outputId": "be15be66-4138-4ea3-87e1-6e039732082f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 9, Class: Ankle boot\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap='gray')\n",
        "print(f'Label: {label}, Class: {class_names[label]}')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "LXpmaotGIvA8",
        "outputId": "2907d382-a266-454b-9811-e5489e8354d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 9, Class: Ankle boot\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(-0.5), np.float64(27.5), np.float64(27.5), np.float64(-0.5))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows*cols+1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    image, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[label])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "id": "QUKQktyWI-sO",
        "outputId": "757158e6-49d2-4df4-b8de-6a0d621ee8ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x900 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you thnk the items of clothing could be modelled with pure linear lined"
      ],
      "metadata": {
        "id": "ISMcYkPk7NHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "F04jAx6gJReD",
        "outputId": "5a3796d5-8515-4de7-ede4-30f249b08e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset FashionMNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " Dataset FashionMNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: ToTensor())"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare DataLoader\n",
        "\n",
        "Right now, our data is in the form of Pytorch Datasets\n",
        "\n",
        "DataLoader,turns our data into batches ( or mini batches)\n",
        "\n",
        "More specfically, we want to turn our data into batches\n",
        "\n",
        "Why would you do this?\n",
        "\n",
        "1. It is more computationally efficient, as in, your computing hardware may not be able to look (store in memory) at 60000 images in one bit. So we break it down to 32 images at a time ( batch size of 32)\n",
        "2. It gives our neural network more chances to update its gradients per epoch"
      ],
      "metadata": {
        "id": "Dkk4Q28q73gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data set into Data loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup batch size hyperparameter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader\n"
      ],
      "metadata": {
        "id": "gIRLsHHg7cVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aab6c4f-8a5b-445f-c14d-ab2064232f02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x787278273aa0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7872784089b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we have created\n",
        "print(f'Dataloader: {train_dataloader}\\nLength: {len(train_dataloader)}\\nType: {type(train_dataloader)}')\n",
        "print(f'Dataloader: {test_dataloader}\\nLength: {len(test_dataloader)}\\nType: {type(test_dataloader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8cLhzgJ9zJo",
        "outputId": "a06b184b-91cb-4621-abde-17c4c044dba0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloader: <torch.utils.data.dataloader.DataLoader object at 0x787278273aa0>\n",
            "Length: 1875\n",
            "Type: <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "Dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7872784089b0>\n",
            "Length: 313\n",
            "Type: <class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside train batch loader\n",
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "print(f'Image shape: {train_features_batch.shape}\\nLabel shape: {train_labels_batch.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGKFd2du-nED",
        "outputId": "5ee4b002-7a4f-4d47-c7b0-fe4ede7c3f00"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([32, 1, 28, 28])\n",
            "Label shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "\n",
        "random_image = train_features_batch[random_idx]\n",
        "random_label = train_labels_batch[random_idx]\n",
        "\n",
        "plt.imshow(random_image.squeeze(), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.title(class_names[random_label])\n",
        "print(f'Label: {random_label}, Class: {class_names[random_label]}')\n",
        "print(f'image shape: {random_image.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "Kmjwj-jO_ZQN",
        "outputId": "5669fd1d-2ac8-4f6c-b9f6-6a6eefcce341"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 8, Class: Bag\n",
            "image shape: torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0 : Build a baseline model\n",
        "\n",
        "A baseline model is a simple model you try and improve upon with subsequent models/experiments.\n",
        "\n",
        "In other words: start simply and complexity when necessary\n",
        "\n"
      ],
      "metadata": {
        "id": "UAtHQOwr_4Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "# Print out what happened\n",
        "print(f'Shape before flattening: {x.shape}\\nShape after flattening: {output.shape}')\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWBXgrbwDF3m",
        "outputId": "3a7dd508-c443-4389-f507-d9abd5b8d793"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([1, 28, 28])\n",
            "Shape after flattening: torch.Size([1, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hptWPDNpdZSx",
        "outputId": "134d39f3-ab31-4c3b-c11b-6a2bec92921e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0039, 0.0039, 0.0000, 0.0000, 0.0078, 0.0078, 0.0000, 0.0000, 0.0039,\n",
              "         0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 0.0000, 0.0000,\n",
              "         0.0078],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3725, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.3373, 0.3569, 0.2039, 0.4980, 0.4196,\n",
              "         0.4706, 0.3608, 0.3961, 0.4706, 0.4471, 1.0000, 0.4314, 0.3451, 0.0078,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.0824, 0.0706, 0.4588, 0.4118,\n",
              "         0.4980, 0.2588, 0.2235, 0.2588, 0.0824, 0.0510, 0.1922, 0.5137, 0.5765,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1333, 0.8000, 0.5608, 0.5255,\n",
              "         0.2431],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.9137, 0.9686, 0.5137, 0.4353,\n",
              "         0.6471],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0588, 0.3843, 0.6980, 0.0588, 0.2824,\n",
              "         0.1686],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.1333, 0.2078, 0.2157, 0.6745, 0.2941, 0.1059,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0039, 0.0000, 0.0078, 0.3333, 0.2980, 0.2941, 0.2039, 0.0314, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
              "         0.0039, 0.0000, 0.2196, 0.5020, 0.0157, 0.0706, 0.3451, 0.3216, 0.0588,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0157, 0.4863, 0.3843, 0.1804, 0.6235, 0.7882, 0.6000, 0.1569,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.2863, 0.4431, 0.4196, 0.5882, 0.5020, 0.1020, 0.2235, 0.0549,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0039, 0.4078, 0.4314, 0.7137, 0.1843, 0.2196, 0.4118, 0.3216, 0.0196,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.2549, 0.5647, 0.6275, 0.0824, 0.0000, 0.0000, 0.5098, 0.3333, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0039, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333,\n",
              "         0.5647, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.6510, 0.3059, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922, 0.7216,\n",
              "         0.4510, 0.0000, 0.0000, 0.0157, 0.0000, 0.0000, 0.6275, 0.2667, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0784, 0.0784, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6392, 0.3804,\n",
              "         0.0000, 0.0000, 0.0000, 0.0314, 0.0000, 0.0000, 0.6667, 0.1529, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0039, 0.0000, 0.0314, 0.2471, 0.2980, 0.1686, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5255, 0.5333, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0000, 0.6784, 0.0706, 0.0000,\n",
              "         0.0039],\n",
              "        [0.0039, 0.0039, 0.0000, 0.0000, 0.0706, 0.0941, 0.0000, 0.0196, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3451, 0.7137, 0.0275, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6588, 0.0039, 0.0000,\n",
              "         0.0039],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.1922, 0.1059, 0.1216, 0.2196,\n",
              "         0.0667, 0.0000, 0.0000, 0.0000, 0.3451, 0.6000, 0.1922, 0.0000, 0.0196,\n",
              "         0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.6471, 0.0000, 0.0000,\n",
              "         0.0039],\n",
              "        [0.0510, 0.0275, 0.0000, 0.0000, 0.0000, 0.3294, 0.3804, 0.4000, 0.4941,\n",
              "         0.3882, 0.0000, 0.0196, 0.5020, 0.6000, 0.2863, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.5451, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.3176, 0.5961, 0.5725, 0.5490, 0.4863, 0.4824, 0.5098, 0.4941, 0.4431,\n",
              "         0.4431, 0.4471, 0.7216, 0.6235, 0.1647, 0.0000, 0.0000, 0.0000, 0.0078,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.0000, 0.0000,\n",
              "         0.0039],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0941, 0.1647, 0.1804, 0.2235, 0.2549, 0.2706,\n",
              "         0.2549, 0.2471, 0.1569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7137, 0.0157, 0.0000,\n",
              "         0.0039],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FasionMNISTModelV0(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "Jxp1T5K2djU3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set up the model with input parameters\n",
        "\n",
        "model_0 = FasionMNISTModelV0(\n",
        "    input_shape= 784,\n",
        "    hidden_units= 10,\n",
        "    output_shape= len(class_names)\n",
        ")\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA8jOl96fJtS",
        "outputId": "9f43a002-030a-4317-f006-d0d7ea4445b7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasionMNISTModelV0(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1,1,28,28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iawrf2cftVh",
        "outputId": "7f4b6d01-94a9-4d92-dab9-ba7feb7346fe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0315,  0.3171,  0.0531, -0.2525,  0.5959,  0.2112,  0.3233,  0.2694,\n",
              "         -0.1004,  0.0157]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(dummy_x.squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "g1Lk_psGf4nW",
        "outputId": "4e425e40-161c-4d81-f255-98b8dab13f3d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x787278372240>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAK49JREFUeJzt3X1c1fX9//EnlwdROIgoF4mG15UXmSmx0lmSFzVn5TattmlzOgtbRlezb2W1fqPZas4ya1vp+n5Ly01tteaWmjhLbZrO7IKUkWIKmgUHUJCLz+8Pb7FRmrxO4Bvscb/dzu0m8H76efPhA08O5/AixPM8TwAAnGKhrjcAAPh6ooAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOBHuegOfV1dXp3379ikmJkYhISGutwMAMPI8T2VlZUpJSVFo6Inv57S4Atq3b59SU1NdbwMA8BUVFhaqc+fOJ3x7iyugmJgYSdLwlCkKD41sdC59ab75WKseuMickaT+t283Z177y3nmTOyHdebMzbOeN2d+/szV5owkdXin2pw5eG6EOZPyj8PmTNjmPHNGkvb+dKA5UxNrn2aVss5+7h6d97g5c/WTM80ZSQqtsWd+8ZOnzZkb1/3AnGmXZ7+Gjg4uM2ckKe2mvebM4k3rzJmLNts/B2PbHDFnJCn2rsZ/Xf3MlGdfMa0/XF6rqUPfr/96fiLNVkDz58/XQw89pKKiIg0YMECPPvqohgwZctLcZz92Cw+NVHior9HHi2pnvyjDI6LMGUmKDOJYYT77scIi7QUUHRNmP04Qe5Ok8IhgjhXExyncfh7CQuzHkYI7F3VR9gIKD7efu3Yx9odsg/3Yhtq3p7ZBXHuhbYL4vAjiGgqLthe+JIWH2L9YxwbzcYpu/Ne6z4RH2z8vJCk8zP4+BfN1RdJJH0ZplichPP/888rOztbs2bP11ltvacCAARo1apQOHDjQHIcDALRCzVJAjzzyiKZOnarrrrtOZ599tp544glFR0fr6aftd9EBAKenJi+go0ePasuWLcrMzPzPQUJDlZmZqQ0bNnxhfVVVlQKBQIMbAOD01+QF9PHHH6u2tlaJiYkNXp+YmKiioqIvrM/JyZHf76+/8Qw4APh6cP6LqLNmzVJpaWn9rbCw0PWWAACnQJM/Cy4hIUFhYWEqLi5u8Pri4mIlJSV9Yb3P55PPZ38GCACgdWvye0CRkZEaNGiQVq9eXf+6uro6rV69WhkZGU19OABAK9UsvweUnZ2tSZMm6fzzz9eQIUM0d+5cVVRU6LrrrmuOwwEAWqFmKaAJEybo4MGDuueee1RUVKRzzz1XK1eu/MITEwAAX18hnufZf427GQUCAfn9fmV2ucE0CaH40hPPGzqR+PeCG2VRmWD/TeKiqyvNmbSJ9pE/wfwK++4lZ9uPIym2rf19OrSzgznTYat9KG11THCDbKva2zNhVfZM0ib7tZf1+6XmzG3//I45I0nd5taaM/uGf/nYlePpvPITc+Zox7bmzE1PLDZnJOne98aaM539pebMzjXdzJlI+2EkSYH+R82ZPvNs47Bqaqu0ZvsvVVpaqtjY2BOuc/4sOADA1xMFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnGixw0iHa5zCQyIanfvee1/8c98n8//+foU5I0k/GP4Pc2Zg9G5z5ul9F5kz+/43zZw50jG4wZ3VfvulUx1rH3IZUmvfX595B80ZSTrcwz4sdfrcP5ozd/7te+ZM8nr7ebjm3r+YM5L0+8fsQzh9gTpzZur/rDBn/jTW/nfFAgM6mTOSdOuDz5ozdyz5gTnT/n3759K8Bx41ZyRp4rqfmDO9r3/ftL7GO6o1h5cwjBQA0DJRQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgRIudhj3irFsVHuZrdO6jS+1TjI8kBfeu93jmkDnz8RD7/sImHDBn4r5jnwquIC+Bwv+zT96urGz8hPPP9PjhDnMmxNf4a+e/3bdjrTkzxGd/n76RPd2ceeORJ8yZ35ammDOSNCL6A3Pm+z+71Zw5nGT/HvjyH643Z9442M2ckaS29g+TBi7LN2fu7viWOfOtCVPNGUkK37rTnPFqakzra7xqvVb1AtOwAQAtEwUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcCHe9gROpjouSFx7V6PVti+rMx4h/zzZgr97BT82Rb2dvN2fu6PCOObN9R60587eyfuaMJL35aYw5s/8p+1DIsNQzzJnKtARzRpIeLIw2Z3bsSzZnqkfar73Ma35kzlQm2AelStLCyBBzpqSX/fvZsCpzRIs3p5szHd4M7ktd0ST7eTg6I86cmfpQe3MmcvfH5owk5f22lznT40fvBnWsk+EeEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA40WKHkdZGhyskvPHbW/WreeZjDHlspjkjSd55Pc2Z5MNHzJlxl44xZ/Z+v4c5cyTRM2ckqSa+2pyJ7mwf7tg+rp0549u805yRpH+9dbY5s3jco+bMzJ/daM7k/8A+wHT6kDXmjCStGRBrzvxfQa45U1ZnH5Z6w6ybzJmnHnzYnJGk3MP2z/XzfvihOdM2xP6xHf/wNHNGki7rtsOc2T7iXNP6mupK6e8nX8c9IACAExQQAMCJJi+ge++9VyEhIQ1uffr0aerDAABauWZ5DOicc87RqlWr/nMQw2M5AICvh2ZphvDwcCUlJTXHfw0AOE00y2NAO3fuVEpKirp166Zrr71We/bsOeHaqqoqBQKBBjcAwOmvyQsoPT1dixYt0sqVK7VgwQIVFBRo6NChKisrO+76nJwc+f3++ltqampTbwkA0AI1eQGNGTNG3/3ud9W/f3+NGjVKr7zyikpKSvTCCy8cd/2sWbNUWlpafyssLGzqLQEAWqBmf3ZAXFycevXqpV27dh337T6fTz6fr7m3AQBoYZr994DKy8uVn5+v5OTk5j4UAKAVafICuvXWW5Wbm6sPP/xQb7zxhq688kqFhYXp6quvbupDAQBasSb/EdzevXt19dVX69ChQ+rYsaMuuugibdy4UR07dmzqQwEAWrEmL6AlS5Y0yf9TOKFWodG1jV5fpzrzMaI+Dm4IZ/nIcnNmZ459yGXo/x4wZ6KqD5oz9/f6qzkjSbe8/H1zpt/l75szvScUmzP/+690c0aSek/bZs68O/oMc6bd3kpzZnBv+xN0cicMNGck6cBy+w9Hxi2wn/MuT75nzqT8+fiPJ3+ZCfNuNWckKbrY/nXltzH2gbtTfvqyOXNVz3+ZM5L00od9zZnU2z4yra+pqGIYKQCg5aKAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAEyGe5wU3kbOZBAIB+f1+XbP6GkW2i2x07qPDfvOxHkxbZs5I0h9Lzzdn/nF3hjlzcIB9Vmzf0XnmzE+S15ozkvTI8DHmTOB8++DOYPx53q+DyrULtf9xxG9mZ5kzlfH2gZWlve2fqnWR9mGakhT3rv3aS1r3iTlzJDXGnNmfYd/b0cQac0aSQirt36P3vvNdc+axHfaBwNN+dJM5I0kf97df469mP2RaX1ZWp15nFau0tFSxsbEnXMc9IACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhhHyt7iqz/59kKjYpq9Po+jxaZj/HsHy8wZyQpd549948n5pkzF/zCPu22fFoHc+bhkO+YM5KUl93enPF9bP+e55HrnjJnzntppjkjSd2X2qcm77/Wnjnr12XmTEl6O3OmbWylOSNJSdn55ozXt4c5UxdhnwoeU2CO6EhFhD0kqevID+2hxARz5MGiUeZMRElwH9ulM+1fiyZOsX0tqqmplHTvSddxDwgA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnGixw0h7PnVI4WG+Rq+f8Mp68zG6Rx4wZyTptQj7MNLdNUfNmZKB9kzMt6vNmaKtSeaMJMX82z5IMqLCM2cevWi4OZOTu9SckaTXzu9jzixKetWcmTJ1qDmz89It5szjJWnmjCT9dvGF5ky7F+zDUn8ye5k5U1ztN2e+FbPdnJGk24d+15x57wH7kN7Ap/bPpbZvvWvOSFJ8EHc7LvmV7etrZXm1Xl998nXcAwIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJ0I8z7NPh2xGgUBAfr9fmWfOUHho44eR6kil+Vjv/c+Z5owkPX3Z78yZm96eYM4EPo02ZyKKIs2ZtFkbzBlJGryt1pyJCbN/nF774RBzpnC0fWClJL014zfmzNA7f2rOxL9TZs70++075kxBRQdzRpLuS33JnPnxrJvNmZKrKsyZYPx+0B+Cyi391H7trcw/y5wJfTvGnAmpMUckSR3/ZR9YHH3bR6b1NRVVWn35kyotLVVsbOwJ13EPCADgBAUEAHDCXEDr1q3T2LFjlZKSopCQEK1YsaLB2z3P0z333KPk5GS1adNGmZmZ2rlzZ1PtFwBwmjAXUEVFhQYMGKD58+cf9+1z5szRvHnz9MQTT2jTpk1q27atRo0apcpK+8/+AQCnL/NfRB0zZozGjBlz3Ld5nqe5c+fqrrvu0rhx4yRJzzzzjBITE7VixQpNnDjxq+0WAHDaaNLHgAoKClRUVKTMzMz61/n9fqWnp2vDhuM/06qqqkqBQKDBDQBw+mvSAioqKpIkJSYmNnh9YmJi/ds+LycnR36/v/6WmpralFsCALRQzp8FN2vWLJWWltbfCgsLXW8JAHAKNGkBJSUlSZKKi4sbvL64uLj+bZ/n8/kUGxvb4AYAOP01aQGlpaUpKSlJq1evrn9dIBDQpk2blJGR0ZSHAgC0cuZnwZWXl2vXrl31LxcUFGjbtm2Kj49Xly5dNHPmTD3wwAPq2bOn0tLSdPfddyslJUVXXHFFU+4bANDKmQto8+bNuvjii+tfzs7OliRNmjRJixYt0u23366KigpNmzZNJSUluuiii7Ry5UpFRUU13a4BAK1eix1G+o8dKWoX0/ifEM64yT4QMvqjw+aMJE197s/mzK3rv2fOnPFKmDkz7t5V5szab51jzkhSYGCyOXO4k/2nvoHu5ohCugQ35PL8VPuTYD54uo85U3amOaKkTfbhr75DVfYDSfrwJvuXhQfOe9GcOSfy+M+O/TK3XXqtORO7qMSckaQP/q+3OVPWNYgDhdgjKyY8EsSBpErP/nXlnou/Y1pfU1elVbvnM4wUANAyUUAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4IT5zzGcKj/852SFRjf+Tzhcee+b5mO88uHZ5owk3fGifRrvGZvqzJl2+eXmzKp+9r8om/9QijkjSfFv20f4Voywv08JK9qaM7/57lPmjCTdc9Vkc+b5FQ+ZM9fcdZs5U/jdGnOmw9poc0aSeiTuNmfiQu3T5RPD7J8XdbFtzJnh7TeZM5J0wYx/mzOvXtbPnKl52j59fNwb15szknRZr3fMmV/nLjatLy+r06pGDNnnHhAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAONFih5HWHIhWaJvGDyNd87cM8zFiK+wDACWpfHS1OfNJn0hzJmanfVDjB78ZYs6Mvyi4QY1/irQf68d97ENjV0ReYs6U1AY3hHP3nfbvySY8YB8s2mv6++ZMzxD79Zq/so85I0m7/36mOfP8uHRzZu63fOZM1WL7QNtHto8wZySp28P2c757cow5E1X+iTlz+8C/mzOSFBt6xJyZcfUNpvU1NZWS/t9J13EPCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcaLHDSLstO6Lw8MYPAox/qNB8jGfPXGXOSNI71UfNmVu6ftecWTL9BXPmsptnmjPLqy4wZyTprPkfmTOv/ekb5kz8Yfvwybnn248jSRGLasyZI6MD5syhyrbmzJikHebMp5vizBlJqoxPMWcmJNiH2s7VMHPm+53tx3m80n4cSQo7WGnO1J4VYs6U7vabM3MqRpozkhTzun1Qb9qDO03rqyuOSqNPvo57QAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgRIsdRjrjyT8pOias0es/qm5vPsbZT2eZM5LUbY59KOQr768wZwb980fmzPJfPWzOTHrvh+aMJL0/0z6wctSF28yZyFD7gNBX/j7YnJGkmtIqcyasyGfOXDl2rTmz6tBZ5syHE+0fI0nKvf4hcyYqpPGfr/WZN1ebM9e9cZ05UxeIMGckqV1fe+b+gUvNmT98rxGTOz8ntPgTc0aS9v82zpw5MrrCtL7Ga9zAZu4BAQCcoIAAAE6YC2jdunUaO3asUlJSFBISohUrVjR4++TJkxUSEtLgNnq0/e4lAOD0Zi6giooKDRgwQPPnzz/hmtGjR2v//v31t8WLF3+lTQIATj/mJyGMGTNGY8aM+dI1Pp9PSUlJQW8KAHD6a5bHgNauXatOnTqpd+/euv7663Xo0KETrq2qqlIgEGhwAwCc/pq8gEaPHq1nnnlGq1ev1i9/+Uvl5uZqzJgxqq2tPe76nJwc+f3++ltqampTbwkA0AI1+e8BTZw4sf7f/fr1U//+/dW9e3etXbtWI0aM+ML6WbNmKTs7u/7lQCBACQHA10CzPw27W7duSkhI0K5du477dp/Pp9jY2AY3AMDpr9kLaO/evTp06JCSk5Ob+1AAgFbE/CO48vLyBvdmCgoKtG3bNsXHxys+Pl733Xefxo8fr6SkJOXn5+v2229Xjx49NGrUqCbdOACgdTMX0ObNm3XxxRfXv/zZ4zeTJk3SggULtH37dv3hD39QSUmJUlJSNHLkSP385z+Xz2eflwUAOH2FeJ7nud7EfwsEAvL7/Rqw9BaFRTe+tKKetg8j3X9hiDkjSfHb7blfzV5gzjw4doI588Gd0eaMdzC4bw563rHVnLnxnX+ZMzPfnHjyRZ/T638+NWckqaZgtznTJjfRnPlZ6ivmzLT5N5ozYUfMEUlSyvJ/mzOXrX7HnPnDnG+ZMxEVdeZM5LQic0aSHu9p/yX62XvHmjN5H3cyZ5YN/J05I0mXvnSLPRRTbVped6RShT+5X6WlpV/6uD6z4AAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEk/9J7qYS2BGv0KioRq+PO1hpP0iSPSJJbdZGmDMvlQw0Z+ra2I/T+86PzZndE4P7E+hT337PnJk/1j4pOHZYG3Nm14/tGUnq8Lb9Dye2D//AnPGHVpkzwyZuMWdyXxhkzkiS19Z+/l4ZNcCcqfqOfbJ8TZT9+2b/TyPNGUka//Np5ky7l2PsB7qixByZcsPN9uNIisiwn7/odxv/tViSao82bh33gAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADAiRDP8zzXm/hvgUBAfr9fd7wxRr52jR/GuWlogvlYh644x5yRpE8vqzBn2q5vZ86UnVlnztx82V/MmYffHGnOSJJXax8kGZdQbs74n7QPdzzSIbg5u92nv2/ObNzZzZyZcf5r5szvlo42Z9748a/MGUl6+FC6OfOX3w81Z2IKa8yZ/RMaOenyv/S6/aA5I0klGZ3NmcoffmrOfPJRnDkz8OwCc0aS9j3Z3ZyZ/8A80/rysjpd0m+vSktLFRsbe8J13AMCADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACda7DDSjEvvU3hEVKNzXrh9MOZ3HvybOSNJTz5zuTkTlmEfUFizqb050+WvJebMrqv95owk9cvYZc6MSLAP+0yNOGTO3LJ8kjkjSfeOe8Gc+eV79mGu5YUnHtB4QnX2azwiYM9IUlS/EnPm8Ptx5sy0b/3dnFm4ZJQ5k7i52pyRpE+m24fnxi9oa86MeGi9OXNxu3fNGUn6wfofmzNtdzT+a7Ek1VZVKm/unQwjBQC0TBQQAMAJCggA4AQFBABwggICADhBAQEAnKCAAABOUEAAACcoIACAExQQAMAJCggA4AQFBABwItz1Bk5k/0XhCo1q/PYGXvSB+Rh/vWqwOSNJ4fZZiEqY28acidz2jjnjdU0xZ9L+fMSckaTJV71uzmw9fKY589Kk4eZMz0+LzBlJ+saE3eZMdbX90+i8c/PNmbf+3cWc8W/1mTOSFL2hnTlz4X1bzZlVfWPMmbTO9o/R/sft748kaXUHc6Rw0mFzZl9VnDlzd/ZUc0aSuobZB9SGV9q+RtTUVCqvEeu4BwQAcIICAgA4YSqgnJwcDR48WDExMerUqZOuuOIK5eU1vKNVWVmprKwsdejQQe3atdP48eNVXFzcpJsGALR+pgLKzc1VVlaWNm7cqFdffVXV1dUaOXKkKioq6tfcfPPNeumll7R06VLl5uZq3759uuqqq5p84wCA1s306OnKlSsbvLxo0SJ16tRJW7Zs0bBhw1RaWqqnnnpKzz33nC655BJJ0sKFC3XWWWdp48aNuuCCC5pu5wCAVu0rPQZUWloqSYqPj5ckbdmyRdXV1crMzKxf06dPH3Xp0kUbNmw47v9RVVWlQCDQ4AYAOP0FXUB1dXWaOXOmLrzwQvXt21eSVFRUpMjISMXFxTVYm5iYqKKi4z8tNicnR36/v/6Wmpoa7JYAAK1I0AWUlZWlHTt2aMmSJV9pA7NmzVJpaWn9rbCw8Cv9fwCA1iGoX0SdMWOGXn75Za1bt06dO3euf31SUpKOHj2qkpKSBveCiouLlZSUdNz/y+fzyecL7pflAACtl+kekOd5mjFjhpYvX641a9YoLS2twdsHDRqkiIgIrV69uv51eXl52rNnjzIyMppmxwCA04LpHlBWVpaee+45vfjii4qJial/XMfv96tNmzby+/2aMmWKsrOzFR8fr9jYWN14443KyMjgGXAAgAZMBbRgwQJJ0vDhwxu8fuHChZo8ebIk6de//rVCQ0M1fvx4VVVVadSoUXr88cebZLMAgNNHiOd5nutN/LdAICC/368RCVMUHhrZ6Nyu7B7mY70wca45I0nfWzLTnBl96WZzZtsDA82ZwsvsH86z5paaM5K06wf2QY1Kqzj5ms/p8GK0ObPsl78yZyRp1Fv2AY/nJn5kzjzTdZ05860PxpgzO18/05yRpIHDGzNKsqFnz1xlzlx+pv0nI3tvOd+cOffb75ozkvT20rPNmc7L95ozR3p0NGdqo8LMGUlqk22/Xi/uaBv2XFlerfsvWKXS0lLFxsaecB2z4AAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEUH8R9VQomNZdYVFRjV7f8wn7n/K+MukGc0aSYs4qMWc2Pmaf4Bv2kwPmTOKz9qm6n/yqzpyRpLu6LTVnVn1qny686exzzJmY0OAu7aodcebMx1OKzZlnXk8wZxZ1/6M5Mz1snDkjSfd3fsmcuWnfJfYD/c1+Hi5v/4Y58/ZE+7R8SaqYZv/c+CQjxZwp/17AnEmdar/uJOm997ubM9V1tsnbNRVVkk4+HZ17QAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgRIsdRhqzx1NYpNfo9YeGnmE+Rs/fHTFnJKnodnum46u7zZk9nc40Zyoyas2ZmKMR5owkPT98kDnzwSPJ5kz8TnNEg3+fbQ9J6rbsE3uovd8cmb3BPiT0vnD7YMzlQxeYM5LUMTTEnLko9gNz5r3SRHOmc+Sn5sw7T1aYM5LU8/L95sykt941Z2atH2/OfPT93uaMJMXtsGfuHvWyaX1FWa3WN2Id94AAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwIkQz/MaP/HzFAgEAvL7/frN5gvUpl3jZ6XO+eOV5mP5880RSVL0gRpzZvADm82ZrdkDzZmwSvveatsEN5P24E32Ya6/6LvcnJn55kRz5oyEEnNGktr+xP7pUBfTxpw52rGtORNWaR80W945ypyRpOjio+bMp7185kx8XqU583G2/bor3W0fGCtJCrNHzj5njzlTd6X9PBwa28eckaRP+tozPe79l2l9jXdUaw4vUWlpqWJjY0+4jntAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOBEcFMoT4EtZV0V6UU2en33xYfMx7j8jxvMGUma7t9tzpy17jpzJjE+wpyZ8ouXzZnnrhtjzkhS2d4TDxk8kX9162rOvDn0cXPmvL/MNGckKfLHQUyfDGKcb9q9/zRndi7qZ870uafInJGkd+/oaM7E5IWYM5mPrTdnls3JNGfu+Z8/mTOS9Mhvv2POfLgnzZzpUrvDnFl0/8PmjCRdtnKmORPisw2aDfFCpMMnX8c9IACAExQQAMAJUwHl5ORo8ODBiomJUadOnXTFFVcoLy+vwZrhw4crJCSkwW369OlNumkAQOtnKqDc3FxlZWVp48aNevXVV1VdXa2RI0eqoqKiwbqpU6dq//799bc5c+Y06aYBAK2f6UkIK1eubPDyokWL1KlTJ23ZskXDhg2rf310dLSSkpKaZocAgNPSV3oMqLS0VJIUHx/f4PXPPvusEhIS1LdvX82aNUuHD5/46RBVVVUKBAINbgCA01/QT8Ouq6vTzJkzdeGFF6pv3//8kfFrrrlGXbt2VUpKirZv36477rhDeXl5WrZs2XH/n5ycHN13333BbgMA0EoFXUBZWVnasWOH1q9v+Dz+adOm1f+7X79+Sk5O1ogRI5Sfn6/u3bt/4f+ZNWuWsrOz618OBAJKTU0NdlsAgFYiqAKaMWOGXn75Za1bt06dO3f+0rXp6emSpF27dh23gHw+n3zGX3ICALR+pgLyPE833nijli9frrVr1yot7eS/8btt2zZJUnJyclAbBACcnkwFlJWVpeeee04vvviiYmJiVFR0bMyH3+9XmzZtlJ+fr+eee06XXXaZOnTooO3bt+vmm2/WsGHD1L9//2Z5BwAArZOpgBYsWCDp2C+b/reFCxdq8uTJioyM1KpVqzR37lxVVFQoNTVV48eP11133dVkGwYAnB7MP4L7MqmpqcrNzf1KGwIAfD202GnYr73VV6Ftohq9vtdje83HeGLhWHNGks69/jFzZkhX+wTtLf3ONmcmxx4wZ7Twr/aMpF8utk8K/sOfLzFn/jRggDnT4Z9BTLWWNPan9m+gNpzfzpypHDnQnBnYtcCcKS8M7vfq4rfYH7ON+ajanCmsjD/5os85eEGdOfP8RPsEbUl6bJl9Evusn9lHj/37lr4nX/Q5173byZyRpF6LKs2Z/Ow+pvV1lZXSAydfxzBSAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCixQ4jbf92qMIiG9+P+/K7mo8RVfLl071PZH1Fb3Nm90P2TJePysyZb18y2pwpKo8xZyTpaHv7UMi6KHtm43mLzZmMP88wZyRp0yVJ5oxX86k5c3DKYXMmrCranPnwoR7mjCT1ebjQnPGiIs2ZXTf0MmeuX7jKnHmy/VBzRpJm3zDVnIldu82c+cbPqsyZZevSzRlJ2rjsCXPm8gzb4Oaauir9uxHruAcEAHCCAgIAOEEBAQCcoIAAAE5QQAAAJyggAIATFBAAwAkKCADgBAUEAHCCAgIAOEEBAQCcaHGz4Dzv2Hy22qOVplxtVYj5WLVHg5sFV1lebc7UVNveH0mqqbFnQiuOmjO1h+1zqCSprtK+vzrPPgsuUGbPWK+fz9TUBXH+PPv1EMw5rwm3Z4L5GEnHZnlZebX2z6e62jBzprK8xn6cw0Geh+pae8azX0NVQXxNCfZjG8znk/V6+Ozz6LOv5ycS4p1sxSm2d+9epaamut4GAOArKiwsVOfOnU/49hZXQHV1ddq3b59iYmIUEtLwXk0gEFBqaqoKCwsVGxvraIfucR6O4Twcw3k4hvNwTEs4D57nqaysTCkpKQoNPfEjPS3uR3ChoaFf2piSFBsb+7W+wD7DeTiG83AM5+EYzsMxrs+D3+8/6RqehAAAcIICAgA40aoKyOfzafbs2fL5fK634hTn4RjOwzGch2M4D8e0pvPQ4p6EAAD4emhV94AAAKcPCggA4AQFBABwggICADjRagpo/vz5OvPMMxUVFaX09HS9+eabrrd0yt17770KCQlpcOvTp4/rbTW7devWaezYsUpJSVFISIhWrFjR4O2e5+mee+5RcnKy2rRpo8zMTO3cudPNZpvRyc7D5MmTv3B9jB492s1mm0lOTo4GDx6smJgYderUSVdccYXy8vIarKmsrFRWVpY6dOigdu3aafz48SouLna04+bRmPMwfPjwL1wP06dPd7Tj42sVBfT8888rOztbs2fP1ltvvaUBAwZo1KhROnDggOutnXLnnHOO9u/fX39bv3696y01u4qKCg0YMEDz588/7tvnzJmjefPm6YknntCmTZvUtm1bjRo1SpVBDmtsqU52HiRp9OjRDa6PxYsXn8IdNr/c3FxlZWVp48aNevXVV1VdXa2RI0eqoqKifs3NN9+sl156SUuXLlVubq727dunq666yuGum15jzoMkTZ06tcH1MGfOHEc7PgGvFRgyZIiXlZVV/3Jtba2XkpLi5eTkONzVqTd79mxvwIABrrfhlCRv+fLl9S/X1dV5SUlJ3kMPPVT/upKSEs/n83mLFy92sMNT4/PnwfM8b9KkSd64ceOc7MeVAwcOeJK83Nxcz/OOfewjIiK8pUuX1q957733PEnehg0bXG2z2X3+PHie533zm9/0brrpJnebaoQWfw/o6NGj2rJlizIzM+tfFxoaqszMTG3YsMHhztzYuXOnUlJS1K1bN1177bXas2eP6y05VVBQoKKiogbXh9/vV3p6+tfy+li7dq06deqk3r176/rrr9ehQ4dcb6lZlZaWSpLi4+MlSVu2bFF1dXWD66FPnz7q0qXLaX09fP48fObZZ59VQkKC+vbtq1mzZunw4cMutndCLW4Y6ed9/PHHqq2tVWJiYoPXJyYm6v3333e0KzfS09O1aNEi9e7dW/v379d9992noUOHaseOHYqJiXG9PSeKiook6bjXx2dv+7oYPXq0rrrqKqWlpSk/P1933nmnxowZow0bNigszP53d1q6uro6zZw5UxdeeKH69u0r6dj1EBkZqbi4uAZrT+fr4XjnQZKuueYade3aVSkpKdq+fbvuuOMO5eXladmyZQ5321CLLyD8x5gxY+r/3b9/f6Wnp6tr16564YUXNGXKFIc7Q0swceLE+n/369dP/fv3V/fu3bV27VqNGDHC4c6aR1ZWlnbs2PG1eBz0y5zoPEybNq3+3/369VNycrJGjBih/Px8de/e/VRv87ha/I/gEhISFBYW9oVnsRQXFyspKcnRrlqGuLg49erVS7t27XK9FWc+uwa4Pr6oW7duSkhIOC2vjxkzZujll1/Wa6+91uDPtyQlJeno0aMqKSlpsP50vR5OdB6OJz09XZJa1PXQ4gsoMjJSgwYN0urVq+tfV1dXp9WrVysjI8PhztwrLy9Xfn6+kpOTXW/FmbS0NCUlJTW4PgKBgDZt2vS1vz727t2rQ4cOnVbXh+d5mjFjhpYvX641a9YoLS2twdsHDRqkiIiIBtdDXl6e9uzZc1pdDyc7D8ezbds2SWpZ14PrZ0E0xpIlSzyfz+ctWrTIe/fdd71p06Z5cXFxXlFRkeutnVK33HKLt3btWq+goMB7/fXXvczMTC8hIcE7cOCA6601q7KyMm/r1q3e1q1bPUneI4884m3dutXbvXu353me9+CDD3pxcXHeiy++6G3fvt0bN26cl5aW5h05csTxzpvWl52HsrIy79Zbb/U2bNjgFRQUeKtWrfLOO+88r2fPnl5lZaXrrTeZ66+/3vP7/d7atWu9/fv3198OHz5cv2b69Olely5dvDVr1nibN2/2MjIyvIyMDIe7bnonOw+7du3y7r//fm/z5s1eQUGB9+KLL3rdunXzhg0b5njnDbWKAvI8z3v00Ue9Ll26eJGRkd6QIUO8jRs3ut7SKTdhwgQvOTnZi4yM9M444wxvwoQJ3q5du1xvq9m99tprnqQv3CZNmuR53rGnYt99991eYmKi5/P5vBEjRnh5eXluN90Mvuw8HD582Bs5cqTXsWNHLyIiwuvatas3derU0+6btOO9/5K8hQsX1q85cuSId8MNN3jt27f3oqOjvSuvvNLbv3+/u003g5Odhz179njDhg3z4uPjPZ/P5/Xo0cO77bbbvNLSUrcb/xz+HAMAwIkW/xgQAOD0RAEBAJyggAAATlBAAAAnKCAAgBMUEADACQoIAOAEBQQAcIICAgA4QQEBAJyggAAATlBAAAAn/j888zFwVuEMZQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzDj9qiAh9aI",
        "outputId": "0e88f975-5a41-4667-fb6d-1ed3ed23df1f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nUhPIC7iDAa",
        "outputId": "99bd22b4-6e63-46b5-d724-4204d8ebce54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_stack.1.weight',\n",
              "              tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
              "                      [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
              "                      [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
              "                      ...,\n",
              "                      [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
              "                      [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
              "                      [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]])),\n",
              "             ('layer_stack.1.bias',\n",
              "              tensor([-0.0093,  0.0283, -0.0033,  0.0255,  0.0017,  0.0037, -0.0302, -0.0123,\n",
              "                       0.0018,  0.0163])),\n",
              "             ('layer_stack.2.weight',\n",
              "              tensor([[ 0.0614, -0.0687,  0.0021,  0.2718,  0.2109,  0.1079, -0.2279, -0.1063,\n",
              "                        0.2019,  0.2847],\n",
              "                      [-0.1495,  0.1344, -0.0740,  0.2006, -0.0475, -0.2514, -0.3130, -0.0118,\n",
              "                        0.0932, -0.1864],\n",
              "                      [ 0.2488,  0.1500,  0.1907,  0.1457, -0.3050, -0.0580,  0.1643,  0.1565,\n",
              "                       -0.2877, -0.1792],\n",
              "                      [ 0.2305, -0.2618,  0.2397, -0.0610,  0.0232,  0.1542,  0.0851, -0.2027,\n",
              "                        0.1030, -0.2715],\n",
              "                      [-0.1596, -0.0555, -0.0633,  0.2302, -0.1726,  0.2654,  0.1473,  0.1029,\n",
              "                        0.2252, -0.2160],\n",
              "                      [-0.2725,  0.0118,  0.1559,  0.1596,  0.0132,  0.3024,  0.1124,  0.1366,\n",
              "                       -0.1533,  0.0965],\n",
              "                      [-0.1184, -0.2555, -0.2057, -0.1909, -0.0477, -0.1324,  0.2905,  0.1307,\n",
              "                       -0.2629,  0.0133],\n",
              "                      [ 0.2727, -0.0127,  0.0513,  0.0863, -0.1043, -0.2047, -0.1185, -0.0825,\n",
              "                        0.2488, -0.2571],\n",
              "                      [ 0.0425, -0.1209, -0.0336, -0.0281, -0.1227,  0.0730,  0.0747, -0.1816,\n",
              "                        0.1943,  0.2853],\n",
              "                      [-0.1310,  0.0645, -0.1171,  0.2168, -0.0245, -0.2820,  0.0736,  0.2621,\n",
              "                        0.0012, -0.0810]])),\n",
              "             ('layer_stack.2.bias',\n",
              "              tensor([-0.0087,  0.1791,  0.2712, -0.0791,  0.1685,  0.1762,  0.2825,  0.2266,\n",
              "                      -0.2612, -0.2613]))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss Function - nn.CrossEntropyLoss - since working with multi-class data\n",
        "* Optimizer - 'torch.optim.SGD' - Stochastic Gradient Descent\n",
        "* Metrics - 'torchmetrics.Accuracy' - Accuracy"
      ],
      "metadata": {
        "id": "mujmRxPMiWSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device\n",
        "\n",
        "if Path('helper_functions.py').is_file():\n",
        "  print('helper_functions.py already exists')\n",
        "  print('Skipping download')\n",
        "  pass\n",
        "else:\n",
        "  print('Downloading helper_functions.py')\n",
        "  request = requests.get('https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')\n",
        "  with open('helper_functions.py', 'wb') as f:\n",
        "    f.write(request.content)\n",
        "    print('helper_functions.py downloaded')\n",
        "\n",
        "\n",
        "# Import accuracy metric\n",
        "\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Set up loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                              lr=0.01)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-ImeSrri6Wi",
        "outputId": "3d180afe-fdb7-4e06-9599-08cf3c7f52ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading helper_functions.py\n",
            "helper_functions.py downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Creating a function to time our experiments\n",
        "\n",
        "* Machine learning is very experimental\n",
        "\n",
        "* Two of the main things you will often want to track are:\n",
        "  1. Models Performance ( loss and accuracy values etc)\n",
        "  2. How fast it runs\n",
        "\n"
      ],
      "metadata": {
        "id": "RvAiOq9ml5V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def print_train_time(start: float,\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "  total_time = end - start\n",
        "  print(f'Train time on {device}: {total_time:.3f} seconds')\n",
        "  return total_time\n",
        "\n",
        "start_time = timer()\n",
        "\n",
        "# some _code\n",
        "\n",
        "end_time = timer()\n",
        "print_train_time(start_time, end_time, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uKxeekWmXHZ",
        "outputId": "2dd08e4d-f995-4b0a-d630-ae7cd741cd2f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train time on cuda: 0.000 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.7313999964917457e-05"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Creating a training loop and training a model on batches of data\n",
        "\n",
        "1. Loop through epochs.\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*\n",
        "4. Print out what happened\n",
        "5. time it all"
      ],
      "metadata": {
        "id": "rLQUSLcKnglE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs ( keep them small for faster training time)\n",
        "\n",
        "epochs = 4\n",
        "\n",
        "# Send model to the target device\n",
        "model_0.to(device)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "\n",
        "  ## Training\n",
        "  train_loss = 0\n",
        "\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "# Training Model\n",
        "    model_0.train()\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_pred = model_0(X)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 400 == 0:\n",
        "      print(f'Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples')\n",
        "\n",
        "  train_loss /= len(train_dataloader)\n",
        "  print(f'Train Loss: {train_loss:.5f}')\n",
        "\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_loss = 0\n",
        "    for X_test, y_test in test_dataloader:\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "      test_pred = model_0(X_test)\n",
        "      test_loss += loss_fn(test_pred, y_test)\n",
        "      test_loss /= len(test_dataloader)\n",
        "      test_acc = accuracy_fn(y_true=y_test,\n",
        "                             y_pred=test_pred.argmax(dim=1))\n",
        "    print(f'Test Loss: {test_loss:.5f}')\n",
        "    print(f'Test Acc: {test_acc:.5f}')\n",
        "\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                            end=train_time_end_on_cpu,\n",
        "                                            device=str(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806,
          "referenced_widgets": [
            "4bcc2fe5fcb24e7783c2884ef8782148",
            "32417f9086484c00983acffc5dad074d",
            "170c57c7aaf84f38843e2a130a7d9551",
            "387c5ce60a8341e0938dcdae5c6c0ae0",
            "8f5d49bb81d8458fa459066c5bc60fab",
            "c2126b7fa9324371920d7e29dbb78d87",
            "ba4a2845f2614758a57dd36fe213810f",
            "db75168f93b54e5ba2779d5d1bde5349",
            "3228e763532e4528bee92c55ebd06fc3",
            "92e13007e5f24f89af5b771bab06e264",
            "ed4139fd8bc84e9ca895d47674bc1d46"
          ]
        },
        "id": "v6zYCWPpoLry",
        "outputId": "4d1e6d63-f85a-4123-8729-e74af9054e44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bcc2fe5fcb24e7783c2884ef8782148"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "Train Loss: 0.43878\n",
            "Test Loss: 0.00100\n",
            "Test Acc: 87.50000\n",
            "Epoch: 1\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "Train Loss: 0.43276\n",
            "Test Loss: 0.00103\n",
            "Test Acc: 87.50000\n",
            "Epoch: 2\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "Train Loss: 0.42745\n",
            "Test Loss: 0.00107\n",
            "Test Acc: 93.75000\n",
            "Epoch: 3\n",
            "-------\n",
            "Looked at 0/60000 samples\n",
            "Looked at 12800/60000 samples\n",
            "Looked at 25600/60000 samples\n",
            "Looked at 38400/60000 samples\n",
            "Looked at 51200/60000 samples\n",
            "Train Loss: 0.42286\n",
            "Test Loss: 0.00107\n",
            "Test Acc: 87.50000\n",
            "Train time on cuda: 37.342 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make Predictions and get Model_0 results"
      ],
      "metadata": {
        "id": "1yQtg4V72E6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model:torch.nn.Module,\n",
        "               data_loader:torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device:torch.device = device):\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      y_pred = model(X)\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y,\n",
        "                         y_pred=y_pred.argmax(dim=1))\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "\n",
        "  return {\"model_name\":model.__class__.__name__,\n",
        "          \"model_loss\":loss.item(),\n",
        "          \"model_acc\":acc}\n",
        "\n",
        "# Calculate the model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "\n",
        "model_0_results\n"
      ],
      "metadata": {
        "id": "V-K0_pqq5OoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5. Model 1: Building a better model with Non linearlity\n",
        "\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units_1: int,\n",
        "               hidden_units_2: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units_1),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units_1,\n",
        "                  out_features=hidden_units_2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units_2,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)\n",
        "\n",
        "model_1 = FashionMNISTModelV1(input_shape=784,\n",
        "                              hidden_units_1= 10,\n",
        "                              hidden_units_2=10,\n",
        "                              output_shape=len(class_names))\n",
        "model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFZ1iK_G6uBr",
        "outputId": "99d357a2-0757-49f0-9a8b-8fad518de171"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_stack.1.weight',\n",
              "              tensor([[-0.0282, -0.0165, -0.0101,  ..., -0.0272,  0.0052,  0.0173],\n",
              "                      [ 0.0309, -0.0218, -0.0176,  ...,  0.0272, -0.0169,  0.0213],\n",
              "                      [ 0.0087, -0.0238,  0.0124,  ..., -0.0115, -0.0096,  0.0006],\n",
              "                      ...,\n",
              "                      [-0.0131,  0.0312,  0.0269,  ..., -0.0275,  0.0082,  0.0330],\n",
              "                      [ 0.0051,  0.0048, -0.0134,  ..., -0.0047, -0.0345,  0.0140],\n",
              "                      [ 0.0278,  0.0035, -0.0279,  ...,  0.0152, -0.0084,  0.0227]])),\n",
              "             ('layer_stack.1.bias',\n",
              "              tensor([-0.0054, -0.0284, -0.0354, -0.0013,  0.0105, -0.0211,  0.0281,  0.0169,\n",
              "                       0.0215,  0.0165])),\n",
              "             ('layer_stack.3.weight',\n",
              "              tensor([[-0.3050, -0.0580,  0.1643,  0.1565, -0.2877, -0.1792,  0.2305, -0.2618,\n",
              "                        0.2397, -0.0610],\n",
              "                      [ 0.0232,  0.1542,  0.0851, -0.2027,  0.1030, -0.2715, -0.1596, -0.0555,\n",
              "                       -0.0633,  0.2302],\n",
              "                      [-0.1726,  0.2654,  0.1473,  0.1029,  0.2252, -0.2160, -0.2725,  0.0118,\n",
              "                        0.1559,  0.1596],\n",
              "                      [ 0.0132,  0.3024,  0.1124,  0.1366, -0.1533,  0.0965, -0.1184, -0.2555,\n",
              "                       -0.2057, -0.1909],\n",
              "                      [-0.0477, -0.1324,  0.2905,  0.1307, -0.2629,  0.0133,  0.2727, -0.0127,\n",
              "                        0.0513,  0.0863],\n",
              "                      [-0.1043, -0.2047, -0.1185, -0.0825,  0.2488, -0.2571,  0.0425, -0.1209,\n",
              "                       -0.0336, -0.0281],\n",
              "                      [-0.1227,  0.0730,  0.0747, -0.1816,  0.1943,  0.2853, -0.1310,  0.0645,\n",
              "                       -0.1171,  0.2168],\n",
              "                      [-0.0245, -0.2820,  0.0736,  0.2621,  0.0012, -0.0810, -0.0087,  0.1791,\n",
              "                        0.2712, -0.0791],\n",
              "                      [ 0.1685,  0.1762,  0.2825,  0.2266, -0.2612, -0.2613, -0.2624,  0.1987,\n",
              "                       -0.1606,  0.1747],\n",
              "                      [-0.0471, -0.1303,  0.2380, -0.0611, -0.1707, -0.0485, -0.2011, -0.3045,\n",
              "                       -0.0554, -0.0178]])),\n",
              "             ('layer_stack.3.bias',\n",
              "              tensor([-0.1802,  0.2803, -0.0706, -0.0803,  0.2506,  0.0352, -0.0744,  0.0727,\n",
              "                      -0.2857,  0.3109])),\n",
              "             ('layer_stack.5.weight',\n",
              "              tensor([[ 0.0834,  0.1112, -0.2332,  0.2418, -0.2599, -0.1099, -0.2028, -0.1597,\n",
              "                        0.0675, -0.3136],\n",
              "                      [-0.3096,  0.0722,  0.1042,  0.0045, -0.1201, -0.0885, -0.0437, -0.0641,\n",
              "                       -0.1956,  0.2380],\n",
              "                      [ 0.0640, -0.2116, -0.0643, -0.2288, -0.1848,  0.2402,  0.0965,  0.1385,\n",
              "                       -0.0410, -0.0789],\n",
              "                      [ 0.0409, -0.2247, -0.3147,  0.2223,  0.0064,  0.1918,  0.2379,  0.1383,\n",
              "                       -0.1081, -0.0580],\n",
              "                      [-0.0165,  0.0189,  0.0123, -0.2472, -0.1738,  0.2304,  0.0101, -0.1434,\n",
              "                        0.2138, -0.0939],\n",
              "                      [-0.1286,  0.2210, -0.2495, -0.1514,  0.2393, -0.0309, -0.1304,  0.2624,\n",
              "                        0.0481, -0.2971],\n",
              "                      [-0.2338, -0.0802, -0.0174, -0.2381, -0.1192,  0.1069, -0.2949, -0.2963,\n",
              "                        0.0822,  0.0604],\n",
              "                      [-0.3054, -0.2942,  0.2627,  0.3156, -0.1382,  0.3150,  0.2184,  0.2811,\n",
              "                       -0.1361, -0.0774],\n",
              "                      [ 0.1864,  0.1173, -0.2524,  0.1893,  0.3084, -0.2008, -0.2936, -0.3012,\n",
              "                       -0.2001, -0.1393],\n",
              "                      [-0.1787, -0.2770,  0.1975,  0.3093,  0.2647,  0.1980,  0.1404, -0.1247,\n",
              "                       -0.2552, -0.1868]])),\n",
              "             ('layer_stack.5.bias',\n",
              "              tensor([ 0.1432,  0.0692,  0.0404, -0.1919, -0.1488, -0.1334, -0.0401, -0.0597,\n",
              "                       0.0334, -0.1053]))])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), # Use model_1 parameters for the optimizer\n",
        "                              lr=0.01)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# Send model to the target device\n",
        "model_1.to(device)\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f'Epoch: {epoch}\\n-------')\n",
        "\n",
        "  ## Training\n",
        "\n",
        "  train_step(model=model_1,\n",
        "             data_loader=train_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             accuracy_fn=accuracy_fn,\n",
        "             device=device\n",
        "             )\n",
        "\n",
        "\n",
        "  model_1_results = test_step(model=model_1,\n",
        "                              data_loader=test_dataloader,\n",
        "                              loss_fn=loss_fn,\n",
        "                              accuracy_fn=accuracy_fn,\n",
        "                              device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "20b6cd4eb74f4f52a10af2f4c1f269ed",
            "03818ebefcf148c3833402c3867eb014",
            "a03087d5b7dd4f76b600593862195364",
            "3f14cd2e79914cafad1b73604bb7a12c",
            "07f2202217f345ab8f862a3480f0ee27",
            "ea432312e27741b6b0a4f1c426b2a312",
            "1fda17e0c65a400ab477da4ff7290605",
            "894ecad95bf44fa1ace25b84531bcf63",
            "c0fbfcf3bb0b428fbb5c8325b289e807",
            "98a3c87bcabd4ad4ab9002cc33f97cb9",
            "e5a9555a8e6845ffbd67936b58c24f03"
          ]
        },
        "id": "Eit3Ijd3-RhX",
        "outputId": "bee342b6-f491-42e6-d993-114ff7536369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20b6cd4eb74f4f52a10af2f4c1f269ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Loss: 666.67929 | Train Acc: 154128.12500\n",
            "Train Loss: 666.96599 | Train Acc: 154215.62500\n",
            "Train Loss: 667.41587 | Train Acc: 154306.25000\n",
            "Train Loss: 667.69039 | Train Acc: 154403.12500\n",
            "Train Loss: 668.03138 | Train Acc: 154490.62500\n",
            "Train Loss: 668.48164 | Train Acc: 154571.87500\n",
            "Train Loss: 668.90109 | Train Acc: 154659.37500\n",
            "Train Loss: 669.27680 | Train Acc: 154743.75000\n",
            "Train Loss: 669.69214 | Train Acc: 154821.87500\n",
            "Train Loss: 670.16942 | Train Acc: 154900.00000\n",
            "Train Loss: 670.78333 | Train Acc: 154981.25000\n",
            "Train Loss: 671.40692 | Train Acc: 155056.25000\n",
            "Train Loss: 671.73438 | Train Acc: 155146.87500\n",
            "Train Loss: 671.99609 | Train Acc: 155234.37500\n",
            "Train Loss: 672.21217 | Train Acc: 155328.12500\n",
            "Train Loss: 672.56335 | Train Acc: 155412.50000\n",
            "Train Loss: 672.82938 | Train Acc: 155503.12500\n",
            "Train Loss: 673.10605 | Train Acc: 155590.62500\n",
            "Train Loss: 673.56894 | Train Acc: 155675.00000\n",
            "Train Loss: 673.99045 | Train Acc: 155762.50000\n",
            "Train Loss: 674.47655 | Train Acc: 155846.87500\n",
            "Train Loss: 674.83559 | Train Acc: 155937.50000\n",
            "Train Loss: 675.25350 | Train Acc: 156018.75000\n",
            "Train Loss: 675.46317 | Train Acc: 156112.50000\n",
            "Train Loss: 676.17643 | Train Acc: 156193.75000\n",
            "Train Loss: 676.45293 | Train Acc: 156290.62500\n",
            "Train Loss: 676.79507 | Train Acc: 156381.25000\n",
            "Train Loss: 677.09048 | Train Acc: 156468.75000\n",
            "Train Loss: 677.44880 | Train Acc: 156556.25000\n",
            "Train Loss: 677.78377 | Train Acc: 156643.75000\n",
            "Train Loss: 678.10680 | Train Acc: 156731.25000\n",
            "Train Loss: 678.34786 | Train Acc: 156821.87500\n",
            "Train Loss: 678.67915 | Train Acc: 156909.37500\n",
            "Train Loss: 678.83515 | Train Acc: 157006.25000\n",
            "Train Loss: 679.48728 | Train Acc: 157081.25000\n",
            "Train Loss: 679.90224 | Train Acc: 157162.50000\n",
            "Train Loss: 680.37874 | Train Acc: 157243.75000\n",
            "Train Loss: 680.58825 | Train Acc: 157337.50000\n",
            "Train Loss: 681.00589 | Train Acc: 157421.87500\n",
            "Train Loss: 681.22096 | Train Acc: 157515.62500\n",
            "Train Loss: 681.64651 | Train Acc: 157600.00000\n",
            "Train Loss: 682.02525 | Train Acc: 157687.50000\n",
            "Train Loss: 682.52228 | Train Acc: 157768.75000\n",
            "Train Loss: 682.93589 | Train Acc: 157856.25000\n",
            "Train Loss: 683.33057 | Train Acc: 157943.75000\n",
            "Train Loss: 683.71810 | Train Acc: 158028.12500\n",
            "Train Loss: 684.15520 | Train Acc: 158106.25000\n",
            "Train Loss: 684.93786 | Train Acc: 158184.37500\n",
            "Train Loss: 685.38001 | Train Acc: 158271.87500\n",
            "Train Loss: 685.74572 | Train Acc: 158353.12500\n",
            "Train Loss: 685.89397 | Train Acc: 158446.87500\n",
            "Train Loss: 686.12497 | Train Acc: 158537.50000\n",
            "Train Loss: 686.52868 | Train Acc: 158625.00000\n",
            "Train Loss: 686.97856 | Train Acc: 158709.37500\n",
            "Train Loss: 687.34199 | Train Acc: 158796.87500\n",
            "Train Loss: 687.74909 | Train Acc: 158881.25000\n",
            "Train Loss: 688.03850 | Train Acc: 158968.75000\n",
            "Train Loss: 688.27026 | Train Acc: 159059.37500\n",
            "Train Loss: 688.80227 | Train Acc: 159137.50000\n",
            "Train Loss: 689.05068 | Train Acc: 159225.00000\n",
            "Train Loss: 689.54397 | Train Acc: 159306.25000\n",
            "Train Loss: 689.85651 | Train Acc: 159400.00000\n",
            "Train Loss: 690.38398 | Train Acc: 159484.37500\n",
            "Train Loss: 690.77138 | Train Acc: 159568.75000\n",
            "Train Loss: 691.04153 | Train Acc: 159659.37500\n",
            "Train Loss: 691.52828 | Train Acc: 159743.75000\n",
            "Train Loss: 691.83060 | Train Acc: 159831.25000\n",
            "Train Loss: 692.36559 | Train Acc: 159912.50000\n",
            "Train Loss: 692.54312 | Train Acc: 160006.25000\n",
            "Train Loss: 692.79860 | Train Acc: 160100.00000\n",
            "Train Loss: 693.16561 | Train Acc: 160184.37500\n",
            "Train Loss: 693.48194 | Train Acc: 160271.87500\n",
            "Train Loss: 693.81830 | Train Acc: 160356.25000\n",
            "Train Loss: 694.13862 | Train Acc: 160446.87500\n",
            "Train Loss: 694.46036 | Train Acc: 160531.25000\n",
            "Train Loss: 694.98920 | Train Acc: 160612.50000\n",
            "Train Loss: 695.24515 | Train Acc: 160706.25000\n",
            "Train Loss: 695.44344 | Train Acc: 160800.00000\n",
            "Train Loss: 695.69832 | Train Acc: 160890.62500\n",
            "Train Loss: 695.99372 | Train Acc: 160978.12500\n",
            "Train Loss: 696.49483 | Train Acc: 161062.50000\n",
            "Train Loss: 696.66675 | Train Acc: 161156.25000\n",
            "Train Loss: 697.05639 | Train Acc: 161243.75000\n",
            "Train Loss: 697.38246 | Train Acc: 161337.50000\n",
            "Train Loss: 697.59862 | Train Acc: 161431.25000\n",
            "Train Loss: 698.34614 | Train Acc: 161503.12500\n",
            "Train Loss: 698.71023 | Train Acc: 161590.62500\n",
            "Train Loss: 699.16842 | Train Acc: 161675.00000\n",
            "Train Loss: 699.57568 | Train Acc: 161759.37500\n",
            "Train Loss: 700.26840 | Train Acc: 161843.75000\n",
            "Train Loss: 700.85094 | Train Acc: 161928.12500\n",
            "Train Loss: 701.23579 | Train Acc: 162018.75000\n",
            "Train Loss: 701.43441 | Train Acc: 162115.62500\n",
            "Train Loss: 702.13115 | Train Acc: 162196.87500\n",
            "Train Loss: 702.30697 | Train Acc: 162296.87500\n",
            "Train Loss: 702.57458 | Train Acc: 162387.50000\n",
            "Train Loss: 703.05902 | Train Acc: 162471.87500\n",
            "Train Loss: 703.60893 | Train Acc: 162556.25000\n",
            "Train Loss: 704.04976 | Train Acc: 162631.25000\n",
            "Test_loss:0.42388,Test_accuracy:81.25000\n",
            "Test_loss:0.81328,Test_accuracy:162.50000\n",
            "Test_loss:1.19988,Test_accuracy:250.00000\n",
            "Test_loss:1.41930,Test_accuracy:340.62500\n",
            "Test_loss:1.99119,Test_accuracy:421.87500\n",
            "Test_loss:2.11417,Test_accuracy:521.87500\n",
            "Test_loss:2.35968,Test_accuracy:612.50000\n",
            "Test_loss:2.86005,Test_accuracy:696.87500\n",
            "Test_loss:3.19862,Test_accuracy:781.25000\n",
            "Test_loss:3.55373,Test_accuracy:868.75000\n",
            "Test_loss:4.01304,Test_accuracy:956.25000\n",
            "Test_loss:4.48834,Test_accuracy:1043.75000\n",
            "Test_loss:4.91404,Test_accuracy:1131.25000\n",
            "Test_loss:5.12423,Test_accuracy:1225.00000\n",
            "Test_loss:5.76283,Test_accuracy:1303.12500\n",
            "Test_loss:6.12343,Test_accuracy:1390.62500\n",
            "Test_loss:6.49601,Test_accuracy:1475.00000\n",
            "Test_loss:6.95912,Test_accuracy:1553.12500\n",
            "Test_loss:7.38136,Test_accuracy:1637.50000\n",
            "Test_loss:7.93044,Test_accuracy:1721.87500\n",
            "Test_loss:8.45599,Test_accuracy:1800.00000\n",
            "Test_loss:8.99337,Test_accuracy:1878.12500\n",
            "Test_loss:9.61909,Test_accuracy:1962.50000\n",
            "Test_loss:9.98151,Test_accuracy:2050.00000\n",
            "Test_loss:10.30688,Test_accuracy:2146.87500\n",
            "Test_loss:10.69514,Test_accuracy:2231.25000\n",
            "Test_loss:10.90774,Test_accuracy:2325.00000\n",
            "Test_loss:11.13581,Test_accuracy:2415.62500\n",
            "Test_loss:11.56470,Test_accuracy:2500.00000\n",
            "Test_loss:11.93950,Test_accuracy:2584.37500\n",
            "Test_loss:12.75740,Test_accuracy:2656.25000\n",
            "Test_loss:13.22181,Test_accuracy:2743.75000\n",
            "Test_loss:13.53381,Test_accuracy:2831.25000\n",
            "Test_loss:13.89746,Test_accuracy:2909.37500\n",
            "Test_loss:14.44442,Test_accuracy:2993.75000\n",
            "Test_loss:14.77907,Test_accuracy:3075.00000\n",
            "Test_loss:15.16093,Test_accuracy:3162.50000\n",
            "Test_loss:15.50270,Test_accuracy:3250.00000\n",
            "Test_loss:16.00899,Test_accuracy:3331.25000\n",
            "Test_loss:16.28582,Test_accuracy:3428.12500\n",
            "Test_loss:16.65676,Test_accuracy:3515.62500\n",
            "Test_loss:17.20858,Test_accuracy:3603.12500\n",
            "Test_loss:17.40494,Test_accuracy:3693.75000\n",
            "Test_loss:17.83718,Test_accuracy:3781.25000\n",
            "Test_loss:18.00321,Test_accuracy:3878.12500\n",
            "Test_loss:18.63660,Test_accuracy:3959.37500\n",
            "Test_loss:19.14919,Test_accuracy:4037.50000\n",
            "Test_loss:19.53537,Test_accuracy:4115.62500\n",
            "Test_loss:19.71405,Test_accuracy:4209.37500\n",
            "Test_loss:19.99414,Test_accuracy:4296.87500\n",
            "Test_loss:20.25070,Test_accuracy:4387.50000\n",
            "Test_loss:20.90562,Test_accuracy:4462.50000\n",
            "Test_loss:21.09199,Test_accuracy:4556.25000\n",
            "Test_loss:21.37721,Test_accuracy:4643.75000\n",
            "Test_loss:21.99149,Test_accuracy:4731.25000\n",
            "Test_loss:22.25665,Test_accuracy:4825.00000\n",
            "Test_loss:22.69343,Test_accuracy:4906.25000\n",
            "Test_loss:23.27135,Test_accuracy:4987.50000\n",
            "Test_loss:23.45886,Test_accuracy:5081.25000\n",
            "Test_loss:23.76361,Test_accuracy:5175.00000\n",
            "Test_loss:24.07042,Test_accuracy:5262.50000\n",
            "Test_loss:24.78029,Test_accuracy:5337.50000\n",
            "Test_loss:25.16991,Test_accuracy:5428.12500\n",
            "Test_loss:25.90817,Test_accuracy:5509.37500\n",
            "Test_loss:26.47972,Test_accuracy:5584.37500\n",
            "Test_loss:26.66969,Test_accuracy:5681.25000\n",
            "Test_loss:26.80512,Test_accuracy:5781.25000\n",
            "Test_loss:27.10331,Test_accuracy:5865.62500\n",
            "Test_loss:27.48150,Test_accuracy:5953.12500\n",
            "Test_loss:27.63129,Test_accuracy:6053.12500\n",
            "Test_loss:27.80338,Test_accuracy:6150.00000\n",
            "Test_loss:28.13014,Test_accuracy:6240.62500\n",
            "Test_loss:28.51670,Test_accuracy:6328.12500\n",
            "Test_loss:28.84354,Test_accuracy:6415.62500\n",
            "Test_loss:29.38984,Test_accuracy:6496.87500\n",
            "Test_loss:29.83117,Test_accuracy:6578.12500\n",
            "Test_loss:30.08738,Test_accuracy:6668.75000\n",
            "Test_loss:30.89101,Test_accuracy:6746.87500\n",
            "Test_loss:31.35152,Test_accuracy:6828.12500\n",
            "Test_loss:31.73681,Test_accuracy:6912.50000\n",
            "Test_loss:32.06965,Test_accuracy:6996.87500\n",
            "Test_loss:32.58887,Test_accuracy:7084.37500\n",
            "Test_loss:32.99719,Test_accuracy:7168.75000\n",
            "Test_loss:33.44645,Test_accuracy:7246.87500\n",
            "Test_loss:33.88680,Test_accuracy:7328.12500\n",
            "Test_loss:34.43922,Test_accuracy:7409.37500\n",
            "Test_loss:34.82124,Test_accuracy:7487.50000\n",
            "Test_loss:35.07096,Test_accuracy:7578.12500\n",
            "Test_loss:35.59227,Test_accuracy:7659.37500\n",
            "Test_loss:35.98331,Test_accuracy:7750.00000\n",
            "Test_loss:36.88583,Test_accuracy:7812.50000\n",
            "Test_loss:37.48261,Test_accuracy:7887.50000\n",
            "Test_loss:38.06362,Test_accuracy:7962.50000\n",
            "Test_loss:38.58014,Test_accuracy:8040.62500\n",
            "Test_loss:39.06305,Test_accuracy:8125.00000\n",
            "Test_loss:39.55832,Test_accuracy:8206.25000\n",
            "Test_loss:40.12366,Test_accuracy:8278.12500\n",
            "Test_loss:40.64090,Test_accuracy:8365.62500\n",
            "Test_loss:40.83674,Test_accuracy:8459.37500\n",
            "Test_loss:41.35953,Test_accuracy:8537.50000\n",
            "Test_loss:41.77843,Test_accuracy:8612.50000\n",
            "Test_loss:42.85950,Test_accuracy:8681.25000\n",
            "Test_loss:43.32179,Test_accuracy:8759.37500\n",
            "Test_loss:43.84741,Test_accuracy:8840.62500\n",
            "Test_loss:44.23292,Test_accuracy:8934.37500\n",
            "Test_loss:44.42679,Test_accuracy:9025.00000\n",
            "Test_loss:44.64802,Test_accuracy:9118.75000\n",
            "Test_loss:45.08406,Test_accuracy:9203.12500\n",
            "Test_loss:46.01926,Test_accuracy:9278.12500\n",
            "Test_loss:46.44179,Test_accuracy:9362.50000\n",
            "Test_loss:47.06913,Test_accuracy:9440.62500\n",
            "Test_loss:47.42635,Test_accuracy:9531.25000\n",
            "Test_loss:47.72854,Test_accuracy:9625.00000\n",
            "Test_loss:48.07219,Test_accuracy:9709.37500\n",
            "Test_loss:48.59126,Test_accuracy:9793.75000\n",
            "Test_loss:49.01791,Test_accuracy:9875.00000\n",
            "Test_loss:49.37032,Test_accuracy:9956.25000\n",
            "Test_loss:49.79367,Test_accuracy:10043.75000\n",
            "Test_loss:50.28213,Test_accuracy:10128.12500\n",
            "Test_loss:50.63013,Test_accuracy:10218.75000\n",
            "Test_loss:51.13857,Test_accuracy:10306.25000\n",
            "Test_loss:51.60017,Test_accuracy:10387.50000\n",
            "Test_loss:51.84794,Test_accuracy:10475.00000\n",
            "Test_loss:52.42019,Test_accuracy:10553.12500\n",
            "Test_loss:52.88176,Test_accuracy:10637.50000\n",
            "Test_loss:53.20030,Test_accuracy:10721.87500\n",
            "Test_loss:53.68410,Test_accuracy:10803.12500\n",
            "Test_loss:54.41007,Test_accuracy:10887.50000\n",
            "Test_loss:54.78215,Test_accuracy:10975.00000\n",
            "Test_loss:55.73441,Test_accuracy:11040.62500\n",
            "Test_loss:56.03627,Test_accuracy:11128.12500\n",
            "Test_loss:56.36282,Test_accuracy:11215.62500\n",
            "Test_loss:56.85551,Test_accuracy:11303.12500\n",
            "Test_loss:57.23314,Test_accuracy:11387.50000\n",
            "Test_loss:57.52639,Test_accuracy:11478.12500\n",
            "Test_loss:57.75631,Test_accuracy:11575.00000\n",
            "Test_loss:57.96984,Test_accuracy:11665.62500\n",
            "Test_loss:58.45989,Test_accuracy:11746.87500\n",
            "Test_loss:58.80042,Test_accuracy:11837.50000\n",
            "Test_loss:59.01412,Test_accuracy:11931.25000\n",
            "Test_loss:59.56405,Test_accuracy:12003.12500\n",
            "Test_loss:59.77320,Test_accuracy:12093.75000\n",
            "Test_loss:60.00049,Test_accuracy:12187.50000\n",
            "Test_loss:60.18266,Test_accuracy:12281.25000\n",
            "Test_loss:60.38605,Test_accuracy:12375.00000\n",
            "Test_loss:61.04469,Test_accuracy:12453.12500\n",
            "Test_loss:61.75710,Test_accuracy:12537.50000\n",
            "Test_loss:62.11311,Test_accuracy:12628.12500\n",
            "Test_loss:62.52890,Test_accuracy:12715.62500\n",
            "Test_loss:62.78707,Test_accuracy:12806.25000\n",
            "Test_loss:63.50021,Test_accuracy:12887.50000\n",
            "Test_loss:64.02387,Test_accuracy:12956.25000\n",
            "Test_loss:64.47730,Test_accuracy:13037.50000\n",
            "Test_loss:65.10252,Test_accuracy:13115.62500\n",
            "Test_loss:65.46742,Test_accuracy:13203.12500\n",
            "Test_loss:65.64894,Test_accuracy:13296.87500\n",
            "Test_loss:66.27139,Test_accuracy:13378.12500\n",
            "Test_loss:66.92089,Test_accuracy:13453.12500\n",
            "Test_loss:67.61706,Test_accuracy:13528.12500\n",
            "Test_loss:67.94333,Test_accuracy:13618.75000\n",
            "Test_loss:68.40873,Test_accuracy:13706.25000\n",
            "Test_loss:68.97841,Test_accuracy:13784.37500\n",
            "Test_loss:69.63490,Test_accuracy:13865.62500\n",
            "Test_loss:69.94798,Test_accuracy:13956.25000\n",
            "Test_loss:70.69617,Test_accuracy:14034.37500\n",
            "Test_loss:70.99185,Test_accuracy:14121.87500\n",
            "Test_loss:71.72416,Test_accuracy:14206.25000\n",
            "Test_loss:71.87962,Test_accuracy:14306.25000\n",
            "Test_loss:72.27249,Test_accuracy:14396.87500\n",
            "Test_loss:72.64799,Test_accuracy:14484.37500\n",
            "Test_loss:73.40244,Test_accuracy:14559.37500\n",
            "Test_loss:73.91740,Test_accuracy:14646.87500\n",
            "Test_loss:74.99966,Test_accuracy:14718.75000\n",
            "Test_loss:75.44550,Test_accuracy:14800.00000\n",
            "Test_loss:76.35225,Test_accuracy:14875.00000\n",
            "Test_loss:76.69790,Test_accuracy:14965.62500\n",
            "Test_loss:77.05156,Test_accuracy:15046.87500\n",
            "Test_loss:77.36460,Test_accuracy:15134.37500\n",
            "Test_loss:78.01727,Test_accuracy:15209.37500\n",
            "Test_loss:78.38927,Test_accuracy:15290.62500\n",
            "Test_loss:78.83730,Test_accuracy:15381.25000\n",
            "Test_loss:79.33835,Test_accuracy:15459.37500\n",
            "Test_loss:79.66426,Test_accuracy:15546.87500\n",
            "Test_loss:79.90166,Test_accuracy:15640.62500\n",
            "Test_loss:80.28878,Test_accuracy:15728.12500\n",
            "Test_loss:80.54721,Test_accuracy:15818.75000\n",
            "Test_loss:80.85522,Test_accuracy:15909.37500\n",
            "Test_loss:81.40243,Test_accuracy:15990.62500\n",
            "Test_loss:81.87630,Test_accuracy:16075.00000\n",
            "Test_loss:82.17510,Test_accuracy:16165.62500\n",
            "Test_loss:82.67008,Test_accuracy:16243.75000\n",
            "Test_loss:83.00248,Test_accuracy:16334.37500\n",
            "Test_loss:83.49716,Test_accuracy:16412.50000\n",
            "Test_loss:83.72900,Test_accuracy:16503.12500\n",
            "Test_loss:83.92588,Test_accuracy:16596.87500\n",
            "Test_loss:84.35580,Test_accuracy:16678.12500\n",
            "Test_loss:84.80750,Test_accuracy:16762.50000\n",
            "Test_loss:85.30739,Test_accuracy:16850.00000\n",
            "Test_loss:85.75020,Test_accuracy:16937.50000\n",
            "Test_loss:86.00735,Test_accuracy:17025.00000\n",
            "Test_loss:86.54840,Test_accuracy:17109.37500\n",
            "Test_loss:87.07109,Test_accuracy:17187.50000\n",
            "Test_loss:87.66556,Test_accuracy:17265.62500\n",
            "Test_loss:88.03664,Test_accuracy:17350.00000\n",
            "Test_loss:88.50782,Test_accuracy:17434.37500\n",
            "Test_loss:89.24097,Test_accuracy:17512.50000\n",
            "Test_loss:89.71764,Test_accuracy:17600.00000\n",
            "Test_loss:90.21302,Test_accuracy:17678.12500\n",
            "Test_loss:90.66131,Test_accuracy:17762.50000\n",
            "Test_loss:91.42284,Test_accuracy:17840.62500\n",
            "Test_loss:91.73297,Test_accuracy:17931.25000\n",
            "Test_loss:91.94221,Test_accuracy:18021.87500\n",
            "Test_loss:92.28049,Test_accuracy:18106.25000\n",
            "Test_loss:92.77473,Test_accuracy:18193.75000\n",
            "Test_loss:93.29540,Test_accuracy:18278.12500\n",
            "Test_loss:93.55688,Test_accuracy:18365.62500\n",
            "Test_loss:93.99199,Test_accuracy:18450.00000\n",
            "Test_loss:94.25304,Test_accuracy:18540.62500\n",
            "Test_loss:94.71557,Test_accuracy:18618.75000\n",
            "Test_loss:95.01365,Test_accuracy:18703.12500\n",
            "Test_loss:95.40600,Test_accuracy:18787.50000\n",
            "Test_loss:95.71539,Test_accuracy:18878.12500\n",
            "Test_loss:96.27646,Test_accuracy:18959.37500\n",
            "Test_loss:96.54527,Test_accuracy:19043.75000\n",
            "Test_loss:97.12518,Test_accuracy:19128.12500\n",
            "Test_loss:97.59942,Test_accuracy:19209.37500\n",
            "Test_loss:98.06000,Test_accuracy:19293.75000\n",
            "Test_loss:98.34636,Test_accuracy:19381.25000\n",
            "Test_loss:98.67481,Test_accuracy:19468.75000\n",
            "Test_loss:99.23432,Test_accuracy:19543.75000\n",
            "Test_loss:99.55212,Test_accuracy:19637.50000\n",
            "Test_loss:99.76157,Test_accuracy:19731.25000\n",
            "Test_loss:100.21545,Test_accuracy:19812.50000\n",
            "Test_loss:100.29379,Test_accuracy:19912.50000\n",
            "Test_loss:100.62463,Test_accuracy:20000.00000\n",
            "Test_loss:100.97555,Test_accuracy:20087.50000\n",
            "Test_loss:101.33356,Test_accuracy:20175.00000\n",
            "Test_loss:101.79447,Test_accuracy:20259.37500\n",
            "Test_loss:102.23460,Test_accuracy:20346.87500\n",
            "Test_loss:102.51662,Test_accuracy:20431.25000\n",
            "Test_loss:102.79562,Test_accuracy:20518.75000\n",
            "Test_loss:103.13121,Test_accuracy:20603.12500\n",
            "Test_loss:103.32380,Test_accuracy:20696.87500\n",
            "Test_loss:103.66967,Test_accuracy:20781.25000\n",
            "Test_loss:104.16953,Test_accuracy:20862.50000\n",
            "Test_loss:104.48920,Test_accuracy:20946.87500\n",
            "Test_loss:105.08224,Test_accuracy:21028.12500\n",
            "Test_loss:105.38671,Test_accuracy:21115.62500\n",
            "Test_loss:105.94966,Test_accuracy:21196.87500\n",
            "Test_loss:106.42831,Test_accuracy:21278.12500\n",
            "Test_loss:106.82004,Test_accuracy:21359.37500\n",
            "Test_loss:107.27817,Test_accuracy:21437.50000\n",
            "Test_loss:107.85135,Test_accuracy:21515.62500\n",
            "Test_loss:108.16043,Test_accuracy:21606.25000\n",
            "Test_loss:109.16339,Test_accuracy:21687.50000\n",
            "Test_loss:109.43435,Test_accuracy:21775.00000\n",
            "Test_loss:109.79430,Test_accuracy:21856.25000\n",
            "Test_loss:110.08456,Test_accuracy:21950.00000\n",
            "Test_loss:110.54998,Test_accuracy:22034.37500\n",
            "Test_loss:111.02059,Test_accuracy:22115.62500\n",
            "Test_loss:111.34881,Test_accuracy:22206.25000\n",
            "Test_loss:111.61190,Test_accuracy:22300.00000\n",
            "Test_loss:111.90415,Test_accuracy:22393.75000\n",
            "Test_loss:112.21993,Test_accuracy:22481.25000\n",
            "Test_loss:112.70406,Test_accuracy:22556.25000\n",
            "Test_loss:112.94147,Test_accuracy:22650.00000\n",
            "Test_loss:113.39787,Test_accuracy:22734.37500\n",
            "Test_loss:113.82513,Test_accuracy:22815.62500\n",
            "Test_loss:114.43065,Test_accuracy:22900.00000\n",
            "Test_loss:115.01776,Test_accuracy:22981.25000\n",
            "Test_loss:115.49185,Test_accuracy:23068.75000\n",
            "Test_loss:115.68684,Test_accuracy:23162.50000\n",
            "Test_loss:116.30222,Test_accuracy:23234.37500\n",
            "Test_loss:117.21150,Test_accuracy:23309.37500\n",
            "Test_loss:117.73376,Test_accuracy:23393.75000\n",
            "Test_loss:118.13831,Test_accuracy:23475.00000\n",
            "Test_loss:118.51009,Test_accuracy:23562.50000\n",
            "Test_loss:118.99572,Test_accuracy:23643.75000\n",
            "Test_loss:119.56998,Test_accuracy:23725.00000\n",
            "Test_loss:120.11799,Test_accuracy:23809.37500\n",
            "Test_loss:120.39008,Test_accuracy:23893.75000\n",
            "Test_loss:120.77526,Test_accuracy:23981.25000\n",
            "Test_loss:121.08749,Test_accuracy:24071.87500\n",
            "Test_loss:121.69914,Test_accuracy:24140.62500\n",
            "Test_loss:122.04755,Test_accuracy:24228.12500\n",
            "Test_loss:122.61608,Test_accuracy:24306.25000\n",
            "Test_loss:123.19231,Test_accuracy:24384.37500\n",
            "Test_loss:123.59846,Test_accuracy:24471.87500\n",
            "Test_loss:123.92189,Test_accuracy:24556.25000\n",
            "Test_loss:124.42637,Test_accuracy:24646.87500\n",
            "Test_loss:124.85630,Test_accuracy:24728.12500\n",
            "Test_loss:125.11335,Test_accuracy:24825.00000\n",
            "Test_loss:125.75571,Test_accuracy:24893.75000\n",
            "Test_loss:125.89312,Test_accuracy:24987.50000\n",
            "Test_loss:126.02503,Test_accuracy:25084.37500\n",
            "Test_loss:126.38142,Test_accuracy:25168.75000\n",
            "Test_loss:127.13952,Test_accuracy:25243.75000\n",
            "Test_loss:127.53290,Test_accuracy:25331.25000\n",
            "Test_loss:127.98452,Test_accuracy:25418.75000\n",
            "Test_loss:128.64726,Test_accuracy:25500.00000\n",
            "Test_loss:129.02020,Test_accuracy:25587.50000\n",
            "Test_loss:129.40125,Test_accuracy:25668.75000\n",
            "Test_loss:129.88353,Test_accuracy:25750.00000\n",
            "Test_loss:130.12556,Test_accuracy:25843.75000\n",
            "Test_loss:130.34769,Test_accuracy:25937.50000\n",
            "Test_loss:130.56549,Test_accuracy:26034.37500\n",
            "Test_loss:130.84608,Test_accuracy:26125.00000\n",
            "Test_loss:131.03308,Test_accuracy:26215.62500\n",
            "Test_loss:131.55084,Test_accuracy:26293.75000\n",
            "Test_loss:131.93533,Test_accuracy:26375.00000\n",
            "Test_loss:132.53575,Test_accuracy:26459.37500\n",
            "Test_loss:133.24615,Test_accuracy:26531.25000\n",
            "Test_loss:133.62476,Test_accuracy:26618.75000\n",
            "Epoch: 7\n",
            "-------\n",
            "Train Loss: 0.26800 | Train Acc: 90.62500\n",
            "Train Loss: 0.46300 | Train Acc: 184.37500\n",
            "Train Loss: 0.92344 | Train Acc: 271.87500\n",
            "Train Loss: 1.39959 | Train Acc: 350.00000\n",
            "Train Loss: 1.67696 | Train Acc: 440.62500\n",
            "Train Loss: 2.00301 | Train Acc: 528.12500\n",
            "Train Loss: 2.33939 | Train Acc: 615.62500\n",
            "Train Loss: 2.61309 | Train Acc: 709.37500\n",
            "Train Loss: 2.83660 | Train Acc: 806.25000\n",
            "Train Loss: 3.02199 | Train Acc: 900.00000\n",
            "Train Loss: 3.47551 | Train Acc: 990.62500\n",
            "Train Loss: 3.81332 | Train Acc: 1078.12500\n",
            "Train Loss: 4.14535 | Train Acc: 1165.62500\n",
            "Train Loss: 4.66789 | Train Acc: 1243.75000\n",
            "Train Loss: 4.99092 | Train Acc: 1334.37500\n",
            "Train Loss: 5.56373 | Train Acc: 1415.62500\n",
            "Train Loss: 6.00113 | Train Acc: 1496.87500\n",
            "Train Loss: 6.24855 | Train Acc: 1590.62500\n",
            "Train Loss: 6.60839 | Train Acc: 1675.00000\n",
            "Train Loss: 6.80246 | Train Acc: 1768.75000\n",
            "Train Loss: 7.09374 | Train Acc: 1856.25000\n",
            "Train Loss: 7.29961 | Train Acc: 1946.87500\n",
            "Train Loss: 7.75713 | Train Acc: 2028.12500\n",
            "Train Loss: 8.03099 | Train Acc: 2115.62500\n",
            "Train Loss: 8.38121 | Train Acc: 2206.25000\n",
            "Train Loss: 8.79115 | Train Acc: 2287.50000\n",
            "Train Loss: 9.14412 | Train Acc: 2375.00000\n",
            "Train Loss: 9.44357 | Train Acc: 2456.25000\n",
            "Train Loss: 10.08641 | Train Acc: 2537.50000\n",
            "Train Loss: 10.39891 | Train Acc: 2631.25000\n",
            "Train Loss: 10.93455 | Train Acc: 2715.62500\n",
            "Train Loss: 11.25939 | Train Acc: 2809.37500\n",
            "Train Loss: 11.60928 | Train Acc: 2903.12500\n",
            "Train Loss: 12.03621 | Train Acc: 2987.50000\n",
            "Train Loss: 12.40650 | Train Acc: 3078.12500\n",
            "Train Loss: 13.01461 | Train Acc: 3156.25000\n",
            "Train Loss: 13.30742 | Train Acc: 3243.75000\n",
            "Train Loss: 13.48032 | Train Acc: 3340.62500\n",
            "Train Loss: 13.71019 | Train Acc: 3431.25000\n",
            "Train Loss: 14.14968 | Train Acc: 3515.62500\n",
            "Train Loss: 14.68075 | Train Acc: 3596.87500\n",
            "Train Loss: 15.08650 | Train Acc: 3681.25000\n",
            "Train Loss: 15.33200 | Train Acc: 3775.00000\n",
            "Train Loss: 15.65766 | Train Acc: 3865.62500\n",
            "Train Loss: 15.98447 | Train Acc: 3950.00000\n",
            "Train Loss: 16.36709 | Train Acc: 4037.50000\n",
            "Train Loss: 16.81392 | Train Acc: 4121.87500\n",
            "Train Loss: 17.29477 | Train Acc: 4209.37500\n",
            "Train Loss: 18.06918 | Train Acc: 4287.50000\n",
            "Train Loss: 18.51868 | Train Acc: 4368.75000\n",
            "Train Loss: 19.02389 | Train Acc: 4450.00000\n",
            "Train Loss: 19.38092 | Train Acc: 4540.62500\n",
            "Train Loss: 19.73514 | Train Acc: 4631.25000\n",
            "Train Loss: 20.17235 | Train Acc: 4712.50000\n",
            "Train Loss: 20.36168 | Train Acc: 4803.12500\n",
            "Train Loss: 20.98988 | Train Acc: 4884.37500\n",
            "Train Loss: 21.53316 | Train Acc: 4962.50000\n",
            "Train Loss: 21.95664 | Train Acc: 5050.00000\n",
            "Train Loss: 22.17314 | Train Acc: 5143.75000\n",
            "Train Loss: 22.68529 | Train Acc: 5221.87500\n",
            "Train Loss: 23.18280 | Train Acc: 5300.00000\n",
            "Train Loss: 23.83660 | Train Acc: 5378.12500\n",
            "Train Loss: 24.18068 | Train Acc: 5459.37500\n",
            "Train Loss: 24.54175 | Train Acc: 5543.75000\n",
            "Train Loss: 24.80881 | Train Acc: 5634.37500\n",
            "Train Loss: 25.29353 | Train Acc: 5721.87500\n",
            "Train Loss: 25.88542 | Train Acc: 5800.00000\n",
            "Train Loss: 26.23563 | Train Acc: 5884.37500\n",
            "Train Loss: 26.56381 | Train Acc: 5975.00000\n",
            "Train Loss: 26.89318 | Train Acc: 6059.37500\n",
            "Train Loss: 27.22976 | Train Acc: 6150.00000\n",
            "Train Loss: 27.63700 | Train Acc: 6237.50000\n",
            "Train Loss: 28.05911 | Train Acc: 6325.00000\n",
            "Train Loss: 28.23044 | Train Acc: 6421.87500\n",
            "Train Loss: 28.36557 | Train Acc: 6518.75000\n",
            "Train Loss: 28.61954 | Train Acc: 6612.50000\n",
            "Train Loss: 28.89330 | Train Acc: 6706.25000\n",
            "Train Loss: 29.09634 | Train Acc: 6796.87500\n",
            "Train Loss: 29.32295 | Train Acc: 6887.50000\n",
            "Train Loss: 29.68219 | Train Acc: 6971.87500\n",
            "Train Loss: 29.91024 | Train Acc: 7065.62500\n",
            "Train Loss: 30.45783 | Train Acc: 7146.87500\n",
            "Train Loss: 30.73646 | Train Acc: 7237.50000\n",
            "Train Loss: 31.23760 | Train Acc: 7321.87500\n",
            "Train Loss: 31.72970 | Train Acc: 7403.12500\n",
            "Train Loss: 32.33692 | Train Acc: 7490.62500\n",
            "Train Loss: 32.51386 | Train Acc: 7584.37500\n",
            "Train Loss: 32.79415 | Train Acc: 7665.62500\n",
            "Train Loss: 33.24651 | Train Acc: 7750.00000\n",
            "Train Loss: 33.72179 | Train Acc: 7840.62500\n",
            "Train Loss: 33.86741 | Train Acc: 7934.37500\n",
            "Train Loss: 34.31840 | Train Acc: 8018.75000\n",
            "Train Loss: 34.59504 | Train Acc: 8109.37500\n",
            "Train Loss: 34.95162 | Train Acc: 8200.00000\n",
            "Train Loss: 35.16162 | Train Acc: 8290.62500\n",
            "Train Loss: 35.65373 | Train Acc: 8368.75000\n",
            "Train Loss: 35.92275 | Train Acc: 8456.25000\n",
            "Train Loss: 36.31679 | Train Acc: 8543.75000\n",
            "Train Loss: 36.64445 | Train Acc: 8634.37500\n",
            "Train Loss: 37.33343 | Train Acc: 8709.37500\n",
            "Train Loss: 37.59824 | Train Acc: 8796.87500\n",
            "Train Loss: 37.72927 | Train Acc: 8893.75000\n",
            "Train Loss: 37.90655 | Train Acc: 8990.62500\n",
            "Train Loss: 38.15113 | Train Acc: 9087.50000\n",
            "Train Loss: 38.55920 | Train Acc: 9171.87500\n",
            "Train Loss: 38.86456 | Train Acc: 9265.62500\n",
            "Train Loss: 39.22414 | Train Acc: 9353.12500\n",
            "Train Loss: 39.34108 | Train Acc: 9446.87500\n",
            "Train Loss: 39.82719 | Train Acc: 9531.25000\n",
            "Train Loss: 40.00746 | Train Acc: 9628.12500\n",
            "Train Loss: 40.42605 | Train Acc: 9715.62500\n",
            "Train Loss: 40.56922 | Train Acc: 9812.50000\n",
            "Train Loss: 41.06216 | Train Acc: 9893.75000\n",
            "Train Loss: 41.48400 | Train Acc: 9984.37500\n",
            "Train Loss: 41.79161 | Train Acc: 10078.12500\n",
            "Train Loss: 42.03322 | Train Acc: 10165.62500\n",
            "Train Loss: 42.48711 | Train Acc: 10246.87500\n",
            "Train Loss: 42.70005 | Train Acc: 10337.50000\n",
            "Train Loss: 43.02203 | Train Acc: 10428.12500\n",
            "Train Loss: 43.17136 | Train Acc: 10525.00000\n",
            "Train Loss: 43.76451 | Train Acc: 10606.25000\n",
            "Train Loss: 44.08237 | Train Acc: 10693.75000\n",
            "Train Loss: 44.35506 | Train Acc: 10781.25000\n",
            "Train Loss: 44.65227 | Train Acc: 10871.87500\n",
            "Train Loss: 44.93750 | Train Acc: 10959.37500\n",
            "Train Loss: 45.31745 | Train Acc: 11043.75000\n",
            "Train Loss: 45.64136 | Train Acc: 11128.12500\n",
            "Train Loss: 46.12054 | Train Acc: 11218.75000\n",
            "Train Loss: 46.23124 | Train Acc: 11318.75000\n",
            "Train Loss: 46.63307 | Train Acc: 11409.37500\n",
            "Train Loss: 46.87433 | Train Acc: 11493.75000\n",
            "Train Loss: 47.12294 | Train Acc: 11587.50000\n",
            "Train Loss: 47.54616 | Train Acc: 11671.87500\n",
            "Train Loss: 48.05951 | Train Acc: 11759.37500\n",
            "Train Loss: 48.35204 | Train Acc: 11846.87500\n",
            "Train Loss: 48.85533 | Train Acc: 11931.25000\n",
            "Train Loss: 49.43555 | Train Acc: 12012.50000\n",
            "Train Loss: 49.66605 | Train Acc: 12103.12500\n",
            "Train Loss: 49.78774 | Train Acc: 12200.00000\n",
            "Train Loss: 50.02388 | Train Acc: 12290.62500\n",
            "Train Loss: 50.43869 | Train Acc: 12375.00000\n",
            "Train Loss: 51.09740 | Train Acc: 12462.50000\n",
            "Train Loss: 51.36158 | Train Acc: 12550.00000\n",
            "Train Loss: 51.85263 | Train Acc: 12631.25000\n",
            "Train Loss: 52.41944 | Train Acc: 12712.50000\n",
            "Train Loss: 52.74850 | Train Acc: 12806.25000\n",
            "Train Loss: 52.98982 | Train Acc: 12896.87500\n",
            "Train Loss: 53.53940 | Train Acc: 12981.25000\n",
            "Train Loss: 53.81426 | Train Acc: 13071.87500\n",
            "Train Loss: 54.03629 | Train Acc: 13162.50000\n",
            "Train Loss: 54.44326 | Train Acc: 13243.75000\n",
            "Train Loss: 54.63685 | Train Acc: 13337.50000\n",
            "Train Loss: 54.83634 | Train Acc: 13428.12500\n",
            "Train Loss: 55.15379 | Train Acc: 13515.62500\n",
            "Train Loss: 55.46508 | Train Acc: 13600.00000\n",
            "Train Loss: 55.98860 | Train Acc: 13687.50000\n",
            "Train Loss: 56.53190 | Train Acc: 13765.62500\n",
            "Train Loss: 56.87929 | Train Acc: 13853.12500\n",
            "Train Loss: 57.17879 | Train Acc: 13940.62500\n",
            "Train Loss: 57.39520 | Train Acc: 14034.37500\n",
            "Train Loss: 57.96813 | Train Acc: 14106.25000\n",
            "Train Loss: 58.32597 | Train Acc: 14196.87500\n",
            "Train Loss: 58.74016 | Train Acc: 14281.25000\n",
            "Train Loss: 58.94267 | Train Acc: 14375.00000\n",
            "Train Loss: 59.27155 | Train Acc: 14462.50000\n",
            "Train Loss: 59.56806 | Train Acc: 14546.87500\n",
            "Train Loss: 59.88992 | Train Acc: 14637.50000\n",
            "Train Loss: 60.43581 | Train Acc: 14721.87500\n",
            "Train Loss: 60.64972 | Train Acc: 14815.62500\n",
            "Train Loss: 61.02844 | Train Acc: 14906.25000\n",
            "Train Loss: 61.40144 | Train Acc: 14990.62500\n",
            "Train Loss: 61.69858 | Train Acc: 15078.12500\n",
            "Train Loss: 61.94978 | Train Acc: 15171.87500\n",
            "Train Loss: 62.30432 | Train Acc: 15256.25000\n",
            "Train Loss: 62.47305 | Train Acc: 15353.12500\n",
            "Train Loss: 62.95311 | Train Acc: 15434.37500\n",
            "Train Loss: 63.33680 | Train Acc: 15515.62500\n",
            "Train Loss: 63.61969 | Train Acc: 15606.25000\n",
            "Train Loss: 64.36094 | Train Acc: 15687.50000\n",
            "Train Loss: 64.82992 | Train Acc: 15771.87500\n",
            "Train Loss: 65.19053 | Train Acc: 15862.50000\n",
            "Train Loss: 65.59486 | Train Acc: 15953.12500\n",
            "Train Loss: 66.31335 | Train Acc: 16028.12500\n",
            "Train Loss: 66.60797 | Train Acc: 16112.50000\n",
            "Train Loss: 67.10976 | Train Acc: 16196.87500\n",
            "Train Loss: 67.55787 | Train Acc: 16281.25000\n",
            "Train Loss: 67.97475 | Train Acc: 16368.75000\n",
            "Train Loss: 68.40697 | Train Acc: 16450.00000\n",
            "Train Loss: 68.56785 | Train Acc: 16546.87500\n",
            "Train Loss: 69.00482 | Train Acc: 16628.12500\n",
            "Train Loss: 69.65595 | Train Acc: 16712.50000\n",
            "Train Loss: 70.31118 | Train Acc: 16796.87500\n",
            "Train Loss: 70.45642 | Train Acc: 16893.75000\n",
            "Train Loss: 70.97227 | Train Acc: 16978.12500\n",
            "Train Loss: 71.38501 | Train Acc: 17062.50000\n",
            "Train Loss: 71.64238 | Train Acc: 17153.12500\n",
            "Train Loss: 71.83881 | Train Acc: 17246.87500\n",
            "Train Loss: 72.38874 | Train Acc: 17328.12500\n",
            "Train Loss: 72.66248 | Train Acc: 17421.87500\n",
            "Train Loss: 72.92045 | Train Acc: 17515.62500\n",
            "Train Loss: 73.18000 | Train Acc: 17603.12500\n",
            "Train Loss: 73.41079 | Train Acc: 17693.75000\n",
            "Train Loss: 73.72494 | Train Acc: 17781.25000\n",
            "Train Loss: 74.10229 | Train Acc: 17871.87500\n",
            "Train Loss: 74.41312 | Train Acc: 17962.50000\n",
            "Train Loss: 74.68248 | Train Acc: 18050.00000\n",
            "Train Loss: 75.09430 | Train Acc: 18134.37500\n",
            "Train Loss: 75.57103 | Train Acc: 18221.87500\n",
            "Train Loss: 76.13337 | Train Acc: 18296.87500\n",
            "Train Loss: 76.45626 | Train Acc: 18384.37500\n",
            "Train Loss: 76.71864 | Train Acc: 18475.00000\n",
            "Train Loss: 76.97366 | Train Acc: 18562.50000\n",
            "Train Loss: 77.31811 | Train Acc: 18650.00000\n",
            "Train Loss: 77.54169 | Train Acc: 18740.62500\n",
            "Train Loss: 78.05702 | Train Acc: 18825.00000\n",
            "Train Loss: 78.63650 | Train Acc: 18906.25000\n",
            "Train Loss: 78.77633 | Train Acc: 19006.25000\n",
            "Train Loss: 79.29583 | Train Acc: 19087.50000\n",
            "Train Loss: 79.84637 | Train Acc: 19165.62500\n",
            "Train Loss: 80.16132 | Train Acc: 19256.25000\n",
            "Train Loss: 80.51969 | Train Acc: 19343.75000\n",
            "Train Loss: 80.77967 | Train Acc: 19431.25000\n",
            "Train Loss: 81.21411 | Train Acc: 19512.50000\n",
            "Train Loss: 81.46531 | Train Acc: 19600.00000\n",
            "Train Loss: 81.87171 | Train Acc: 19678.12500\n",
            "Train Loss: 82.18955 | Train Acc: 19771.87500\n",
            "Train Loss: 82.43813 | Train Acc: 19862.50000\n",
            "Train Loss: 83.01189 | Train Acc: 19950.00000\n",
            "Train Loss: 83.44856 | Train Acc: 20031.25000\n",
            "Train Loss: 83.80906 | Train Acc: 20115.62500\n",
            "Train Loss: 84.29301 | Train Acc: 20200.00000\n",
            "Train Loss: 84.60685 | Train Acc: 20293.75000\n",
            "Train Loss: 85.07454 | Train Acc: 20371.87500\n",
            "Train Loss: 85.30327 | Train Acc: 20462.50000\n",
            "Train Loss: 85.66956 | Train Acc: 20550.00000\n",
            "Train Loss: 86.08165 | Train Acc: 20634.37500\n",
            "Train Loss: 86.52090 | Train Acc: 20715.62500\n",
            "Train Loss: 86.78806 | Train Acc: 20809.37500\n",
            "Train Loss: 87.09199 | Train Acc: 20900.00000\n",
            "Train Loss: 87.34775 | Train Acc: 20990.62500\n",
            "Train Loss: 87.61513 | Train Acc: 21081.25000\n",
            "Train Loss: 88.07130 | Train Acc: 21165.62500\n",
            "Train Loss: 88.45732 | Train Acc: 21250.00000\n",
            "Train Loss: 88.77627 | Train Acc: 21337.50000\n",
            "Train Loss: 89.01139 | Train Acc: 21434.37500\n",
            "Train Loss: 89.24565 | Train Acc: 21518.75000\n",
            "Train Loss: 89.58127 | Train Acc: 21606.25000\n",
            "Train Loss: 89.86468 | Train Acc: 21696.87500\n",
            "Train Loss: 90.33642 | Train Acc: 21778.12500\n",
            "Train Loss: 90.59386 | Train Acc: 21865.62500\n",
            "Train Loss: 90.80364 | Train Acc: 21962.50000\n",
            "Train Loss: 91.11503 | Train Acc: 22053.12500\n",
            "Train Loss: 91.55296 | Train Acc: 22143.75000\n",
            "Train Loss: 92.01359 | Train Acc: 22228.12500\n",
            "Train Loss: 92.55243 | Train Acc: 22303.12500\n",
            "Train Loss: 93.08514 | Train Acc: 22381.25000\n",
            "Train Loss: 93.36402 | Train Acc: 22475.00000\n",
            "Train Loss: 93.66110 | Train Acc: 22568.75000\n",
            "Train Loss: 94.08228 | Train Acc: 22653.12500\n",
            "Train Loss: 94.41339 | Train Acc: 22740.62500\n",
            "Train Loss: 94.82607 | Train Acc: 22818.75000\n",
            "Train Loss: 94.96924 | Train Acc: 22918.75000\n",
            "Train Loss: 95.51820 | Train Acc: 22996.87500\n",
            "Train Loss: 95.90031 | Train Acc: 23084.37500\n",
            "Train Loss: 96.18982 | Train Acc: 23171.87500\n",
            "Train Loss: 96.63235 | Train Acc: 23253.12500\n",
            "Train Loss: 96.93428 | Train Acc: 23340.62500\n",
            "Train Loss: 97.28536 | Train Acc: 23431.25000\n",
            "Train Loss: 97.41067 | Train Acc: 23525.00000\n",
            "Train Loss: 97.72488 | Train Acc: 23612.50000\n",
            "Train Loss: 98.13128 | Train Acc: 23703.12500\n",
            "Train Loss: 98.75076 | Train Acc: 23787.50000\n",
            "Train Loss: 98.97822 | Train Acc: 23881.25000\n",
            "Train Loss: 99.17337 | Train Acc: 23975.00000\n",
            "Train Loss: 99.53996 | Train Acc: 24062.50000\n",
            "Train Loss: 99.76526 | Train Acc: 24153.12500\n",
            "Train Loss: 100.39249 | Train Acc: 24231.25000\n",
            "Train Loss: 100.63627 | Train Acc: 24318.75000\n",
            "Train Loss: 100.93359 | Train Acc: 24412.50000\n",
            "Train Loss: 101.28905 | Train Acc: 24500.00000\n",
            "Train Loss: 101.49779 | Train Acc: 24596.87500\n",
            "Train Loss: 101.87989 | Train Acc: 24675.00000\n",
            "Train Loss: 102.05679 | Train Acc: 24768.75000\n",
            "Train Loss: 102.51345 | Train Acc: 24850.00000\n",
            "Train Loss: 102.82666 | Train Acc: 24937.50000\n",
            "Train Loss: 103.63826 | Train Acc: 25018.75000\n",
            "Train Loss: 103.99440 | Train Acc: 25103.12500\n",
            "Train Loss: 104.38865 | Train Acc: 25193.75000\n",
            "Train Loss: 104.71375 | Train Acc: 25290.62500\n",
            "Train Loss: 105.03460 | Train Acc: 25381.25000\n",
            "Train Loss: 105.20728 | Train Acc: 25475.00000\n",
            "Train Loss: 106.12847 | Train Acc: 25540.62500\n",
            "Train Loss: 106.43526 | Train Acc: 25634.37500\n",
            "Train Loss: 106.76471 | Train Acc: 25721.87500\n",
            "Train Loss: 107.22292 | Train Acc: 25812.50000\n",
            "Train Loss: 107.39936 | Train Acc: 25906.25000\n",
            "Train Loss: 107.56073 | Train Acc: 26003.12500\n",
            "Train Loss: 107.74483 | Train Acc: 26093.75000\n",
            "Train Loss: 108.01923 | Train Acc: 26178.12500\n",
            "Train Loss: 108.54707 | Train Acc: 26259.37500\n",
            "Train Loss: 108.83214 | Train Acc: 26350.00000\n",
            "Train Loss: 109.47044 | Train Acc: 26431.25000\n",
            "Train Loss: 109.71737 | Train Acc: 26521.87500\n",
            "Train Loss: 110.03061 | Train Acc: 26612.50000\n",
            "Train Loss: 110.24449 | Train Acc: 26706.25000\n",
            "Train Loss: 110.41772 | Train Acc: 26800.00000\n",
            "Train Loss: 110.97506 | Train Acc: 26887.50000\n",
            "Train Loss: 111.22858 | Train Acc: 26975.00000\n",
            "Train Loss: 111.64708 | Train Acc: 27053.12500\n",
            "Train Loss: 112.21410 | Train Acc: 27137.50000\n",
            "Train Loss: 112.69123 | Train Acc: 27218.75000\n",
            "Train Loss: 113.04318 | Train Acc: 27306.25000\n",
            "Train Loss: 113.43380 | Train Acc: 27387.50000\n",
            "Train Loss: 113.77500 | Train Acc: 27478.12500\n",
            "Train Loss: 114.26953 | Train Acc: 27568.75000\n",
            "Train Loss: 114.75648 | Train Acc: 27656.25000\n",
            "Train Loss: 115.24992 | Train Acc: 27743.75000\n",
            "Train Loss: 115.65441 | Train Acc: 27828.12500\n",
            "Train Loss: 116.01853 | Train Acc: 27915.62500\n",
            "Train Loss: 116.34019 | Train Acc: 28006.25000\n",
            "Train Loss: 116.61462 | Train Acc: 28096.87500\n",
            "Train Loss: 117.17421 | Train Acc: 28175.00000\n",
            "Train Loss: 117.51464 | Train Acc: 28265.62500\n",
            "Train Loss: 117.83211 | Train Acc: 28359.37500\n",
            "Train Loss: 118.04775 | Train Acc: 28456.25000\n",
            "Train Loss: 118.31762 | Train Acc: 28540.62500\n",
            "Train Loss: 118.52482 | Train Acc: 28637.50000\n",
            "Train Loss: 118.73122 | Train Acc: 28731.25000\n",
            "Train Loss: 119.05906 | Train Acc: 28818.75000\n",
            "Train Loss: 119.42772 | Train Acc: 28909.37500\n",
            "Train Loss: 120.16357 | Train Acc: 28984.37500\n",
            "Train Loss: 120.41564 | Train Acc: 29071.87500\n",
            "Train Loss: 120.85915 | Train Acc: 29153.12500\n",
            "Train Loss: 121.21432 | Train Acc: 29240.62500\n",
            "Train Loss: 121.70606 | Train Acc: 29318.75000\n",
            "Train Loss: 121.98889 | Train Acc: 29403.12500\n",
            "Train Loss: 122.53844 | Train Acc: 29475.00000\n",
            "Train Loss: 123.02214 | Train Acc: 29556.25000\n",
            "Train Loss: 123.27607 | Train Acc: 29646.87500\n",
            "Train Loss: 123.66896 | Train Acc: 29731.25000\n",
            "Train Loss: 124.33657 | Train Acc: 29812.50000\n",
            "Train Loss: 124.68415 | Train Acc: 29900.00000\n",
            "Train Loss: 125.05650 | Train Acc: 29984.37500\n",
            "Train Loss: 125.63642 | Train Acc: 30062.50000\n",
            "Train Loss: 125.99916 | Train Acc: 30146.87500\n",
            "Train Loss: 126.41095 | Train Acc: 30228.12500\n",
            "Train Loss: 126.90391 | Train Acc: 30309.37500\n",
            "Train Loss: 127.07721 | Train Acc: 30403.12500\n",
            "Train Loss: 127.47641 | Train Acc: 30490.62500\n",
            "Train Loss: 127.87180 | Train Acc: 30568.75000\n",
            "Train Loss: 128.36526 | Train Acc: 30650.00000\n",
            "Train Loss: 128.69286 | Train Acc: 30737.50000\n",
            "Train Loss: 128.97905 | Train Acc: 30825.00000\n",
            "Train Loss: 129.34213 | Train Acc: 30912.50000\n",
            "Train Loss: 129.84694 | Train Acc: 30996.87500\n",
            "Train Loss: 130.03434 | Train Acc: 31087.50000\n",
            "Train Loss: 130.44227 | Train Acc: 31175.00000\n",
            "Train Loss: 131.02257 | Train Acc: 31250.00000\n",
            "Train Loss: 131.49031 | Train Acc: 31331.25000\n",
            "Train Loss: 132.06139 | Train Acc: 31415.62500\n",
            "Train Loss: 132.38622 | Train Acc: 31509.37500\n",
            "Train Loss: 132.64409 | Train Acc: 31600.00000\n",
            "Train Loss: 132.83839 | Train Acc: 31690.62500\n",
            "Train Loss: 133.14998 | Train Acc: 31778.12500\n",
            "Train Loss: 133.42124 | Train Acc: 31865.62500\n",
            "Train Loss: 133.86798 | Train Acc: 31953.12500\n",
            "Train Loss: 134.24913 | Train Acc: 32040.62500\n",
            "Train Loss: 134.60943 | Train Acc: 32121.87500\n",
            "Train Loss: 134.88459 | Train Acc: 32209.37500\n",
            "Train Loss: 135.17902 | Train Acc: 32296.87500\n",
            "Train Loss: 135.45165 | Train Acc: 32390.62500\n",
            "Train Loss: 135.67731 | Train Acc: 32481.25000\n",
            "Train Loss: 135.80240 | Train Acc: 32581.25000\n",
            "Train Loss: 136.14083 | Train Acc: 32665.62500\n",
            "Train Loss: 136.50225 | Train Acc: 32756.25000\n",
            "Train Loss: 136.73597 | Train Acc: 32850.00000\n",
            "Train Loss: 137.03622 | Train Acc: 32943.75000\n",
            "Train Loss: 137.33499 | Train Acc: 33031.25000\n",
            "Train Loss: 137.67884 | Train Acc: 33121.87500\n",
            "Train Loss: 138.29281 | Train Acc: 33200.00000\n",
            "Train Loss: 138.51239 | Train Acc: 33293.75000\n",
            "Train Loss: 138.80985 | Train Acc: 33381.25000\n",
            "Train Loss: 139.12799 | Train Acc: 33462.50000\n",
            "Train Loss: 139.46018 | Train Acc: 33550.00000\n",
            "Train Loss: 139.61036 | Train Acc: 33646.87500\n",
            "Train Loss: 140.08722 | Train Acc: 33728.12500\n",
            "Train Loss: 140.37180 | Train Acc: 33815.62500\n",
            "Train Loss: 140.70002 | Train Acc: 33903.12500\n",
            "Train Loss: 141.05245 | Train Acc: 33993.75000\n",
            "Train Loss: 141.40356 | Train Acc: 34081.25000\n",
            "Train Loss: 142.00493 | Train Acc: 34159.37500\n",
            "Train Loss: 142.36211 | Train Acc: 34243.75000\n",
            "Train Loss: 142.79117 | Train Acc: 34318.75000\n",
            "Train Loss: 143.25717 | Train Acc: 34403.12500\n",
            "Train Loss: 143.68507 | Train Acc: 34493.75000\n",
            "Train Loss: 143.98007 | Train Acc: 34581.25000\n",
            "Train Loss: 144.25009 | Train Acc: 34675.00000\n",
            "Train Loss: 144.47749 | Train Acc: 34768.75000\n",
            "Train Loss: 144.84149 | Train Acc: 34859.37500\n",
            "Train Loss: 145.27147 | Train Acc: 34943.75000\n",
            "Train Loss: 145.55389 | Train Acc: 35031.25000\n",
            "Train Loss: 145.91419 | Train Acc: 35121.87500\n",
            "Train Loss: 146.50950 | Train Acc: 35206.25000\n",
            "Train Loss: 146.71202 | Train Acc: 35296.87500\n",
            "Train Loss: 147.20210 | Train Acc: 35384.37500\n",
            "Train Loss: 147.47407 | Train Acc: 35475.00000\n",
            "Train Loss: 147.79307 | Train Acc: 35556.25000\n",
            "Train Loss: 148.44684 | Train Acc: 35631.25000\n",
            "Train Loss: 148.82191 | Train Acc: 35712.50000\n",
            "Train Loss: 149.23451 | Train Acc: 35790.62500\n",
            "Train Loss: 149.47691 | Train Acc: 35878.12500\n",
            "Train Loss: 149.72585 | Train Acc: 35965.62500\n",
            "Train Loss: 150.07601 | Train Acc: 36043.75000\n",
            "Train Loss: 150.45966 | Train Acc: 36137.50000\n",
            "Train Loss: 150.59178 | Train Acc: 36234.37500\n",
            "Train Loss: 150.92161 | Train Acc: 36321.87500\n",
            "Train Loss: 151.25024 | Train Acc: 36409.37500\n",
            "Train Loss: 151.64615 | Train Acc: 36493.75000\n",
            "Train Loss: 151.93411 | Train Acc: 36581.25000\n",
            "Train Loss: 152.08891 | Train Acc: 36681.25000\n",
            "Train Loss: 152.28723 | Train Acc: 36775.00000\n",
            "Train Loss: 152.40903 | Train Acc: 36875.00000\n",
            "Train Loss: 152.64186 | Train Acc: 36968.75000\n",
            "Train Loss: 153.24038 | Train Acc: 37050.00000\n",
            "Train Loss: 153.82985 | Train Acc: 37125.00000\n",
            "Train Loss: 154.10301 | Train Acc: 37215.62500\n",
            "Train Loss: 154.54846 | Train Acc: 37303.12500\n",
            "Train Loss: 155.02429 | Train Acc: 37387.50000\n",
            "Train Loss: 155.50675 | Train Acc: 37471.87500\n",
            "Train Loss: 155.91230 | Train Acc: 37562.50000\n",
            "Train Loss: 156.65846 | Train Acc: 37640.62500\n",
            "Train Loss: 157.11939 | Train Acc: 37728.12500\n",
            "Train Loss: 157.77040 | Train Acc: 37806.25000\n",
            "Train Loss: 158.26318 | Train Acc: 37881.25000\n",
            "Train Loss: 158.75133 | Train Acc: 37965.62500\n",
            "Train Loss: 159.00797 | Train Acc: 38050.00000\n",
            "Train Loss: 159.44170 | Train Acc: 38140.62500\n",
            "Train Loss: 159.78933 | Train Acc: 38231.25000\n",
            "Train Loss: 160.12811 | Train Acc: 38315.62500\n",
            "Train Loss: 160.28723 | Train Acc: 38406.25000\n",
            "Train Loss: 160.91680 | Train Acc: 38487.50000\n",
            "Train Loss: 161.34132 | Train Acc: 38571.87500\n",
            "Train Loss: 161.93274 | Train Acc: 38653.12500\n",
            "Train Loss: 162.45152 | Train Acc: 38734.37500\n",
            "Train Loss: 162.84620 | Train Acc: 38818.75000\n",
            "Train Loss: 163.15603 | Train Acc: 38906.25000\n",
            "Train Loss: 163.44355 | Train Acc: 38993.75000\n",
            "Train Loss: 163.77451 | Train Acc: 39087.50000\n",
            "Train Loss: 164.20566 | Train Acc: 39165.62500\n",
            "Train Loss: 164.50662 | Train Acc: 39250.00000\n",
            "Train Loss: 164.82287 | Train Acc: 39337.50000\n",
            "Train Loss: 165.15441 | Train Acc: 39421.87500\n",
            "Train Loss: 165.85415 | Train Acc: 39500.00000\n",
            "Train Loss: 166.04907 | Train Acc: 39593.75000\n",
            "Train Loss: 166.52614 | Train Acc: 39671.87500\n",
            "Train Loss: 166.70745 | Train Acc: 39768.75000\n",
            "Train Loss: 167.29602 | Train Acc: 39843.75000\n",
            "Train Loss: 167.47647 | Train Acc: 39937.50000\n",
            "Train Loss: 167.71915 | Train Acc: 40025.00000\n",
            "Train Loss: 168.04399 | Train Acc: 40118.75000\n",
            "Train Loss: 168.29238 | Train Acc: 40206.25000\n",
            "Train Loss: 168.73696 | Train Acc: 40290.62500\n",
            "Train Loss: 169.10177 | Train Acc: 40381.25000\n",
            "Train Loss: 169.48779 | Train Acc: 40462.50000\n",
            "Train Loss: 169.95267 | Train Acc: 40534.37500\n",
            "Train Loss: 170.22169 | Train Acc: 40625.00000\n",
            "Train Loss: 170.41478 | Train Acc: 40721.87500\n",
            "Train Loss: 170.87063 | Train Acc: 40806.25000\n",
            "Train Loss: 171.23639 | Train Acc: 40896.87500\n",
            "Train Loss: 171.74968 | Train Acc: 40981.25000\n",
            "Train Loss: 172.34311 | Train Acc: 41065.62500\n",
            "Train Loss: 172.72021 | Train Acc: 41150.00000\n",
            "Train Loss: 172.98360 | Train Acc: 41240.62500\n",
            "Train Loss: 173.36944 | Train Acc: 41325.00000\n",
            "Train Loss: 173.71488 | Train Acc: 41412.50000\n",
            "Train Loss: 174.16043 | Train Acc: 41496.87500\n",
            "Train Loss: 174.28911 | Train Acc: 41596.87500\n",
            "Train Loss: 174.84254 | Train Acc: 41678.12500\n",
            "Train Loss: 175.16915 | Train Acc: 41762.50000\n",
            "Train Loss: 175.48294 | Train Acc: 41853.12500\n",
            "Train Loss: 175.69829 | Train Acc: 41950.00000\n",
            "Train Loss: 176.01318 | Train Acc: 42034.37500\n",
            "Train Loss: 176.34143 | Train Acc: 42121.87500\n",
            "Train Loss: 176.81400 | Train Acc: 42206.25000\n",
            "Train Loss: 177.09152 | Train Acc: 42290.62500\n",
            "Train Loss: 177.47195 | Train Acc: 42362.50000\n",
            "Train Loss: 177.80316 | Train Acc: 42443.75000\n",
            "Train Loss: 178.06146 | Train Acc: 42537.50000\n",
            "Train Loss: 178.94171 | Train Acc: 42612.50000\n",
            "Train Loss: 179.45867 | Train Acc: 42700.00000\n",
            "Train Loss: 180.03314 | Train Acc: 42778.12500\n",
            "Train Loss: 180.58035 | Train Acc: 42862.50000\n",
            "Train Loss: 180.95006 | Train Acc: 42953.12500\n",
            "Train Loss: 181.41116 | Train Acc: 43034.37500\n",
            "Train Loss: 181.61769 | Train Acc: 43125.00000\n",
            "Train Loss: 182.06646 | Train Acc: 43212.50000\n",
            "Train Loss: 182.32122 | Train Acc: 43300.00000\n",
            "Train Loss: 182.59736 | Train Acc: 43384.37500\n",
            "Train Loss: 182.97430 | Train Acc: 43475.00000\n",
            "Train Loss: 183.16909 | Train Acc: 43568.75000\n",
            "Train Loss: 183.62700 | Train Acc: 43656.25000\n",
            "Train Loss: 183.87719 | Train Acc: 43746.87500\n",
            "Train Loss: 184.44659 | Train Acc: 43821.87500\n",
            "Train Loss: 184.68758 | Train Acc: 43909.37500\n",
            "Train Loss: 185.17609 | Train Acc: 43987.50000\n",
            "Train Loss: 185.68361 | Train Acc: 44071.87500\n",
            "Train Loss: 186.10777 | Train Acc: 44156.25000\n",
            "Train Loss: 186.65802 | Train Acc: 44237.50000\n",
            "Train Loss: 186.96795 | Train Acc: 44321.87500\n",
            "Train Loss: 187.30797 | Train Acc: 44412.50000\n",
            "Train Loss: 187.82723 | Train Acc: 44487.50000\n",
            "Train Loss: 188.42787 | Train Acc: 44559.37500\n",
            "Train Loss: 188.71840 | Train Acc: 44653.12500\n",
            "Train Loss: 189.16127 | Train Acc: 44740.62500\n",
            "Train Loss: 189.62064 | Train Acc: 44828.12500\n",
            "Train Loss: 189.95284 | Train Acc: 44918.75000\n",
            "Train Loss: 190.27205 | Train Acc: 45006.25000\n",
            "Train Loss: 190.62885 | Train Acc: 45096.87500\n",
            "Train Loss: 190.95977 | Train Acc: 45190.62500\n",
            "Train Loss: 191.32176 | Train Acc: 45281.25000\n",
            "Train Loss: 191.56086 | Train Acc: 45371.87500\n",
            "Train Loss: 191.79643 | Train Acc: 45468.75000\n",
            "Train Loss: 192.32727 | Train Acc: 45556.25000\n",
            "Train Loss: 193.19557 | Train Acc: 45637.50000\n",
            "Train Loss: 193.71081 | Train Acc: 45718.75000\n",
            "Train Loss: 193.92485 | Train Acc: 45809.37500\n",
            "Train Loss: 194.24224 | Train Acc: 45896.87500\n",
            "Train Loss: 194.44320 | Train Acc: 45987.50000\n",
            "Train Loss: 194.79033 | Train Acc: 46081.25000\n",
            "Train Loss: 195.22882 | Train Acc: 46165.62500\n",
            "Train Loss: 195.36679 | Train Acc: 46256.25000\n",
            "Train Loss: 195.60912 | Train Acc: 46346.87500\n",
            "Train Loss: 196.09803 | Train Acc: 46428.12500\n",
            "Train Loss: 196.24182 | Train Acc: 46525.00000\n",
            "Train Loss: 196.71941 | Train Acc: 46603.12500\n",
            "Train Loss: 197.11778 | Train Acc: 46690.62500\n",
            "Train Loss: 197.78761 | Train Acc: 46762.50000\n",
            "Train Loss: 198.12618 | Train Acc: 46846.87500\n",
            "Train Loss: 198.44848 | Train Acc: 46931.25000\n",
            "Train Loss: 198.94156 | Train Acc: 47009.37500\n",
            "Train Loss: 199.38317 | Train Acc: 47096.87500\n",
            "Train Loss: 199.69092 | Train Acc: 47181.25000\n",
            "Train Loss: 200.14137 | Train Acc: 47262.50000\n",
            "Train Loss: 200.39614 | Train Acc: 47356.25000\n",
            "Train Loss: 200.68373 | Train Acc: 47443.75000\n",
            "Train Loss: 200.97010 | Train Acc: 47534.37500\n",
            "Train Loss: 201.13013 | Train Acc: 47631.25000\n",
            "Train Loss: 201.61275 | Train Acc: 47715.62500\n",
            "Train Loss: 202.00930 | Train Acc: 47803.12500\n",
            "Train Loss: 202.28984 | Train Acc: 47893.75000\n",
            "Train Loss: 202.61784 | Train Acc: 47987.50000\n",
            "Train Loss: 202.95685 | Train Acc: 48068.75000\n",
            "Train Loss: 203.33032 | Train Acc: 48156.25000\n",
            "Train Loss: 203.60089 | Train Acc: 48246.87500\n",
            "Train Loss: 203.88187 | Train Acc: 48334.37500\n",
            "Train Loss: 204.48026 | Train Acc: 48409.37500\n",
            "Train Loss: 204.90546 | Train Acc: 48500.00000\n",
            "Train Loss: 205.23992 | Train Acc: 48590.62500\n",
            "Train Loss: 205.52579 | Train Acc: 48681.25000\n",
            "Train Loss: 205.73824 | Train Acc: 48771.87500\n",
            "Train Loss: 206.26483 | Train Acc: 48856.25000\n",
            "Train Loss: 206.69831 | Train Acc: 48937.50000\n",
            "Train Loss: 206.98731 | Train Acc: 49018.75000\n",
            "Train Loss: 207.34310 | Train Acc: 49109.37500\n",
            "Train Loss: 207.69146 | Train Acc: 49196.87500\n",
            "Train Loss: 208.44693 | Train Acc: 49268.75000\n",
            "Train Loss: 209.05668 | Train Acc: 49346.87500\n",
            "Train Loss: 209.44589 | Train Acc: 49437.50000\n",
            "Train Loss: 209.78755 | Train Acc: 49525.00000\n",
            "Train Loss: 210.04192 | Train Acc: 49618.75000\n",
            "Train Loss: 210.41880 | Train Acc: 49709.37500\n",
            "Train Loss: 210.67347 | Train Acc: 49800.00000\n",
            "Train Loss: 211.03968 | Train Acc: 49884.37500\n",
            "Train Loss: 211.35799 | Train Acc: 49975.00000\n",
            "Train Loss: 211.58088 | Train Acc: 50071.87500\n",
            "Train Loss: 211.90677 | Train Acc: 50156.25000\n",
            "Train Loss: 212.22844 | Train Acc: 50243.75000\n",
            "Train Loss: 212.44224 | Train Acc: 50337.50000\n",
            "Train Loss: 212.62177 | Train Acc: 50431.25000\n",
            "Train Loss: 213.01543 | Train Acc: 50521.87500\n",
            "Train Loss: 213.43341 | Train Acc: 50612.50000\n",
            "Train Loss: 213.78028 | Train Acc: 50693.75000\n",
            "Train Loss: 214.11975 | Train Acc: 50787.50000\n",
            "Train Loss: 214.50142 | Train Acc: 50878.12500\n",
            "Train Loss: 214.77604 | Train Acc: 50965.62500\n",
            "Train Loss: 215.29929 | Train Acc: 51050.00000\n",
            "Train Loss: 215.75083 | Train Acc: 51137.50000\n",
            "Train Loss: 216.40489 | Train Acc: 51206.25000\n",
            "Train Loss: 217.15288 | Train Acc: 51281.25000\n",
            "Train Loss: 217.42298 | Train Acc: 51371.87500\n",
            "Train Loss: 217.89227 | Train Acc: 51456.25000\n",
            "Train Loss: 217.99090 | Train Acc: 51556.25000\n",
            "Train Loss: 218.29847 | Train Acc: 51643.75000\n",
            "Train Loss: 218.70900 | Train Acc: 51731.25000\n",
            "Train Loss: 219.07167 | Train Acc: 51815.62500\n",
            "Train Loss: 219.46528 | Train Acc: 51906.25000\n",
            "Train Loss: 219.74398 | Train Acc: 51996.87500\n",
            "Train Loss: 220.14521 | Train Acc: 52087.50000\n",
            "Train Loss: 220.41434 | Train Acc: 52178.12500\n",
            "Train Loss: 221.18244 | Train Acc: 52259.37500\n",
            "Train Loss: 221.34419 | Train Acc: 52353.12500\n",
            "Train Loss: 221.51270 | Train Acc: 52446.87500\n",
            "Train Loss: 222.07617 | Train Acc: 52525.00000\n",
            "Train Loss: 222.41063 | Train Acc: 52618.75000\n",
            "Train Loss: 222.68629 | Train Acc: 52709.37500\n",
            "Train Loss: 222.96529 | Train Acc: 52796.87500\n",
            "Train Loss: 223.30449 | Train Acc: 52881.25000\n",
            "Train Loss: 223.97132 | Train Acc: 52965.62500\n",
            "Train Loss: 224.27765 | Train Acc: 53056.25000\n",
            "Train Loss: 224.74582 | Train Acc: 53131.25000\n",
            "Train Loss: 225.20375 | Train Acc: 53212.50000\n",
            "Train Loss: 225.42413 | Train Acc: 53309.37500\n",
            "Train Loss: 225.74286 | Train Acc: 53393.75000\n",
            "Train Loss: 226.06816 | Train Acc: 53481.25000\n",
            "Train Loss: 226.48536 | Train Acc: 53565.62500\n",
            "Train Loss: 226.99085 | Train Acc: 53646.87500\n",
            "Train Loss: 227.27851 | Train Acc: 53734.37500\n",
            "Train Loss: 227.67467 | Train Acc: 53825.00000\n",
            "Train Loss: 227.87534 | Train Acc: 53918.75000\n",
            "Train Loss: 228.22469 | Train Acc: 54009.37500\n",
            "Train Loss: 228.53721 | Train Acc: 54103.12500\n",
            "Train Loss: 228.61499 | Train Acc: 54203.12500\n",
            "Train Loss: 229.01065 | Train Acc: 54287.50000\n",
            "Train Loss: 229.23039 | Train Acc: 54384.37500\n",
            "Train Loss: 229.73456 | Train Acc: 54468.75000\n",
            "Train Loss: 230.08955 | Train Acc: 54559.37500\n",
            "Train Loss: 230.45013 | Train Acc: 54643.75000\n",
            "Train Loss: 231.28180 | Train Acc: 54715.62500\n",
            "Train Loss: 231.69098 | Train Acc: 54796.87500\n",
            "Train Loss: 232.20973 | Train Acc: 54878.12500\n",
            "Train Loss: 232.54272 | Train Acc: 54962.50000\n",
            "Train Loss: 232.77842 | Train Acc: 55053.12500\n",
            "Train Loss: 233.00692 | Train Acc: 55143.75000\n",
            "Train Loss: 233.54964 | Train Acc: 55225.00000\n",
            "Train Loss: 233.74457 | Train Acc: 55318.75000\n",
            "Train Loss: 234.12906 | Train Acc: 55403.12500\n",
            "Train Loss: 234.41610 | Train Acc: 55487.50000\n",
            "Train Loss: 234.79878 | Train Acc: 55565.62500\n",
            "Train Loss: 235.45382 | Train Acc: 55637.50000\n",
            "Train Loss: 235.62752 | Train Acc: 55728.12500\n",
            "Train Loss: 235.80970 | Train Acc: 55818.75000\n",
            "Train Loss: 236.14407 | Train Acc: 55903.12500\n",
            "Train Loss: 236.33141 | Train Acc: 56000.00000\n",
            "Train Loss: 236.57276 | Train Acc: 56090.62500\n",
            "Train Loss: 237.02934 | Train Acc: 56171.87500\n",
            "Train Loss: 237.17974 | Train Acc: 56268.75000\n",
            "Train Loss: 237.56053 | Train Acc: 56356.25000\n",
            "Train Loss: 237.87508 | Train Acc: 56446.87500\n",
            "Train Loss: 238.11101 | Train Acc: 56543.75000\n",
            "Train Loss: 238.46305 | Train Acc: 56631.25000\n",
            "Train Loss: 238.62273 | Train Acc: 56728.12500\n",
            "Train Loss: 238.91657 | Train Acc: 56815.62500\n",
            "Train Loss: 239.33588 | Train Acc: 56900.00000\n",
            "Train Loss: 239.72865 | Train Acc: 56990.62500\n",
            "Train Loss: 240.01839 | Train Acc: 57078.12500\n",
            "Train Loss: 240.37645 | Train Acc: 57168.75000\n",
            "Train Loss: 241.17257 | Train Acc: 57243.75000\n",
            "Train Loss: 241.43600 | Train Acc: 57334.37500\n",
            "Train Loss: 241.79449 | Train Acc: 57421.87500\n",
            "Train Loss: 241.98277 | Train Acc: 57515.62500\n",
            "Train Loss: 242.44937 | Train Acc: 57596.87500\n",
            "Train Loss: 242.84258 | Train Acc: 57681.25000\n",
            "Train Loss: 243.08907 | Train Acc: 57771.87500\n",
            "Train Loss: 243.71096 | Train Acc: 57843.75000\n",
            "Train Loss: 243.95972 | Train Acc: 57934.37500\n",
            "Train Loss: 244.23503 | Train Acc: 58021.87500\n",
            "Train Loss: 244.63711 | Train Acc: 58100.00000\n",
            "Train Loss: 244.88022 | Train Acc: 58187.50000\n",
            "Train Loss: 245.16308 | Train Acc: 58275.00000\n",
            "Train Loss: 245.60414 | Train Acc: 58350.00000\n",
            "Train Loss: 245.88551 | Train Acc: 58434.37500\n",
            "Train Loss: 246.03897 | Train Acc: 58528.12500\n",
            "Train Loss: 246.38484 | Train Acc: 58612.50000\n",
            "Train Loss: 246.98834 | Train Acc: 58693.75000\n",
            "Train Loss: 247.23736 | Train Acc: 58781.25000\n",
            "Train Loss: 247.60326 | Train Acc: 58862.50000\n",
            "Train Loss: 247.78532 | Train Acc: 58959.37500\n",
            "Train Loss: 248.32126 | Train Acc: 59037.50000\n",
            "Train Loss: 248.75772 | Train Acc: 59121.87500\n",
            "Train Loss: 249.21330 | Train Acc: 59200.00000\n",
            "Train Loss: 249.80706 | Train Acc: 59281.25000\n",
            "Train Loss: 250.02675 | Train Acc: 59375.00000\n",
            "Train Loss: 250.22645 | Train Acc: 59468.75000\n",
            "Train Loss: 250.59332 | Train Acc: 59553.12500\n",
            "Train Loss: 250.97301 | Train Acc: 59637.50000\n",
            "Train Loss: 252.49249 | Train Acc: 59709.37500\n",
            "Train Loss: 252.79466 | Train Acc: 59800.00000\n",
            "Train Loss: 253.48298 | Train Acc: 59884.37500\n",
            "Train Loss: 253.98263 | Train Acc: 59968.75000\n",
            "Train Loss: 254.37634 | Train Acc: 60046.87500\n",
            "Train Loss: 254.62918 | Train Acc: 60140.62500\n",
            "Train Loss: 255.06147 | Train Acc: 60221.87500\n",
            "Train Loss: 255.29251 | Train Acc: 60318.75000\n",
            "Train Loss: 255.55377 | Train Acc: 60409.37500\n",
            "Train Loss: 255.75352 | Train Acc: 60509.37500\n",
            "Train Loss: 255.97127 | Train Acc: 60603.12500\n",
            "Train Loss: 256.15218 | Train Acc: 60696.87500\n",
            "Train Loss: 256.46086 | Train Acc: 60787.50000\n",
            "Train Loss: 256.82870 | Train Acc: 60865.62500\n",
            "Train Loss: 256.98775 | Train Acc: 60959.37500\n",
            "Train Loss: 257.37592 | Train Acc: 61046.87500\n",
            "Train Loss: 257.68331 | Train Acc: 61137.50000\n",
            "Train Loss: 258.11748 | Train Acc: 61218.75000\n",
            "Train Loss: 258.56507 | Train Acc: 61306.25000\n",
            "Train Loss: 259.22035 | Train Acc: 61381.25000\n",
            "Train Loss: 259.55581 | Train Acc: 61459.37500\n",
            "Train Loss: 260.02933 | Train Acc: 61540.62500\n",
            "Train Loss: 260.52447 | Train Acc: 61628.12500\n",
            "Train Loss: 260.62324 | Train Acc: 61728.12500\n",
            "Train Loss: 260.84300 | Train Acc: 61818.75000\n",
            "Train Loss: 261.18442 | Train Acc: 61909.37500\n",
            "Train Loss: 261.51855 | Train Acc: 61996.87500\n",
            "Train Loss: 261.90044 | Train Acc: 62084.37500\n",
            "Train Loss: 262.28434 | Train Acc: 62168.75000\n",
            "Train Loss: 262.54386 | Train Acc: 62259.37500\n",
            "Train Loss: 262.98162 | Train Acc: 62340.62500\n",
            "Train Loss: 263.36865 | Train Acc: 62421.87500\n",
            "Train Loss: 263.57403 | Train Acc: 62515.62500\n",
            "Train Loss: 263.95779 | Train Acc: 62600.00000\n",
            "Train Loss: 264.33595 | Train Acc: 62681.25000\n",
            "Train Loss: 264.52059 | Train Acc: 62775.00000\n",
            "Train Loss: 264.67705 | Train Acc: 62871.87500\n",
            "Train Loss: 265.08344 | Train Acc: 62956.25000\n",
            "Train Loss: 265.36477 | Train Acc: 63046.87500\n",
            "Train Loss: 265.83825 | Train Acc: 63125.00000\n",
            "Train Loss: 266.55586 | Train Acc: 63200.00000\n",
            "Train Loss: 266.79741 | Train Acc: 63290.62500\n",
            "Train Loss: 267.20967 | Train Acc: 63378.12500\n",
            "Train Loss: 267.47691 | Train Acc: 63468.75000\n",
            "Train Loss: 268.13628 | Train Acc: 63546.87500\n",
            "Train Loss: 268.29981 | Train Acc: 63646.87500\n",
            "Train Loss: 268.73495 | Train Acc: 63728.12500\n",
            "Train Loss: 269.15245 | Train Acc: 63815.62500\n",
            "Train Loss: 269.36312 | Train Acc: 63903.12500\n",
            "Train Loss: 270.08183 | Train Acc: 63987.50000\n",
            "Train Loss: 270.43967 | Train Acc: 64078.12500\n",
            "Train Loss: 270.75069 | Train Acc: 64165.62500\n",
            "Train Loss: 271.15723 | Train Acc: 64250.00000\n",
            "Train Loss: 271.50232 | Train Acc: 64337.50000\n",
            "Train Loss: 271.70016 | Train Acc: 64428.12500\n",
            "Train Loss: 272.07038 | Train Acc: 64509.37500\n",
            "Train Loss: 272.42132 | Train Acc: 64593.75000\n",
            "Train Loss: 272.87055 | Train Acc: 64675.00000\n",
            "Train Loss: 272.97696 | Train Acc: 64771.87500\n",
            "Train Loss: 273.27904 | Train Acc: 64862.50000\n",
            "Train Loss: 273.53258 | Train Acc: 64946.87500\n",
            "Train Loss: 274.00373 | Train Acc: 65028.12500\n",
            "Train Loss: 274.22693 | Train Acc: 65118.75000\n",
            "Train Loss: 274.52017 | Train Acc: 65209.37500\n",
            "Train Loss: 274.85886 | Train Acc: 65300.00000\n",
            "Train Loss: 275.07446 | Train Acc: 65396.87500\n",
            "Train Loss: 275.53520 | Train Acc: 65478.12500\n",
            "Train Loss: 276.26246 | Train Acc: 65562.50000\n",
            "Train Loss: 276.66335 | Train Acc: 65643.75000\n",
            "Train Loss: 277.19279 | Train Acc: 65728.12500\n",
            "Train Loss: 277.44602 | Train Acc: 65818.75000\n",
            "Train Loss: 277.77428 | Train Acc: 65906.25000\n",
            "Train Loss: 278.13781 | Train Acc: 65996.87500\n",
            "Train Loss: 278.44863 | Train Acc: 66087.50000\n",
            "Train Loss: 278.74291 | Train Acc: 66175.00000\n",
            "Train Loss: 278.89138 | Train Acc: 66271.87500\n",
            "Train Loss: 279.29399 | Train Acc: 66362.50000\n",
            "Train Loss: 279.89224 | Train Acc: 66453.12500\n",
            "Train Loss: 280.35134 | Train Acc: 66540.62500\n",
            "Train Loss: 280.91640 | Train Acc: 66618.75000\n",
            "Train Loss: 281.28409 | Train Acc: 66703.12500\n",
            "Train Loss: 281.62590 | Train Acc: 66787.50000\n",
            "Train Loss: 282.06201 | Train Acc: 66871.87500\n",
            "Train Loss: 282.22101 | Train Acc: 66971.87500\n",
            "Train Loss: 282.82189 | Train Acc: 67056.25000\n",
            "Train Loss: 282.97602 | Train Acc: 67153.12500\n",
            "Train Loss: 283.30039 | Train Acc: 67243.75000\n",
            "Train Loss: 283.86643 | Train Acc: 67325.00000\n",
            "Train Loss: 284.17157 | Train Acc: 67412.50000\n",
            "Train Loss: 284.44026 | Train Acc: 67503.12500\n",
            "Train Loss: 284.74897 | Train Acc: 67590.62500\n",
            "Train Loss: 285.31598 | Train Acc: 67671.87500\n",
            "Train Loss: 285.45587 | Train Acc: 67771.87500\n",
            "Train Loss: 285.90898 | Train Acc: 67859.37500\n",
            "Train Loss: 286.25957 | Train Acc: 67950.00000\n",
            "Train Loss: 286.61892 | Train Acc: 68034.37500\n",
            "Train Loss: 287.23435 | Train Acc: 68112.50000\n",
            "Train Loss: 287.74615 | Train Acc: 68187.50000\n",
            "Train Loss: 288.01316 | Train Acc: 68275.00000\n",
            "Train Loss: 288.52713 | Train Acc: 68356.25000\n",
            "Train Loss: 288.86680 | Train Acc: 68446.87500\n",
            "Train Loss: 289.35533 | Train Acc: 68521.87500\n",
            "Train Loss: 289.73374 | Train Acc: 68609.37500\n",
            "Train Loss: 290.02664 | Train Acc: 68693.75000\n",
            "Train Loss: 290.54075 | Train Acc: 68775.00000\n",
            "Train Loss: 290.89696 | Train Acc: 68862.50000\n",
            "Train Loss: 291.12818 | Train Acc: 68956.25000\n",
            "Train Loss: 291.59672 | Train Acc: 69037.50000\n",
            "Train Loss: 291.84020 | Train Acc: 69131.25000\n",
            "Train Loss: 292.06331 | Train Acc: 69225.00000\n",
            "Train Loss: 292.22179 | Train Acc: 69318.75000\n",
            "Train Loss: 292.53872 | Train Acc: 69406.25000\n",
            "Train Loss: 292.79470 | Train Acc: 69496.87500\n",
            "Train Loss: 293.38706 | Train Acc: 69578.12500\n",
            "Train Loss: 293.66999 | Train Acc: 69668.75000\n",
            "Train Loss: 293.98467 | Train Acc: 69762.50000\n",
            "Train Loss: 294.47583 | Train Acc: 69853.12500\n",
            "Train Loss: 294.84174 | Train Acc: 69940.62500\n",
            "Train Loss: 295.16584 | Train Acc: 70021.87500\n",
            "Train Loss: 295.45601 | Train Acc: 70112.50000\n",
            "Train Loss: 295.96281 | Train Acc: 70193.75000\n",
            "Train Loss: 296.28326 | Train Acc: 70281.25000\n",
            "Train Loss: 296.60830 | Train Acc: 70368.75000\n",
            "Train Loss: 297.15995 | Train Acc: 70446.87500\n",
            "Train Loss: 297.46673 | Train Acc: 70540.62500\n",
            "Train Loss: 297.86058 | Train Acc: 70631.25000\n",
            "Train Loss: 298.18733 | Train Acc: 70715.62500\n",
            "Train Loss: 298.64781 | Train Acc: 70796.87500\n",
            "Train Loss: 299.07311 | Train Acc: 70878.12500\n",
            "Train Loss: 299.64588 | Train Acc: 70959.37500\n",
            "Train Loss: 300.13318 | Train Acc: 71040.62500\n",
            "Train Loss: 300.60161 | Train Acc: 71125.00000\n",
            "Train Loss: 300.73782 | Train Acc: 71221.87500\n",
            "Train Loss: 301.47610 | Train Acc: 71300.00000\n",
            "Train Loss: 301.62901 | Train Acc: 71396.87500\n",
            "Train Loss: 301.83485 | Train Acc: 71490.62500\n",
            "Train Loss: 302.17939 | Train Acc: 71575.00000\n",
            "Train Loss: 302.57261 | Train Acc: 71656.25000\n",
            "Train Loss: 302.78912 | Train Acc: 71746.87500\n",
            "Train Loss: 303.61998 | Train Acc: 71825.00000\n",
            "Train Loss: 303.88925 | Train Acc: 71915.62500\n",
            "Train Loss: 304.22854 | Train Acc: 72006.25000\n",
            "Train Loss: 304.40614 | Train Acc: 72100.00000\n",
            "Train Loss: 304.81630 | Train Acc: 72187.50000\n",
            "Train Loss: 305.24496 | Train Acc: 72271.87500\n",
            "Train Loss: 305.56610 | Train Acc: 72356.25000\n",
            "Train Loss: 305.79924 | Train Acc: 72450.00000\n",
            "Train Loss: 306.01876 | Train Acc: 72540.62500\n",
            "Train Loss: 306.25965 | Train Acc: 72634.37500\n",
            "Train Loss: 306.52218 | Train Acc: 72728.12500\n",
            "Train Loss: 306.83771 | Train Acc: 72821.87500\n",
            "Train Loss: 307.21377 | Train Acc: 72906.25000\n",
            "Train Loss: 307.61144 | Train Acc: 72987.50000\n",
            "Train Loss: 307.84684 | Train Acc: 73075.00000\n",
            "Train Loss: 308.06902 | Train Acc: 73165.62500\n",
            "Train Loss: 308.40535 | Train Acc: 73256.25000\n",
            "Train Loss: 308.97660 | Train Acc: 73337.50000\n",
            "Train Loss: 309.58333 | Train Acc: 73425.00000\n",
            "Train Loss: 309.88438 | Train Acc: 73509.37500\n",
            "Train Loss: 310.42439 | Train Acc: 73596.87500\n",
            "Train Loss: 310.69469 | Train Acc: 73690.62500\n",
            "Train Loss: 311.08956 | Train Acc: 73775.00000\n",
            "Train Loss: 311.34455 | Train Acc: 73868.75000\n",
            "Train Loss: 311.71703 | Train Acc: 73956.25000\n",
            "Train Loss: 311.99709 | Train Acc: 74040.62500\n",
            "Train Loss: 312.78904 | Train Acc: 74112.50000\n",
            "Train Loss: 312.88019 | Train Acc: 74212.50000\n",
            "Train Loss: 313.21455 | Train Acc: 74300.00000\n",
            "Train Loss: 313.96878 | Train Acc: 74378.12500\n",
            "Train Loss: 314.28695 | Train Acc: 74468.75000\n",
            "Train Loss: 314.71786 | Train Acc: 74550.00000\n",
            "Train Loss: 315.16285 | Train Acc: 74646.87500\n",
            "Train Loss: 315.48003 | Train Acc: 74737.50000\n",
            "Train Loss: 316.03220 | Train Acc: 74818.75000\n",
            "Train Loss: 316.59400 | Train Acc: 74900.00000\n",
            "Train Loss: 317.02309 | Train Acc: 74993.75000\n",
            "Train Loss: 317.58371 | Train Acc: 75078.12500\n",
            "Train Loss: 317.77892 | Train Acc: 75175.00000\n",
            "Train Loss: 318.58612 | Train Acc: 75243.75000\n",
            "Train Loss: 319.15547 | Train Acc: 75334.37500\n",
            "Train Loss: 319.55883 | Train Acc: 75418.75000\n",
            "Train Loss: 319.80871 | Train Acc: 75509.37500\n",
            "Train Loss: 320.20751 | Train Acc: 75590.62500\n",
            "Train Loss: 320.45966 | Train Acc: 75684.37500\n",
            "Train Loss: 321.23372 | Train Acc: 75759.37500\n",
            "Train Loss: 321.50608 | Train Acc: 75856.25000\n",
            "Train Loss: 321.75225 | Train Acc: 75950.00000\n",
            "Train Loss: 322.26636 | Train Acc: 76031.25000\n",
            "Train Loss: 322.39193 | Train Acc: 76125.00000\n",
            "Train Loss: 323.10212 | Train Acc: 76206.25000\n",
            "Train Loss: 323.36170 | Train Acc: 76296.87500\n",
            "Train Loss: 323.53045 | Train Acc: 76390.62500\n",
            "Train Loss: 323.97452 | Train Acc: 76468.75000\n",
            "Train Loss: 324.29480 | Train Acc: 76553.12500\n",
            "Train Loss: 324.76944 | Train Acc: 76631.25000\n",
            "Train Loss: 324.89885 | Train Acc: 76731.25000\n",
            "Train Loss: 325.20056 | Train Acc: 76821.87500\n",
            "Train Loss: 325.47886 | Train Acc: 76912.50000\n",
            "Train Loss: 325.85111 | Train Acc: 77000.00000\n",
            "Train Loss: 326.09677 | Train Acc: 77090.62500\n",
            "Train Loss: 326.36649 | Train Acc: 77181.25000\n",
            "Train Loss: 326.93444 | Train Acc: 77262.50000\n",
            "Train Loss: 327.93551 | Train Acc: 77337.50000\n",
            "Train Loss: 328.26030 | Train Acc: 77428.12500\n",
            "Train Loss: 328.45166 | Train Acc: 77521.87500\n",
            "Train Loss: 328.77528 | Train Acc: 77603.12500\n",
            "Train Loss: 329.29625 | Train Acc: 77687.50000\n",
            "Train Loss: 329.68685 | Train Acc: 77768.75000\n",
            "Train Loss: 329.99526 | Train Acc: 77859.37500\n",
            "Train Loss: 330.69095 | Train Acc: 77940.62500\n",
            "Train Loss: 330.92155 | Train Acc: 78031.25000\n",
            "Train Loss: 331.37720 | Train Acc: 78112.50000\n",
            "Train Loss: 331.50413 | Train Acc: 78206.25000\n",
            "Train Loss: 331.81972 | Train Acc: 78296.87500\n",
            "Train Loss: 332.19513 | Train Acc: 78381.25000\n",
            "Train Loss: 332.75606 | Train Acc: 78459.37500\n",
            "Train Loss: 332.99651 | Train Acc: 78550.00000\n",
            "Train Loss: 333.21553 | Train Acc: 78640.62500\n",
            "Train Loss: 333.64906 | Train Acc: 78725.00000\n",
            "Train Loss: 333.76693 | Train Acc: 78821.87500\n",
            "Train Loss: 334.11356 | Train Acc: 78909.37500\n",
            "Train Loss: 334.39842 | Train Acc: 79000.00000\n",
            "Train Loss: 334.76776 | Train Acc: 79084.37500\n",
            "Train Loss: 335.15841 | Train Acc: 79175.00000\n",
            "Train Loss: 335.44189 | Train Acc: 79262.50000\n",
            "Train Loss: 335.69938 | Train Acc: 79356.25000\n",
            "Train Loss: 335.91710 | Train Acc: 79450.00000\n",
            "Train Loss: 336.41869 | Train Acc: 79528.12500\n",
            "Train Loss: 336.94598 | Train Acc: 79621.87500\n",
            "Train Loss: 337.43487 | Train Acc: 79706.25000\n",
            "Train Loss: 337.72393 | Train Acc: 79796.87500\n",
            "Train Loss: 337.97389 | Train Acc: 79887.50000\n",
            "Train Loss: 338.18677 | Train Acc: 79984.37500\n",
            "Train Loss: 338.66254 | Train Acc: 80065.62500\n",
            "Train Loss: 339.40250 | Train Acc: 80134.37500\n",
            "Train Loss: 339.71000 | Train Acc: 80218.75000\n",
            "Train Loss: 339.91369 | Train Acc: 80309.37500\n",
            "Train Loss: 340.43236 | Train Acc: 80387.50000\n",
            "Train Loss: 340.82059 | Train Acc: 80465.62500\n",
            "Train Loss: 341.17594 | Train Acc: 80553.12500\n",
            "Train Loss: 341.39174 | Train Acc: 80646.87500\n",
            "Train Loss: 341.77922 | Train Acc: 80731.25000\n",
            "Train Loss: 342.04001 | Train Acc: 80821.87500\n",
            "Train Loss: 342.51930 | Train Acc: 80896.87500\n",
            "Train Loss: 342.98347 | Train Acc: 80978.12500\n",
            "Train Loss: 343.10597 | Train Acc: 81075.00000\n",
            "Train Loss: 343.62777 | Train Acc: 81153.12500\n",
            "Train Loss: 343.93364 | Train Acc: 81243.75000\n",
            "Train Loss: 344.46969 | Train Acc: 81328.12500\n",
            "Train Loss: 344.99844 | Train Acc: 81412.50000\n",
            "Train Loss: 345.52772 | Train Acc: 81487.50000\n",
            "Train Loss: 345.82607 | Train Acc: 81581.25000\n",
            "Train Loss: 346.32986 | Train Acc: 81659.37500\n",
            "Train Loss: 346.68029 | Train Acc: 81740.62500\n",
            "Train Loss: 347.16143 | Train Acc: 81825.00000\n",
            "Train Loss: 347.41054 | Train Acc: 81918.75000\n",
            "Train Loss: 347.73765 | Train Acc: 82003.12500\n",
            "Train Loss: 347.97914 | Train Acc: 82093.75000\n",
            "Train Loss: 348.40480 | Train Acc: 82181.25000\n",
            "Train Loss: 349.01255 | Train Acc: 82268.75000\n",
            "Train Loss: 349.56748 | Train Acc: 82350.00000\n",
            "Train Loss: 350.07929 | Train Acc: 82431.25000\n",
            "Train Loss: 350.39051 | Train Acc: 82518.75000\n",
            "Train Loss: 350.75073 | Train Acc: 82609.37500\n",
            "Train Loss: 351.27547 | Train Acc: 82696.87500\n",
            "Train Loss: 351.59592 | Train Acc: 82781.25000\n",
            "Train Loss: 351.95457 | Train Acc: 82868.75000\n",
            "Train Loss: 352.27893 | Train Acc: 82959.37500\n",
            "Train Loss: 352.38672 | Train Acc: 83059.37500\n",
            "Train Loss: 352.84874 | Train Acc: 83137.50000\n",
            "Train Loss: 353.05654 | Train Acc: 83231.25000\n",
            "Train Loss: 353.48415 | Train Acc: 83315.62500\n",
            "Train Loss: 353.81164 | Train Acc: 83406.25000\n",
            "Train Loss: 353.97506 | Train Acc: 83500.00000\n",
            "Train Loss: 354.29324 | Train Acc: 83587.50000\n",
            "Train Loss: 354.59570 | Train Acc: 83671.87500\n",
            "Train Loss: 355.06499 | Train Acc: 83756.25000\n",
            "Train Loss: 355.46355 | Train Acc: 83840.62500\n",
            "Train Loss: 355.75696 | Train Acc: 83934.37500\n",
            "Train Loss: 356.05795 | Train Acc: 84021.87500\n",
            "Train Loss: 356.35235 | Train Acc: 84112.50000\n",
            "Train Loss: 356.54892 | Train Acc: 84209.37500\n",
            "Train Loss: 356.79059 | Train Acc: 84303.12500\n",
            "Train Loss: 357.42624 | Train Acc: 84381.25000\n",
            "Train Loss: 357.83436 | Train Acc: 84462.50000\n",
            "Train Loss: 358.51982 | Train Acc: 84537.50000\n",
            "Train Loss: 359.35063 | Train Acc: 84615.62500\n",
            "Train Loss: 359.90200 | Train Acc: 84690.62500\n",
            "Train Loss: 360.29453 | Train Acc: 84775.00000\n",
            "Train Loss: 360.56059 | Train Acc: 84871.87500\n",
            "Train Loss: 360.93369 | Train Acc: 84956.25000\n",
            "Train Loss: 361.30334 | Train Acc: 85050.00000\n",
            "Train Loss: 361.69082 | Train Acc: 85131.25000\n",
            "Train Loss: 361.93404 | Train Acc: 85221.87500\n",
            "Train Loss: 362.28687 | Train Acc: 85312.50000\n",
            "Train Loss: 362.86189 | Train Acc: 85400.00000\n",
            "Train Loss: 362.95628 | Train Acc: 85500.00000\n",
            "Train Loss: 363.16315 | Train Acc: 85596.87500\n",
            "Train Loss: 363.48649 | Train Acc: 85693.75000\n",
            "Train Loss: 364.05508 | Train Acc: 85768.75000\n",
            "Train Loss: 364.35942 | Train Acc: 85862.50000\n",
            "Train Loss: 364.63936 | Train Acc: 85950.00000\n",
            "Train Loss: 365.07326 | Train Acc: 86034.37500\n",
            "Train Loss: 365.35444 | Train Acc: 86121.87500\n",
            "Train Loss: 365.96703 | Train Acc: 86200.00000\n",
            "Train Loss: 366.20973 | Train Acc: 86293.75000\n",
            "Train Loss: 366.55804 | Train Acc: 86378.12500\n",
            "Train Loss: 366.76288 | Train Acc: 86471.87500\n",
            "Train Loss: 366.96993 | Train Acc: 86568.75000\n",
            "Train Loss: 367.47725 | Train Acc: 86656.25000\n",
            "Train Loss: 367.91860 | Train Acc: 86743.75000\n",
            "Train Loss: 368.35472 | Train Acc: 86834.37500\n",
            "Train Loss: 368.74145 | Train Acc: 86925.00000\n",
            "Train Loss: 368.98325 | Train Acc: 87015.62500\n",
            "Train Loss: 369.65023 | Train Acc: 87093.75000\n",
            "Train Loss: 369.99877 | Train Acc: 87178.12500\n",
            "Train Loss: 370.17738 | Train Acc: 87271.87500\n",
            "Train Loss: 370.38034 | Train Acc: 87365.62500\n",
            "Train Loss: 370.68208 | Train Acc: 87456.25000\n",
            "Train Loss: 370.95139 | Train Acc: 87546.87500\n",
            "Train Loss: 371.18150 | Train Acc: 87643.75000\n",
            "Train Loss: 371.51948 | Train Acc: 87731.25000\n",
            "Train Loss: 371.91226 | Train Acc: 87821.87500\n",
            "Train Loss: 372.48218 | Train Acc: 87906.25000\n",
            "Train Loss: 372.87958 | Train Acc: 87990.62500\n",
            "Train Loss: 373.15572 | Train Acc: 88084.37500\n",
            "Train Loss: 373.43262 | Train Acc: 88178.12500\n",
            "Train Loss: 373.87204 | Train Acc: 88259.37500\n",
            "Train Loss: 374.23619 | Train Acc: 88350.00000\n",
            "Train Loss: 374.39516 | Train Acc: 88443.75000\n",
            "Train Loss: 374.84370 | Train Acc: 88525.00000\n",
            "Train Loss: 375.38102 | Train Acc: 88609.37500\n",
            "Train Loss: 375.69023 | Train Acc: 88703.12500\n",
            "Train Loss: 376.06727 | Train Acc: 88778.12500\n",
            "Train Loss: 376.41143 | Train Acc: 88859.37500\n",
            "Train Loss: 376.75929 | Train Acc: 88940.62500\n",
            "Train Loss: 377.19274 | Train Acc: 89028.12500\n",
            "Train Loss: 377.58381 | Train Acc: 89112.50000\n",
            "Train Loss: 378.16523 | Train Acc: 89193.75000\n",
            "Train Loss: 378.73347 | Train Acc: 89268.75000\n",
            "Train Loss: 378.95705 | Train Acc: 89359.37500\n",
            "Train Loss: 379.30172 | Train Acc: 89446.87500\n",
            "Train Loss: 379.72937 | Train Acc: 89531.25000\n",
            "Train Loss: 380.32509 | Train Acc: 89606.25000\n",
            "Train Loss: 380.49337 | Train Acc: 89703.12500\n",
            "Train Loss: 381.07312 | Train Acc: 89784.37500\n",
            "Train Loss: 381.54919 | Train Acc: 89868.75000\n",
            "Train Loss: 382.01035 | Train Acc: 89943.75000\n",
            "Train Loss: 382.14280 | Train Acc: 90040.62500\n",
            "Train Loss: 382.34268 | Train Acc: 90137.50000\n",
            "Train Loss: 382.82944 | Train Acc: 90221.87500\n",
            "Train Loss: 383.22984 | Train Acc: 90309.37500\n",
            "Train Loss: 383.54893 | Train Acc: 90400.00000\n",
            "Train Loss: 384.01862 | Train Acc: 90481.25000\n",
            "Train Loss: 384.23976 | Train Acc: 90568.75000\n",
            "Train Loss: 384.78063 | Train Acc: 90656.25000\n",
            "Train Loss: 385.24011 | Train Acc: 90743.75000\n",
            "Train Loss: 385.64174 | Train Acc: 90828.12500\n",
            "Train Loss: 385.86252 | Train Acc: 90918.75000\n",
            "Train Loss: 386.20294 | Train Acc: 91006.25000\n",
            "Train Loss: 386.54204 | Train Acc: 91096.87500\n",
            "Train Loss: 386.94533 | Train Acc: 91178.12500\n",
            "Train Loss: 387.12760 | Train Acc: 91271.87500\n",
            "Train Loss: 387.52949 | Train Acc: 91359.37500\n",
            "Train Loss: 387.72891 | Train Acc: 91450.00000\n",
            "Train Loss: 388.05562 | Train Acc: 91540.62500\n",
            "Train Loss: 388.63837 | Train Acc: 91615.62500\n",
            "Train Loss: 389.02934 | Train Acc: 91706.25000\n",
            "Train Loss: 389.44574 | Train Acc: 91784.37500\n",
            "Train Loss: 390.10013 | Train Acc: 91856.25000\n",
            "Train Loss: 390.43384 | Train Acc: 91946.87500\n",
            "Train Loss: 390.94374 | Train Acc: 92028.12500\n",
            "Train Loss: 391.21704 | Train Acc: 92118.75000\n",
            "Train Loss: 391.75771 | Train Acc: 92196.87500\n",
            "Train Loss: 392.47337 | Train Acc: 92284.37500\n",
            "Train Loss: 392.58340 | Train Acc: 92381.25000\n",
            "Train Loss: 393.30540 | Train Acc: 92456.25000\n",
            "Train Loss: 393.75138 | Train Acc: 92537.50000\n",
            "Train Loss: 394.14469 | Train Acc: 92621.87500\n",
            "Train Loss: 394.52556 | Train Acc: 92706.25000\n",
            "Train Loss: 394.79262 | Train Acc: 92793.75000\n",
            "Train Loss: 395.22550 | Train Acc: 92875.00000\n",
            "Train Loss: 395.84989 | Train Acc: 92950.00000\n",
            "Train Loss: 396.05439 | Train Acc: 93046.87500\n",
            "Train Loss: 396.40170 | Train Acc: 93131.25000\n",
            "Train Loss: 396.72770 | Train Acc: 93215.62500\n",
            "Train Loss: 397.11660 | Train Acc: 93306.25000\n",
            "Train Loss: 397.90386 | Train Acc: 93387.50000\n",
            "Train Loss: 398.17039 | Train Acc: 93481.25000\n",
            "Train Loss: 398.86236 | Train Acc: 93553.12500\n",
            "Train Loss: 399.52088 | Train Acc: 93631.25000\n",
            "Train Loss: 399.96426 | Train Acc: 93718.75000\n",
            "Train Loss: 400.29064 | Train Acc: 93800.00000\n",
            "Train Loss: 400.53298 | Train Acc: 93887.50000\n",
            "Train Loss: 400.82619 | Train Acc: 93978.12500\n",
            "Train Loss: 401.54201 | Train Acc: 94056.25000\n",
            "Train Loss: 401.90487 | Train Acc: 94150.00000\n",
            "Train Loss: 402.27853 | Train Acc: 94234.37500\n",
            "Train Loss: 402.58408 | Train Acc: 94325.00000\n",
            "Train Loss: 403.06980 | Train Acc: 94415.62500\n",
            "Train Loss: 403.25049 | Train Acc: 94509.37500\n",
            "Train Loss: 403.57180 | Train Acc: 94596.87500\n",
            "Train Loss: 404.30809 | Train Acc: 94675.00000\n",
            "Train Loss: 404.90657 | Train Acc: 94746.87500\n",
            "Train Loss: 405.20146 | Train Acc: 94837.50000\n",
            "Train Loss: 405.30736 | Train Acc: 94937.50000\n",
            "Train Loss: 405.73935 | Train Acc: 95025.00000\n",
            "Train Loss: 406.16160 | Train Acc: 95106.25000\n",
            "Train Loss: 406.59147 | Train Acc: 95193.75000\n",
            "Train Loss: 406.82214 | Train Acc: 95287.50000\n",
            "Train Loss: 407.31847 | Train Acc: 95365.62500\n",
            "Train Loss: 407.47319 | Train Acc: 95456.25000\n",
            "Train Loss: 407.64594 | Train Acc: 95550.00000\n",
            "Train Loss: 407.97262 | Train Acc: 95640.62500\n",
            "Train Loss: 408.44060 | Train Acc: 95718.75000\n",
            "Train Loss: 408.83489 | Train Acc: 95800.00000\n",
            "Train Loss: 409.33501 | Train Acc: 95884.37500\n",
            "Train Loss: 409.54818 | Train Acc: 95975.00000\n",
            "Train Loss: 410.14481 | Train Acc: 96059.37500\n",
            "Train Loss: 410.63057 | Train Acc: 96140.62500\n",
            "Train Loss: 411.06071 | Train Acc: 96228.12500\n",
            "Train Loss: 411.52057 | Train Acc: 96315.62500\n",
            "Train Loss: 411.77208 | Train Acc: 96403.12500\n",
            "Train Loss: 412.46141 | Train Acc: 96481.25000\n",
            "Train Loss: 412.61359 | Train Acc: 96575.00000\n",
            "Train Loss: 412.93913 | Train Acc: 96662.50000\n",
            "Train Loss: 413.48402 | Train Acc: 96750.00000\n",
            "Train Loss: 413.81149 | Train Acc: 96834.37500\n",
            "Train Loss: 414.40647 | Train Acc: 96918.75000\n",
            "Train Loss: 414.77707 | Train Acc: 97006.25000\n",
            "Train Loss: 415.18882 | Train Acc: 97096.87500\n",
            "Train Loss: 415.38575 | Train Acc: 97193.75000\n",
            "Train Loss: 415.87016 | Train Acc: 97284.37500\n",
            "Train Loss: 416.07552 | Train Acc: 97378.12500\n",
            "Train Loss: 416.73902 | Train Acc: 97450.00000\n",
            "Train Loss: 417.05725 | Train Acc: 97534.37500\n",
            "Train Loss: 417.67554 | Train Acc: 97612.50000\n",
            "Train Loss: 418.11706 | Train Acc: 97693.75000\n",
            "Train Loss: 418.47990 | Train Acc: 97784.37500\n",
            "Train Loss: 418.66080 | Train Acc: 97878.12500\n",
            "Train Loss: 419.25997 | Train Acc: 97959.37500\n",
            "Train Loss: 419.57977 | Train Acc: 98046.87500\n",
            "Train Loss: 420.06645 | Train Acc: 98121.87500\n",
            "Train Loss: 420.34660 | Train Acc: 98212.50000\n",
            "Train Loss: 420.55549 | Train Acc: 98303.12500\n",
            "Train Loss: 420.96611 | Train Acc: 98390.62500\n",
            "Train Loss: 421.30026 | Train Acc: 98481.25000\n",
            "Train Loss: 421.60402 | Train Acc: 98565.62500\n",
            "Train Loss: 421.80952 | Train Acc: 98662.50000\n",
            "Train Loss: 422.44647 | Train Acc: 98740.62500\n",
            "Train Loss: 422.95886 | Train Acc: 98828.12500\n",
            "Train Loss: 423.23673 | Train Acc: 98915.62500\n",
            "Train Loss: 423.33872 | Train Acc: 99015.62500\n",
            "Train Loss: 423.69936 | Train Acc: 99106.25000\n",
            "Train Loss: 423.84804 | Train Acc: 99206.25000\n",
            "Train Loss: 424.54675 | Train Acc: 99271.87500\n",
            "Train Loss: 424.91366 | Train Acc: 99359.37500\n",
            "Train Loss: 425.12331 | Train Acc: 99453.12500\n",
            "Train Loss: 425.54409 | Train Acc: 99540.62500\n",
            "Train Loss: 425.87654 | Train Acc: 99628.12500\n",
            "Train Loss: 426.32651 | Train Acc: 99712.50000\n",
            "Train Loss: 426.94635 | Train Acc: 99796.87500\n",
            "Train Loss: 427.17907 | Train Acc: 99884.37500\n",
            "Train Loss: 427.64876 | Train Acc: 99971.87500\n",
            "Train Loss: 428.21858 | Train Acc: 100050.00000\n",
            "Train Loss: 428.45026 | Train Acc: 100143.75000\n",
            "Train Loss: 428.88197 | Train Acc: 100215.62500\n",
            "Train Loss: 429.15655 | Train Acc: 100303.12500\n",
            "Train Loss: 429.39446 | Train Acc: 100393.75000\n",
            "Train Loss: 429.90299 | Train Acc: 100471.87500\n",
            "Train Loss: 430.13686 | Train Acc: 100562.50000\n",
            "Train Loss: 430.59014 | Train Acc: 100653.12500\n",
            "Train Loss: 430.90622 | Train Acc: 100740.62500\n",
            "Train Loss: 431.25457 | Train Acc: 100828.12500\n",
            "Train Loss: 431.45530 | Train Acc: 100925.00000\n",
            "Train Loss: 431.74634 | Train Acc: 101009.37500\n",
            "Train Loss: 432.00498 | Train Acc: 101093.75000\n",
            "Train Loss: 432.27330 | Train Acc: 101184.37500\n",
            "Train Loss: 432.58800 | Train Acc: 101278.12500\n",
            "Train Loss: 433.17418 | Train Acc: 101356.25000\n",
            "Train Loss: 433.34424 | Train Acc: 101450.00000\n",
            "Train Loss: 433.57123 | Train Acc: 101540.62500\n",
            "Train Loss: 433.79333 | Train Acc: 101631.25000\n",
            "Train Loss: 434.12593 | Train Acc: 101721.87500\n",
            "Train Loss: 434.53980 | Train Acc: 101806.25000\n",
            "Train Loss: 434.81455 | Train Acc: 101896.87500\n",
            "Train Loss: 435.23073 | Train Acc: 101978.12500\n",
            "Train Loss: 435.76085 | Train Acc: 102056.25000\n",
            "Train Loss: 436.05059 | Train Acc: 102140.62500\n",
            "Train Loss: 436.37364 | Train Acc: 102225.00000\n",
            "Train Loss: 436.84228 | Train Acc: 102312.50000\n",
            "Train Loss: 437.30406 | Train Acc: 102387.50000\n",
            "Train Loss: 437.52467 | Train Acc: 102481.25000\n",
            "Train Loss: 438.18747 | Train Acc: 102568.75000\n",
            "Train Loss: 438.39683 | Train Acc: 102665.62500\n",
            "Train Loss: 438.67146 | Train Acc: 102756.25000\n",
            "Train Loss: 439.29530 | Train Acc: 102831.25000\n",
            "Train Loss: 439.57721 | Train Acc: 102918.75000\n",
            "Train Loss: 439.83630 | Train Acc: 103012.50000\n",
            "Train Loss: 440.02282 | Train Acc: 103106.25000\n",
            "Train Loss: 440.49435 | Train Acc: 103190.62500\n",
            "Train Loss: 440.95122 | Train Acc: 103271.87500\n",
            "Train Loss: 441.46201 | Train Acc: 103356.25000\n",
            "Train Loss: 441.71820 | Train Acc: 103446.87500\n",
            "Train Loss: 441.90159 | Train Acc: 103543.75000\n",
            "Train Loss: 442.18136 | Train Acc: 103634.37500\n",
            "Train Loss: 442.70454 | Train Acc: 103718.75000\n",
            "Train Loss: 443.10051 | Train Acc: 103803.12500\n",
            "Train Loss: 443.92386 | Train Acc: 103878.12500\n",
            "Train Loss: 444.35101 | Train Acc: 103962.50000\n",
            "Train Loss: 444.53645 | Train Acc: 104056.25000\n",
            "Train Loss: 444.91345 | Train Acc: 104146.87500\n",
            "Train Loss: 445.08470 | Train Acc: 104243.75000\n",
            "Train Loss: 445.52182 | Train Acc: 104328.12500\n",
            "Train Loss: 446.06327 | Train Acc: 104412.50000\n",
            "Train Loss: 446.60370 | Train Acc: 104493.75000\n",
            "Train Loss: 446.93282 | Train Acc: 104575.00000\n",
            "Train Loss: 447.64243 | Train Acc: 104656.25000\n",
            "Train Loss: 447.91964 | Train Acc: 104750.00000\n",
            "Train Loss: 448.23190 | Train Acc: 104831.25000\n",
            "Train Loss: 448.52590 | Train Acc: 104921.87500\n",
            "Train Loss: 448.83516 | Train Acc: 105009.37500\n",
            "Train Loss: 449.17551 | Train Acc: 105093.75000\n",
            "Train Loss: 449.48965 | Train Acc: 105184.37500\n",
            "Train Loss: 449.93420 | Train Acc: 105268.75000\n",
            "Train Loss: 450.25312 | Train Acc: 105356.25000\n",
            "Train Loss: 450.56180 | Train Acc: 105440.62500\n",
            "Train Loss: 450.85441 | Train Acc: 105528.12500\n",
            "Train Loss: 451.44627 | Train Acc: 105609.37500\n",
            "Train Loss: 451.68210 | Train Acc: 105700.00000\n",
            "Train Loss: 452.18080 | Train Acc: 105778.12500\n",
            "Train Loss: 452.54310 | Train Acc: 105871.87500\n",
            "Train Loss: 452.72465 | Train Acc: 105965.62500\n",
            "Train Loss: 453.08284 | Train Acc: 106046.87500\n",
            "Train Loss: 453.58870 | Train Acc: 106128.12500\n",
            "Train Loss: 454.00926 | Train Acc: 106212.50000\n",
            "Train Loss: 454.72068 | Train Acc: 106293.75000\n",
            "Train Loss: 455.13711 | Train Acc: 106371.87500\n",
            "Train Loss: 455.44348 | Train Acc: 106459.37500\n",
            "Train Loss: 455.83615 | Train Acc: 106550.00000\n",
            "Train Loss: 455.99897 | Train Acc: 106640.62500\n",
            "Train Loss: 456.30049 | Train Acc: 106728.12500\n",
            "Train Loss: 456.48879 | Train Acc: 106821.87500\n",
            "Train Loss: 456.72783 | Train Acc: 106912.50000\n",
            "Train Loss: 457.13094 | Train Acc: 106993.75000\n",
            "Train Loss: 457.44737 | Train Acc: 107081.25000\n",
            "Train Loss: 457.73945 | Train Acc: 107171.87500\n",
            "Train Loss: 458.09504 | Train Acc: 107256.25000\n",
            "Train Loss: 458.58699 | Train Acc: 107334.37500\n",
            "Train Loss: 459.02191 | Train Acc: 107421.87500\n",
            "Train Loss: 459.37852 | Train Acc: 107509.37500\n",
            "Train Loss: 459.72087 | Train Acc: 107596.87500\n",
            "Train Loss: 459.88174 | Train Acc: 107690.62500\n",
            "Train Loss: 460.35687 | Train Acc: 107775.00000\n",
            "Train Loss: 460.75152 | Train Acc: 107862.50000\n",
            "Train Loss: 461.22882 | Train Acc: 107940.62500\n",
            "Train Loss: 461.47461 | Train Acc: 108034.37500\n",
            "Train Loss: 461.95339 | Train Acc: 108118.75000\n",
            "Train Loss: 462.31565 | Train Acc: 108206.25000\n",
            "Train Loss: 462.67106 | Train Acc: 108290.62500\n",
            "Train Loss: 463.41036 | Train Acc: 108371.87500\n",
            "Train Loss: 463.71519 | Train Acc: 108462.50000\n",
            "Train Loss: 464.04826 | Train Acc: 108550.00000\n",
            "Train Loss: 464.63825 | Train Acc: 108618.75000\n",
            "Train Loss: 465.20647 | Train Acc: 108706.25000\n",
            "Train Loss: 465.59430 | Train Acc: 108790.62500\n",
            "Train Loss: 465.76666 | Train Acc: 108887.50000\n",
            "Train Loss: 466.06340 | Train Acc: 108978.12500\n",
            "Train Loss: 466.27048 | Train Acc: 109071.87500\n",
            "Train Loss: 466.47073 | Train Acc: 109168.75000\n",
            "Train Loss: 466.86297 | Train Acc: 109256.25000\n",
            "Train Loss: 467.13480 | Train Acc: 109346.87500\n",
            "Train Loss: 467.47342 | Train Acc: 109431.25000\n",
            "Train Loss: 467.96813 | Train Acc: 109521.87500\n",
            "Train Loss: 468.30762 | Train Acc: 109612.50000\n",
            "Train Loss: 468.63889 | Train Acc: 109696.87500\n",
            "Train Loss: 468.98533 | Train Acc: 109781.25000\n",
            "Train Loss: 469.29395 | Train Acc: 109865.62500\n",
            "Train Loss: 469.40818 | Train Acc: 109965.62500\n",
            "Train Loss: 469.92371 | Train Acc: 110050.00000\n",
            "Train Loss: 470.18942 | Train Acc: 110140.62500\n",
            "Train Loss: 470.50178 | Train Acc: 110234.37500\n",
            "Train Loss: 470.86343 | Train Acc: 110315.62500\n",
            "Train Loss: 471.07754 | Train Acc: 110406.25000\n",
            "Train Loss: 471.30689 | Train Acc: 110496.87500\n",
            "Train Loss: 471.64850 | Train Acc: 110584.37500\n",
            "Train Loss: 471.75669 | Train Acc: 110681.25000\n",
            "Train Loss: 472.02254 | Train Acc: 110771.87500\n",
            "Train Loss: 472.44117 | Train Acc: 110856.25000\n",
            "Train Loss: 473.05797 | Train Acc: 110934.37500\n",
            "Train Loss: 473.39546 | Train Acc: 111021.87500\n",
            "Train Loss: 473.58905 | Train Acc: 111112.50000\n",
            "Train Loss: 473.92512 | Train Acc: 111196.87500\n",
            "Train Loss: 474.65082 | Train Acc: 111275.00000\n",
            "Train Loss: 474.98352 | Train Acc: 111362.50000\n",
            "Train Loss: 475.38721 | Train Acc: 111446.87500\n",
            "Train Loss: 475.91355 | Train Acc: 111528.12500\n",
            "Train Loss: 476.19875 | Train Acc: 111612.50000\n",
            "Train Loss: 476.78933 | Train Acc: 111684.37500\n",
            "Train Loss: 477.20714 | Train Acc: 111765.62500\n",
            "Train Loss: 477.58688 | Train Acc: 111843.75000\n",
            "Train Loss: 478.10938 | Train Acc: 111928.12500\n",
            "Train Loss: 478.38974 | Train Acc: 112015.62500\n",
            "Train Loss: 479.06104 | Train Acc: 112093.75000\n",
            "Train Loss: 479.48468 | Train Acc: 112181.25000\n",
            "Train Loss: 479.96027 | Train Acc: 112256.25000\n",
            "Train Loss: 480.27508 | Train Acc: 112346.87500\n",
            "Train Loss: 480.82089 | Train Acc: 112425.00000\n",
            "Train Loss: 481.27462 | Train Acc: 112509.37500\n",
            "Train Loss: 481.70041 | Train Acc: 112593.75000\n",
            "Train Loss: 481.97848 | Train Acc: 112684.37500\n",
            "Train Loss: 482.30405 | Train Acc: 112768.75000\n",
            "Train Loss: 482.62087 | Train Acc: 112853.12500\n",
            "Train Loss: 482.94018 | Train Acc: 112934.37500\n",
            "Train Loss: 483.36255 | Train Acc: 113015.62500\n",
            "Train Loss: 483.91380 | Train Acc: 113093.75000\n",
            "Train Loss: 484.34364 | Train Acc: 113181.25000\n",
            "Train Loss: 484.69081 | Train Acc: 113265.62500\n",
            "Train Loss: 485.43169 | Train Acc: 113340.62500\n",
            "Train Loss: 485.63126 | Train Acc: 113434.37500\n",
            "Train Loss: 486.02272 | Train Acc: 113515.62500\n",
            "Train Loss: 486.19182 | Train Acc: 113609.37500\n",
            "Train Loss: 486.57184 | Train Acc: 113696.87500\n",
            "Train Loss: 486.77791 | Train Acc: 113787.50000\n",
            "Train Loss: 487.23799 | Train Acc: 113871.87500\n",
            "Train Loss: 487.57366 | Train Acc: 113962.50000\n",
            "Train Loss: 487.95038 | Train Acc: 114046.87500\n",
            "Train Loss: 488.28774 | Train Acc: 114140.62500\n",
            "Train Loss: 488.63608 | Train Acc: 114231.25000\n",
            "Train Loss: 488.88668 | Train Acc: 114321.87500\n",
            "Train Loss: 489.65791 | Train Acc: 114409.37500\n",
            "Train Loss: 490.02370 | Train Acc: 114500.00000\n",
            "Train Loss: 490.77346 | Train Acc: 114581.25000\n",
            "Train Loss: 490.97961 | Train Acc: 114675.00000\n",
            "Train Loss: 491.29053 | Train Acc: 114765.62500\n",
            "Train Loss: 491.72548 | Train Acc: 114850.00000\n",
            "Train Loss: 492.15040 | Train Acc: 114937.50000\n",
            "Train Loss: 492.40850 | Train Acc: 115031.25000\n",
            "Train Loss: 492.71556 | Train Acc: 115118.75000\n",
            "Train Loss: 493.36351 | Train Acc: 115196.87500\n",
            "Train Loss: 493.50447 | Train Acc: 115293.75000\n",
            "Train Loss: 493.86718 | Train Acc: 115384.37500\n",
            "Train Loss: 494.63143 | Train Acc: 115465.62500\n",
            "Train Loss: 495.11376 | Train Acc: 115553.12500\n",
            "Train Loss: 495.58832 | Train Acc: 115637.50000\n",
            "Train Loss: 495.84327 | Train Acc: 115731.25000\n",
            "Train Loss: 496.10310 | Train Acc: 115825.00000\n",
            "Train Loss: 496.79867 | Train Acc: 115903.12500\n",
            "Train Loss: 497.31612 | Train Acc: 115987.50000\n",
            "Train Loss: 497.78865 | Train Acc: 116068.75000\n",
            "Train Loss: 498.01440 | Train Acc: 116159.37500\n",
            "Train Loss: 498.37383 | Train Acc: 116240.62500\n",
            "Train Loss: 498.49379 | Train Acc: 116337.50000\n",
            "Train Loss: 498.65244 | Train Acc: 116431.25000\n",
            "Train Loss: 499.18755 | Train Acc: 116515.62500\n",
            "Train Loss: 499.51224 | Train Acc: 116606.25000\n",
            "Train Loss: 499.84674 | Train Acc: 116693.75000\n",
            "Train Loss: 500.10948 | Train Acc: 116781.25000\n",
            "Train Loss: 500.41882 | Train Acc: 116875.00000\n",
            "Train Loss: 500.80225 | Train Acc: 116965.62500\n",
            "Train Loss: 501.04474 | Train Acc: 117056.25000\n",
            "Train Loss: 501.55012 | Train Acc: 117134.37500\n",
            "Train Loss: 502.15318 | Train Acc: 117215.62500\n",
            "Train Loss: 502.63622 | Train Acc: 117300.00000\n",
            "Train Loss: 502.84959 | Train Acc: 117393.75000\n",
            "Train Loss: 503.18870 | Train Acc: 117484.37500\n",
            "Train Loss: 503.49483 | Train Acc: 117578.12500\n",
            "Train Loss: 503.80639 | Train Acc: 117665.62500\n",
            "Train Loss: 503.96770 | Train Acc: 117759.37500\n",
            "Train Loss: 504.52172 | Train Acc: 117843.75000\n",
            "Train Loss: 504.89999 | Train Acc: 117925.00000\n",
            "Train Loss: 505.37336 | Train Acc: 118006.25000\n",
            "Train Loss: 505.79066 | Train Acc: 118096.87500\n",
            "Train Loss: 506.02072 | Train Acc: 118184.37500\n",
            "Train Loss: 506.49974 | Train Acc: 118268.75000\n",
            "Train Loss: 506.85573 | Train Acc: 118353.12500\n",
            "Train Loss: 507.17608 | Train Acc: 118440.62500\n",
            "Train Loss: 507.66538 | Train Acc: 118518.75000\n",
            "Train Loss: 507.92281 | Train Acc: 118606.25000\n",
            "Train Loss: 508.18514 | Train Acc: 118693.75000\n",
            "Train Loss: 508.34995 | Train Acc: 118790.62500\n",
            "Train Loss: 508.69587 | Train Acc: 118881.25000\n",
            "Train Loss: 508.98054 | Train Acc: 118968.75000\n",
            "Train Loss: 509.32163 | Train Acc: 119053.12500\n",
            "Train Loss: 509.74635 | Train Acc: 119134.37500\n",
            "Train Loss: 510.04525 | Train Acc: 119221.87500\n",
            "Train Loss: 510.27861 | Train Acc: 119315.62500\n",
            "Train Loss: 510.53408 | Train Acc: 119409.37500\n",
            "Train Loss: 510.96756 | Train Acc: 119487.50000\n",
            "Train Loss: 511.22465 | Train Acc: 119584.37500\n",
            "Train Loss: 511.73460 | Train Acc: 119665.62500\n",
            "Train Loss: 512.14241 | Train Acc: 119746.87500\n",
            "Train Loss: 512.52063 | Train Acc: 119828.12500\n",
            "Train Loss: 512.77167 | Train Acc: 119915.62500\n",
            "Train Loss: 513.15304 | Train Acc: 120003.12500\n",
            "Train Loss: 513.53340 | Train Acc: 120081.25000\n",
            "Train Loss: 513.80851 | Train Acc: 120168.75000\n",
            "Train Loss: 514.17841 | Train Acc: 120259.37500\n",
            "Train Loss: 514.47727 | Train Acc: 120350.00000\n",
            "Train Loss: 514.85297 | Train Acc: 120437.50000\n",
            "Train Loss: 515.09239 | Train Acc: 120528.12500\n",
            "Train Loss: 515.55231 | Train Acc: 120615.62500\n",
            "Train Loss: 515.80620 | Train Acc: 120706.25000\n",
            "Train Loss: 516.14750 | Train Acc: 120793.75000\n",
            "Train Loss: 516.36667 | Train Acc: 120887.50000\n",
            "Train Loss: 516.76279 | Train Acc: 120975.00000\n",
            "Train Loss: 517.05476 | Train Acc: 121068.75000\n",
            "Train Loss: 517.58566 | Train Acc: 121150.00000\n",
            "Train Loss: 517.90049 | Train Acc: 121234.37500\n",
            "Train Loss: 518.10899 | Train Acc: 121328.12500\n",
            "Train Loss: 518.25720 | Train Acc: 121425.00000\n",
            "Train Loss: 518.65707 | Train Acc: 121509.37500\n",
            "Train Loss: 519.01520 | Train Acc: 121596.87500\n",
            "Train Loss: 519.24049 | Train Acc: 121687.50000\n",
            "Train Loss: 519.49241 | Train Acc: 121775.00000\n",
            "Train Loss: 519.71852 | Train Acc: 121868.75000\n",
            "Train Loss: 520.15267 | Train Acc: 121953.12500\n",
            "Train Loss: 520.51304 | Train Acc: 122037.50000\n",
            "Train Loss: 520.91563 | Train Acc: 122115.62500\n",
            "Train Loss: 521.35861 | Train Acc: 122203.12500\n",
            "Train Loss: 521.95480 | Train Acc: 122281.25000\n",
            "Train Loss: 522.30678 | Train Acc: 122368.75000\n",
            "Train Loss: 522.50683 | Train Acc: 122462.50000\n",
            "Train Loss: 522.86181 | Train Acc: 122553.12500\n",
            "Train Loss: 522.98913 | Train Acc: 122646.87500\n",
            "Train Loss: 523.43970 | Train Acc: 122728.12500\n",
            "Train Loss: 523.64664 | Train Acc: 122821.87500\n",
            "Train Loss: 524.14021 | Train Acc: 122903.12500\n",
            "Train Loss: 524.76593 | Train Acc: 122981.25000\n",
            "Train Loss: 525.09133 | Train Acc: 123068.75000\n",
            "Train Loss: 525.23796 | Train Acc: 123165.62500\n",
            "Train Loss: 525.45082 | Train Acc: 123256.25000\n",
            "Train Loss: 526.00222 | Train Acc: 123337.50000\n",
            "Train Loss: 526.58844 | Train Acc: 123415.62500\n",
            "Train Loss: 527.23721 | Train Acc: 123496.87500\n",
            "Train Loss: 527.69095 | Train Acc: 123581.25000\n",
            "Train Loss: 528.05105 | Train Acc: 123665.62500\n",
            "Train Loss: 528.43539 | Train Acc: 123753.12500\n",
            "Train Loss: 528.72951 | Train Acc: 123840.62500\n",
            "Train Loss: 529.56795 | Train Acc: 123915.62500\n",
            "Train Loss: 530.02001 | Train Acc: 124003.12500\n",
            "Train Loss: 530.61656 | Train Acc: 124087.50000\n",
            "Train Loss: 530.96370 | Train Acc: 124178.12500\n",
            "Train Loss: 531.56870 | Train Acc: 124259.37500\n",
            "Train Loss: 531.77135 | Train Acc: 124356.25000\n",
            "Train Loss: 532.05298 | Train Acc: 124446.87500\n",
            "Train Loss: 532.23602 | Train Acc: 124543.75000\n",
            "Train Loss: 532.68134 | Train Acc: 124631.25000\n",
            "Train Loss: 533.15383 | Train Acc: 124715.62500\n",
            "Train Loss: 533.48855 | Train Acc: 124803.12500\n",
            "Train Loss: 533.84673 | Train Acc: 124887.50000\n",
            "Train Loss: 534.04853 | Train Acc: 124981.25000\n",
            "Train Loss: 534.45516 | Train Acc: 125071.87500\n",
            "Train Loss: 534.64224 | Train Acc: 125165.62500\n",
            "Train Loss: 534.98991 | Train Acc: 125256.25000\n",
            "Train Loss: 535.43526 | Train Acc: 125346.87500\n",
            "Train Loss: 535.69336 | Train Acc: 125437.50000\n",
            "Train Loss: 535.95503 | Train Acc: 125528.12500\n",
            "Train Loss: 536.21872 | Train Acc: 125621.87500\n",
            "Train Loss: 536.56014 | Train Acc: 125712.50000\n",
            "Train Loss: 536.95883 | Train Acc: 125790.62500\n",
            "Train Loss: 537.15329 | Train Acc: 125884.37500\n",
            "Train Loss: 537.61160 | Train Acc: 125968.75000\n",
            "Train Loss: 537.89515 | Train Acc: 126056.25000\n",
            "Train Loss: 538.19978 | Train Acc: 126146.87500\n",
            "Train Loss: 538.40876 | Train Acc: 126237.50000\n",
            "Train Loss: 538.84482 | Train Acc: 126325.00000\n",
            "Train Loss: 539.38993 | Train Acc: 126406.25000\n",
            "Train Loss: 539.89444 | Train Acc: 126484.37500\n",
            "Train Loss: 540.28788 | Train Acc: 126568.75000\n",
            "Train Loss: 540.70500 | Train Acc: 126653.12500\n",
            "Train Loss: 540.99721 | Train Acc: 126743.75000\n",
            "Train Loss: 541.26115 | Train Acc: 126834.37500\n",
            "Train Loss: 541.51684 | Train Acc: 126921.87500\n",
            "Train Loss: 541.93150 | Train Acc: 127006.25000\n",
            "Train Loss: 542.40005 | Train Acc: 127087.50000\n",
            "Train Loss: 542.65239 | Train Acc: 127175.00000\n",
            "Train Loss: 543.04660 | Train Acc: 127259.37500\n",
            "Train Loss: 543.24129 | Train Acc: 127350.00000\n",
            "Train Loss: 543.51219 | Train Acc: 127443.75000\n",
            "Train Loss: 543.91437 | Train Acc: 127531.25000\n",
            "Train Loss: 544.11252 | Train Acc: 127621.87500\n",
            "Train Loss: 544.47841 | Train Acc: 127706.25000\n",
            "Train Loss: 544.76321 | Train Acc: 127793.75000\n",
            "Train Loss: 545.23644 | Train Acc: 127878.12500\n",
            "Train Loss: 545.56499 | Train Acc: 127965.62500\n",
            "Train Loss: 545.74471 | Train Acc: 128056.25000\n",
            "Train Loss: 545.94291 | Train Acc: 128150.00000\n",
            "Train Loss: 546.44293 | Train Acc: 128237.50000\n",
            "Train Loss: 546.72166 | Train Acc: 128331.25000\n",
            "Train Loss: 547.03244 | Train Acc: 128412.50000\n",
            "Train Loss: 547.41327 | Train Acc: 128500.00000\n",
            "Train Loss: 547.65847 | Train Acc: 128596.87500\n",
            "Train Loss: 548.05799 | Train Acc: 128678.12500\n",
            "Train Loss: 548.42631 | Train Acc: 128762.50000\n",
            "Train Loss: 549.13080 | Train Acc: 128834.37500\n",
            "Train Loss: 549.34236 | Train Acc: 128928.12500\n",
            "Train Loss: 549.67209 | Train Acc: 129015.62500\n",
            "Train Loss: 550.11417 | Train Acc: 129096.87500\n",
            "Train Loss: 550.77757 | Train Acc: 129168.75000\n",
            "Train Loss: 551.16137 | Train Acc: 129256.25000\n",
            "Train Loss: 551.62381 | Train Acc: 129340.62500\n",
            "Train Loss: 552.10169 | Train Acc: 129425.00000\n",
            "Train Loss: 552.47732 | Train Acc: 129506.25000\n",
            "Train Loss: 552.83185 | Train Acc: 129600.00000\n",
            "Train Loss: 553.21473 | Train Acc: 129687.50000\n",
            "Train Loss: 553.59376 | Train Acc: 129765.62500\n",
            "Train Loss: 553.96815 | Train Acc: 129850.00000\n",
            "Train Loss: 554.37042 | Train Acc: 129928.12500\n",
            "Train Loss: 554.52228 | Train Acc: 130025.00000\n",
            "Train Loss: 555.22219 | Train Acc: 130109.37500\n",
            "Train Loss: 555.60297 | Train Acc: 130193.75000\n",
            "Train Loss: 555.86219 | Train Acc: 130281.25000\n",
            "Train Loss: 556.18741 | Train Acc: 130359.37500\n",
            "Train Loss: 556.51548 | Train Acc: 130450.00000\n",
            "Train Loss: 557.09961 | Train Acc: 130531.25000\n",
            "Train Loss: 557.47786 | Train Acc: 130621.87500\n",
            "Train Loss: 557.53848 | Train Acc: 130721.87500\n",
            "Train Loss: 557.80202 | Train Acc: 130812.50000\n",
            "Train Loss: 558.14380 | Train Acc: 130900.00000\n",
            "Train Loss: 558.37951 | Train Acc: 130987.50000\n",
            "Train Loss: 558.91615 | Train Acc: 131071.87500\n",
            "Train Loss: 559.25814 | Train Acc: 131156.25000\n",
            "Train Loss: 559.51847 | Train Acc: 131246.87500\n",
            "Train Loss: 560.19120 | Train Acc: 131331.25000\n",
            "Train Loss: 560.40859 | Train Acc: 131425.00000\n",
            "Train Loss: 560.66307 | Train Acc: 131512.50000\n",
            "Train Loss: 560.86512 | Train Acc: 131609.37500\n",
            "Train Loss: 561.36964 | Train Acc: 131690.62500\n",
            "Train Loss: 561.68318 | Train Acc: 131778.12500\n",
            "Train Loss: 562.03771 | Train Acc: 131865.62500\n",
            "Train Loss: 562.25718 | Train Acc: 131959.37500\n",
            "Train Loss: 562.93285 | Train Acc: 132037.50000\n",
            "Train Loss: 563.22530 | Train Acc: 132125.00000\n",
            "Train Loss: 563.42592 | Train Acc: 132221.87500\n",
            "Train Loss: 563.82129 | Train Acc: 132306.25000\n",
            "Train Loss: 564.07595 | Train Acc: 132403.12500\n",
            "Train Loss: 564.41940 | Train Acc: 132490.62500\n",
            "Train Loss: 564.63781 | Train Acc: 132587.50000\n",
            "Train Loss: 565.07373 | Train Acc: 132671.87500\n",
            "Train Loss: 565.38105 | Train Acc: 132765.62500\n",
            "Train Loss: 565.66137 | Train Acc: 132856.25000\n",
            "Train Loss: 566.12041 | Train Acc: 132943.75000\n",
            "Train Loss: 566.81396 | Train Acc: 133025.00000\n",
            "Train Loss: 567.56348 | Train Acc: 133103.12500\n",
            "Train Loss: 567.92919 | Train Acc: 133187.50000\n",
            "Train Loss: 568.29048 | Train Acc: 133278.12500\n",
            "Train Loss: 568.66593 | Train Acc: 133362.50000\n",
            "Train Loss: 568.89998 | Train Acc: 133456.25000\n",
            "Train Loss: 569.24882 | Train Acc: 133550.00000\n",
            "Train Loss: 569.96848 | Train Acc: 133640.62500\n",
            "Train Loss: 570.27920 | Train Acc: 133725.00000\n",
            "Train Loss: 570.60582 | Train Acc: 133806.25000\n",
            "Train Loss: 570.80079 | Train Acc: 133900.00000\n",
            "Train Loss: 571.48010 | Train Acc: 133971.87500\n",
            "Train Loss: 571.59018 | Train Acc: 134071.87500\n",
            "Train Loss: 571.95261 | Train Acc: 134156.25000\n",
            "Train Loss: 572.24438 | Train Acc: 134243.75000\n",
            "Train Loss: 572.54082 | Train Acc: 134331.25000\n",
            "Train Loss: 572.83333 | Train Acc: 134421.87500\n",
            "Train Loss: 573.10303 | Train Acc: 134515.62500\n",
            "Train Loss: 573.80939 | Train Acc: 134593.75000\n",
            "Train Loss: 574.29100 | Train Acc: 134681.25000\n",
            "Train Loss: 574.72618 | Train Acc: 134768.75000\n",
            "Train Loss: 575.06879 | Train Acc: 134853.12500\n",
            "Train Loss: 575.56929 | Train Acc: 134937.50000\n",
            "Train Loss: 575.99664 | Train Acc: 135018.75000\n",
            "Train Loss: 576.26717 | Train Acc: 135112.50000\n",
            "Train Loss: 576.56422 | Train Acc: 135203.12500\n",
            "Train Loss: 577.26166 | Train Acc: 135284.37500\n",
            "Train Loss: 577.57083 | Train Acc: 135368.75000\n",
            "Train Loss: 577.83216 | Train Acc: 135456.25000\n",
            "Train Loss: 578.29723 | Train Acc: 135537.50000\n",
            "Train Loss: 578.56214 | Train Acc: 135628.12500\n",
            "Train Loss: 578.92480 | Train Acc: 135709.37500\n",
            "Train Loss: 579.40514 | Train Acc: 135793.75000\n",
            "Train Loss: 579.82755 | Train Acc: 135871.87500\n",
            "Train Loss: 580.16678 | Train Acc: 135965.62500\n",
            "Train Loss: 580.45566 | Train Acc: 136056.25000\n",
            "Train Loss: 580.88498 | Train Acc: 136140.62500\n",
            "Train Loss: 581.20550 | Train Acc: 136228.12500\n",
            "Train Loss: 581.53634 | Train Acc: 136309.37500\n",
            "Train Loss: 581.71207 | Train Acc: 136406.25000\n",
            "Train Loss: 582.13328 | Train Acc: 136493.75000\n",
            "Train Loss: 582.75859 | Train Acc: 136575.00000\n",
            "Train Loss: 582.94066 | Train Acc: 136668.75000\n",
            "Train Loss: 583.13766 | Train Acc: 136756.25000\n",
            "Train Loss: 583.26673 | Train Acc: 136853.12500\n",
            "Train Loss: 583.86099 | Train Acc: 136943.75000\n",
            "Train Loss: 584.25214 | Train Acc: 137034.37500\n",
            "Train Loss: 584.49655 | Train Acc: 137128.12500\n",
            "Train Loss: 584.96377 | Train Acc: 137206.25000\n",
            "Train Loss: 585.46929 | Train Acc: 137293.75000\n",
            "Train Loss: 585.84309 | Train Acc: 137384.37500\n",
            "Train Loss: 586.19751 | Train Acc: 137468.75000\n",
            "Train Loss: 586.51828 | Train Acc: 137559.37500\n",
            "Train Loss: 587.19693 | Train Acc: 137646.87500\n",
            "Train Loss: 587.66722 | Train Acc: 137721.87500\n",
            "Train Loss: 588.08802 | Train Acc: 137800.00000\n",
            "Train Loss: 588.66611 | Train Acc: 137875.00000\n",
            "Train Loss: 588.95425 | Train Acc: 137965.62500\n",
            "Train Loss: 589.40580 | Train Acc: 138050.00000\n",
            "Train Loss: 589.77703 | Train Acc: 138137.50000\n",
            "Train Loss: 590.30203 | Train Acc: 138221.87500\n",
            "Train Loss: 590.50784 | Train Acc: 138315.62500\n",
            "Train Loss: 590.72406 | Train Acc: 138409.37500\n",
            "Train Loss: 591.07947 | Train Acc: 138496.87500\n",
            "Train Loss: 591.32524 | Train Acc: 138587.50000\n",
            "Train Loss: 591.70592 | Train Acc: 138671.87500\n",
            "Train Loss: 592.16454 | Train Acc: 138750.00000\n",
            "Train Loss: 592.40717 | Train Acc: 138840.62500\n",
            "Train Loss: 593.11209 | Train Acc: 138918.75000\n",
            "Train Loss: 593.51306 | Train Acc: 139003.12500\n",
            "Train Loss: 593.68924 | Train Acc: 139100.00000\n",
            "Train Loss: 593.99025 | Train Acc: 139187.50000\n",
            "Train Loss: 594.29249 | Train Acc: 139275.00000\n",
            "Train Loss: 594.74390 | Train Acc: 139362.50000\n",
            "Train Loss: 594.97784 | Train Acc: 139456.25000\n",
            "Train Loss: 595.48023 | Train Acc: 139537.50000\n",
            "Train Loss: 595.93132 | Train Acc: 139625.00000\n",
            "Train Loss: 596.19224 | Train Acc: 139715.62500\n",
            "Train Loss: 597.01457 | Train Acc: 139793.75000\n",
            "Train Loss: 597.39631 | Train Acc: 139878.12500\n",
            "Train Loss: 597.83704 | Train Acc: 139959.37500\n",
            "Train Loss: 598.14029 | Train Acc: 140043.75000\n",
            "Train Loss: 598.50967 | Train Acc: 140125.00000\n",
            "Train Loss: 598.72234 | Train Acc: 140218.75000\n",
            "Train Loss: 599.17716 | Train Acc: 140303.12500\n",
            "Train Loss: 599.68194 | Train Acc: 140384.37500\n",
            "Train Loss: 599.89531 | Train Acc: 140475.00000\n",
            "Train Loss: 600.10319 | Train Acc: 140568.75000\n",
            "Train Loss: 600.19925 | Train Acc: 140668.75000\n",
            "Train Loss: 600.42694 | Train Acc: 140759.37500\n",
            "Train Loss: 600.63067 | Train Acc: 140853.12500\n",
            "Train Loss: 600.79560 | Train Acc: 140946.87500\n",
            "Train Loss: 601.19682 | Train Acc: 141031.25000\n",
            "Train Loss: 601.55303 | Train Acc: 141118.75000\n",
            "Train Loss: 602.19063 | Train Acc: 141200.00000\n",
            "Train Loss: 602.68172 | Train Acc: 141284.37500\n",
            "Train Loss: 602.80406 | Train Acc: 141384.37500\n",
            "Train Loss: 603.11033 | Train Acc: 141475.00000\n",
            "Train Loss: 603.31173 | Train Acc: 141565.62500\n",
            "Train Loss: 604.02529 | Train Acc: 141650.00000\n",
            "Train Loss: 604.16706 | Train Acc: 141746.87500\n",
            "Train Loss: 604.62188 | Train Acc: 141828.12500\n",
            "Train Loss: 604.98808 | Train Acc: 141912.50000\n",
            "Train Loss: 605.23692 | Train Acc: 142000.00000\n",
            "Train Loss: 605.42381 | Train Acc: 142093.75000\n",
            "Train Loss: 605.81349 | Train Acc: 142184.37500\n",
            "Train Loss: 606.16028 | Train Acc: 142275.00000\n",
            "Train Loss: 606.59469 | Train Acc: 142359.37500\n",
            "Train Loss: 607.06663 | Train Acc: 142446.87500\n",
            "Train Loss: 607.35089 | Train Acc: 142534.37500\n",
            "Train Loss: 607.72049 | Train Acc: 142618.75000\n",
            "Train Loss: 607.87579 | Train Acc: 142715.62500\n",
            "Train Loss: 608.24036 | Train Acc: 142806.25000\n",
            "Train Loss: 608.56036 | Train Acc: 142884.37500\n",
            "Train Loss: 608.70773 | Train Acc: 142981.25000\n",
            "Train Loss: 609.19865 | Train Acc: 143059.37500\n",
            "Train Loss: 609.53437 | Train Acc: 143146.87500\n",
            "Train Loss: 610.04114 | Train Acc: 143231.25000\n",
            "Train Loss: 610.21101 | Train Acc: 143328.12500\n",
            "Train Loss: 610.42413 | Train Acc: 143421.87500\n",
            "Train Loss: 610.88078 | Train Acc: 143500.00000\n",
            "Train Loss: 611.21361 | Train Acc: 143584.37500\n",
            "Train Loss: 611.77830 | Train Acc: 143662.50000\n",
            "Train Loss: 612.11310 | Train Acc: 143750.00000\n",
            "Train Loss: 612.43411 | Train Acc: 143834.37500\n",
            "Train Loss: 612.72960 | Train Acc: 143925.00000\n",
            "Train Loss: 612.92078 | Train Acc: 144018.75000\n",
            "Train Loss: 613.09689 | Train Acc: 144112.50000\n",
            "Train Loss: 613.47334 | Train Acc: 144196.87500\n",
            "Train Loss: 613.98353 | Train Acc: 144278.12500\n",
            "Train Loss: 614.49259 | Train Acc: 144359.37500\n",
            "Train Loss: 614.86240 | Train Acc: 144446.87500\n",
            "Train Loss: 615.10909 | Train Acc: 144543.75000\n",
            "Train Loss: 615.47501 | Train Acc: 144631.25000\n",
            "Train Loss: 615.80610 | Train Acc: 144715.62500\n",
            "Train Loss: 616.20979 | Train Acc: 144806.25000\n",
            "Train Loss: 616.57259 | Train Acc: 144890.62500\n",
            "Train Loss: 617.09504 | Train Acc: 144962.50000\n",
            "Train Loss: 617.43192 | Train Acc: 145053.12500\n",
            "Train Loss: 617.69154 | Train Acc: 145143.75000\n",
            "Train Loss: 618.29735 | Train Acc: 145218.75000\n",
            "Train Loss: 618.51844 | Train Acc: 145312.50000\n",
            "Train Loss: 618.77463 | Train Acc: 145406.25000\n",
            "Train Loss: 618.93849 | Train Acc: 145500.00000\n",
            "Train Loss: 619.13772 | Train Acc: 145584.37500\n",
            "Train Loss: 619.80603 | Train Acc: 145656.25000\n",
            "Train Loss: 620.36043 | Train Acc: 145728.12500\n",
            "Train Loss: 620.83024 | Train Acc: 145812.50000\n",
            "Train Loss: 621.04200 | Train Acc: 145906.25000\n",
            "Train Loss: 621.33595 | Train Acc: 145990.62500\n",
            "Train Loss: 621.76458 | Train Acc: 146075.00000\n",
            "Train Loss: 622.00348 | Train Acc: 146165.62500\n",
            "Train Loss: 622.44073 | Train Acc: 146250.00000\n",
            "Train Loss: 622.98329 | Train Acc: 146334.37500\n",
            "Train Loss: 623.27335 | Train Acc: 146418.75000\n",
            "Train Loss: 623.80483 | Train Acc: 146500.00000\n",
            "Train Loss: 624.06135 | Train Acc: 146590.62500\n",
            "Train Loss: 624.35356 | Train Acc: 146684.37500\n",
            "Train Loss: 624.67120 | Train Acc: 146765.62500\n",
            "Train Loss: 625.09060 | Train Acc: 146853.12500\n",
            "Train Loss: 625.44393 | Train Acc: 146937.50000\n",
            "Train Loss: 625.54191 | Train Acc: 147034.37500\n",
            "Train Loss: 625.90159 | Train Acc: 147125.00000\n",
            "Train Loss: 626.20526 | Train Acc: 147212.50000\n",
            "Train Loss: 626.77150 | Train Acc: 147300.00000\n",
            "Train Loss: 627.01556 | Train Acc: 147393.75000\n",
            "Train Loss: 627.77903 | Train Acc: 147468.75000\n",
            "Train Loss: 628.22074 | Train Acc: 147556.25000\n",
            "Train Loss: 628.30723 | Train Acc: 147656.25000\n",
            "Train Loss: 628.57347 | Train Acc: 147746.87500\n",
            "Train Loss: 629.13325 | Train Acc: 147831.25000\n",
            "Train Loss: 629.42830 | Train Acc: 147921.87500\n",
            "Train Loss: 629.96784 | Train Acc: 148006.25000\n",
            "Train Loss: 630.27705 | Train Acc: 148096.87500\n",
            "Train Loss: 630.59655 | Train Acc: 148181.25000\n",
            "Train Loss: 630.69408 | Train Acc: 148278.12500\n",
            "Train Loss: 631.34735 | Train Acc: 148368.75000\n",
            "Train Loss: 631.55233 | Train Acc: 148462.50000\n",
            "Train Loss: 631.71856 | Train Acc: 148556.25000\n",
            "Train Loss: 632.21751 | Train Acc: 148640.62500\n",
            "Train Loss: 632.42499 | Train Acc: 148734.37500\n",
            "Train Loss: 632.83207 | Train Acc: 148821.87500\n",
            "Train Loss: 633.32803 | Train Acc: 148900.00000\n",
            "Train Loss: 633.53571 | Train Acc: 148993.75000\n",
            "Train Loss: 633.89139 | Train Acc: 149081.25000\n",
            "Train Loss: 634.02778 | Train Acc: 149178.12500\n",
            "Train Loss: 634.54441 | Train Acc: 149262.50000\n",
            "Train Loss: 634.77837 | Train Acc: 149356.25000\n",
            "Train Loss: 635.31718 | Train Acc: 149437.50000\n",
            "Train Loss: 636.20647 | Train Acc: 149500.00000\n",
            "Train Loss: 636.62107 | Train Acc: 149581.25000\n",
            "Train Loss: 636.82093 | Train Acc: 149671.87500\n",
            "Train Loss: 637.10654 | Train Acc: 149762.50000\n",
            "Train Loss: 637.55025 | Train Acc: 149843.75000\n",
            "Train Loss: 637.71206 | Train Acc: 149934.37500\n",
            "Train Loss: 638.05147 | Train Acc: 150021.87500\n",
            "Train Loss: 638.73849 | Train Acc: 150096.87500\n",
            "Train Loss: 639.06165 | Train Acc: 150184.37500\n",
            "Train Loss: 639.40418 | Train Acc: 150265.62500\n",
            "Train Loss: 639.86887 | Train Acc: 150346.87500\n",
            "Train Loss: 640.54829 | Train Acc: 150425.00000\n",
            "Train Loss: 640.75422 | Train Acc: 150515.62500\n",
            "Train Loss: 641.15416 | Train Acc: 150603.12500\n",
            "Train Loss: 641.54698 | Train Acc: 150687.50000\n",
            "Train Loss: 642.38414 | Train Acc: 150765.62500\n",
            "Train Loss: 642.83532 | Train Acc: 150856.25000\n",
            "Train Loss: 643.18186 | Train Acc: 150940.62500\n",
            "Train Loss: 643.57371 | Train Acc: 151025.00000\n",
            "Train Loss: 644.26870 | Train Acc: 151106.25000\n",
            "Train Loss: 644.51852 | Train Acc: 151196.87500\n",
            "Train Loss: 644.69900 | Train Acc: 151293.75000\n",
            "Train Loss: 645.25287 | Train Acc: 151375.00000\n",
            "Train Loss: 645.77835 | Train Acc: 151456.25000\n",
            "Train Loss: 646.34242 | Train Acc: 151543.75000\n",
            "Train Loss: 646.63409 | Train Acc: 151634.37500\n",
            "Train Loss: 647.07993 | Train Acc: 151718.75000\n",
            "Train Loss: 647.39910 | Train Acc: 151809.37500\n",
            "Train Loss: 647.75623 | Train Acc: 151896.87500\n",
            "Train Loss: 647.98890 | Train Acc: 151987.50000\n",
            "Train Loss: 648.24691 | Train Acc: 152075.00000\n",
            "Train Loss: 648.54931 | Train Acc: 152165.62500\n",
            "Train Loss: 648.74508 | Train Acc: 152262.50000\n",
            "Train Loss: 649.32647 | Train Acc: 152340.62500\n",
            "Train Loss: 649.67702 | Train Acc: 152428.12500\n",
            "Train Loss: 650.01613 | Train Acc: 152512.50000\n",
            "Train Loss: 650.26111 | Train Acc: 152606.25000\n",
            "Train Loss: 650.48011 | Train Acc: 152696.87500\n",
            "Train Loss: 650.91266 | Train Acc: 152784.37500\n",
            "Train Loss: 651.15878 | Train Acc: 152875.00000\n",
            "Train Loss: 651.44786 | Train Acc: 152965.62500\n",
            "Train Loss: 651.89996 | Train Acc: 153050.00000\n",
            "Train Loss: 652.22642 | Train Acc: 153137.50000\n",
            "Train Loss: 652.55774 | Train Acc: 153225.00000\n",
            "Train Loss: 652.87345 | Train Acc: 153315.62500\n",
            "Train Loss: 653.19502 | Train Acc: 153403.12500\n",
            "Train Loss: 653.51881 | Train Acc: 153493.75000\n",
            "Train Loss: 653.78632 | Train Acc: 153584.37500\n",
            "Train Loss: 654.11996 | Train Acc: 153671.87500\n",
            "Train Loss: 654.31694 | Train Acc: 153765.62500\n",
            "Train Loss: 654.65198 | Train Acc: 153843.75000\n",
            "Train Loss: 655.03836 | Train Acc: 153925.00000\n",
            "Train Loss: 655.29921 | Train Acc: 154015.62500\n",
            "Train Loss: 655.57398 | Train Acc: 154109.37500\n",
            "Train Loss: 655.74626 | Train Acc: 154203.12500\n",
            "Train Loss: 656.06257 | Train Acc: 154290.62500\n",
            "Train Loss: 656.54995 | Train Acc: 154371.87500\n",
            "Train Loss: 657.11164 | Train Acc: 154459.37500\n",
            "Train Loss: 657.47408 | Train Acc: 154550.00000\n",
            "Train Loss: 657.67782 | Train Acc: 154637.50000\n",
            "Train Loss: 658.00687 | Train Acc: 154725.00000\n",
            "Train Loss: 658.44007 | Train Acc: 154809.37500\n",
            "Train Loss: 658.81221 | Train Acc: 154893.75000\n",
            "Train Loss: 659.24527 | Train Acc: 154975.00000\n",
            "Train Loss: 659.61480 | Train Acc: 155059.37500\n",
            "Train Loss: 659.95344 | Train Acc: 155140.62500\n",
            "Train Loss: 660.28936 | Train Acc: 155228.12500\n",
            "Train Loss: 660.60847 | Train Acc: 155318.75000\n",
            "Train Loss: 661.33269 | Train Acc: 155400.00000\n",
            "Train Loss: 661.83040 | Train Acc: 155484.37500\n",
            "Train Loss: 662.11343 | Train Acc: 155571.87500\n",
            "Train Loss: 662.34713 | Train Acc: 155665.62500\n",
            "Train Loss: 662.71834 | Train Acc: 155756.25000\n",
            "Train Loss: 662.90214 | Train Acc: 155846.87500\n",
            "Train Loss: 663.48887 | Train Acc: 155918.75000\n",
            "Train Loss: 663.82659 | Train Acc: 156003.12500\n",
            "Train Loss: 664.15625 | Train Acc: 156093.75000\n",
            "Train Loss: 664.51189 | Train Acc: 156178.12500\n",
            "Train Loss: 664.77631 | Train Acc: 156262.50000\n",
            "Train Loss: 665.17344 | Train Acc: 156350.00000\n",
            "Train Loss: 665.60673 | Train Acc: 156428.12500\n",
            "Train Loss: 665.81226 | Train Acc: 156521.87500\n",
            "Train Loss: 666.03191 | Train Acc: 156615.62500\n",
            "Train Loss: 666.22970 | Train Acc: 156709.37500\n",
            "Train Loss: 666.42574 | Train Acc: 156803.12500\n",
            "Train Loss: 666.61615 | Train Acc: 156896.87500\n",
            "Train Loss: 666.89215 | Train Acc: 156990.62500\n",
            "Train Loss: 667.15813 | Train Acc: 157084.37500\n",
            "Train Loss: 667.56479 | Train Acc: 157175.00000\n",
            "Train Loss: 667.83797 | Train Acc: 157262.50000\n",
            "Train Loss: 668.51175 | Train Acc: 157340.62500\n",
            "Train Loss: 669.12208 | Train Acc: 157412.50000\n",
            "Train Loss: 669.48005 | Train Acc: 157500.00000\n",
            "Train Loss: 669.96599 | Train Acc: 157584.37500\n",
            "Train Loss: 670.28202 | Train Acc: 157668.75000\n",
            "Train Loss: 670.39751 | Train Acc: 157765.62500\n",
            "Train Loss: 670.51561 | Train Acc: 157862.50000\n",
            "Train Loss: 670.82945 | Train Acc: 157950.00000\n",
            "Train Loss: 671.08692 | Train Acc: 158046.87500\n",
            "Train Loss: 671.67210 | Train Acc: 158125.00000\n",
            "Train Loss: 671.88333 | Train Acc: 158221.87500\n",
            "Train Loss: 672.18917 | Train Acc: 158315.62500\n",
            "Train Loss: 672.62723 | Train Acc: 158400.00000\n",
            "Train Loss: 673.35691 | Train Acc: 158471.87500\n",
            "Train Loss: 673.58512 | Train Acc: 158565.62500\n",
            "Train Loss: 673.89737 | Train Acc: 158656.25000\n",
            "Train Loss: 674.27903 | Train Acc: 158740.62500\n",
            "Train Loss: 674.73644 | Train Acc: 158818.75000\n",
            "Train Loss: 675.17924 | Train Acc: 158903.12500\n",
            "Train Loss: 675.74848 | Train Acc: 158981.25000\n",
            "Train Loss: 676.15657 | Train Acc: 159062.50000\n",
            "Train Loss: 676.63774 | Train Acc: 159143.75000\n",
            "Train Loss: 677.09614 | Train Acc: 159221.87500\n",
            "Train Loss: 677.44582 | Train Acc: 159312.50000\n",
            "Train Loss: 677.62250 | Train Acc: 159406.25000\n",
            "Train Loss: 677.82345 | Train Acc: 159500.00000\n",
            "Train Loss: 678.22907 | Train Acc: 159587.50000\n",
            "Train Loss: 678.47748 | Train Acc: 159681.25000\n",
            "Train Loss: 678.96142 | Train Acc: 159762.50000\n",
            "Train Loss: 679.52700 | Train Acc: 159846.87500\n",
            "Train Loss: 679.84835 | Train Acc: 159940.62500\n",
            "Train Loss: 680.49335 | Train Acc: 160018.75000\n",
            "Train Loss: 680.87286 | Train Acc: 160106.25000\n",
            "Train Loss: 681.15074 | Train Acc: 160193.75000\n",
            "Train Loss: 681.78225 | Train Acc: 160268.75000\n",
            "Train Loss: 682.05264 | Train Acc: 160362.50000\n",
            "Train Loss: 682.48392 | Train Acc: 160443.75000\n",
            "Train Loss: 682.79363 | Train Acc: 160534.37500\n",
            "Train Loss: 683.53368 | Train Acc: 160609.37500\n",
            "Train Loss: 683.89517 | Train Acc: 160690.62500\n",
            "Train Loss: 684.28398 | Train Acc: 160771.87500\n",
            "Train Loss: 684.97106 | Train Acc: 160846.87500\n",
            "Train Loss: 685.34608 | Train Acc: 160931.25000\n",
            "Train Loss: 685.71622 | Train Acc: 161018.75000\n",
            "Train Loss: 686.20534 | Train Acc: 161096.87500\n",
            "Train Loss: 686.40963 | Train Acc: 161187.50000\n",
            "Train Loss: 686.84922 | Train Acc: 161271.87500\n",
            "Train Loss: 687.36989 | Train Acc: 161356.25000\n",
            "Train Loss: 687.89363 | Train Acc: 161437.50000\n",
            "Train Loss: 688.44979 | Train Acc: 161521.87500\n",
            "Train Loss: 688.84475 | Train Acc: 161600.00000\n",
            "Train Loss: 689.22962 | Train Acc: 161684.37500\n",
            "Train Loss: 689.98253 | Train Acc: 161759.37500\n",
            "Train Loss: 690.45846 | Train Acc: 161834.37500\n",
            "Train Loss: 690.90267 | Train Acc: 161915.62500\n",
            "Train Loss: 691.36838 | Train Acc: 161996.87500\n",
            "Train Loss: 691.77972 | Train Acc: 162084.37500\n",
            "Train Loss: 692.06563 | Train Acc: 162168.75000\n",
            "Train Loss: 692.51911 | Train Acc: 162250.00000\n",
            "Train Loss: 693.04930 | Train Acc: 162337.50000\n",
            "Train Loss: 693.66647 | Train Acc: 162415.62500\n",
            "Train Loss: 694.14498 | Train Acc: 162506.25000\n",
            "Train Loss: 694.59679 | Train Acc: 162590.62500\n",
            "Train Loss: 695.21688 | Train Acc: 162671.87500\n",
            "Train Loss: 695.78544 | Train Acc: 162753.12500\n",
            "Train Loss: 696.21003 | Train Acc: 162834.37500\n",
            "Train Loss: 696.59419 | Train Acc: 162909.37500\n",
            "Train Loss: 696.98763 | Train Acc: 163000.00000\n",
            "Test_loss:0.45946,Test_accuracy:84.37500\n",
            "Test_loss:0.79887,Test_accuracy:171.87500\n",
            "Test_loss:1.20345,Test_accuracy:259.37500\n",
            "Test_loss:1.45146,Test_accuracy:346.87500\n",
            "Test_loss:2.06633,Test_accuracy:428.12500\n",
            "Test_loss:2.18905,Test_accuracy:528.12500\n",
            "Test_loss:2.43403,Test_accuracy:615.62500\n",
            "Test_loss:2.94960,Test_accuracy:700.00000\n",
            "Test_loss:3.32747,Test_accuracy:784.37500\n",
            "Test_loss:3.72446,Test_accuracy:871.87500\n",
            "Test_loss:4.18629,Test_accuracy:959.37500\n",
            "Test_loss:4.62939,Test_accuracy:1043.75000\n",
            "Test_loss:5.03786,Test_accuracy:1128.12500\n",
            "Test_loss:5.28342,Test_accuracy:1221.87500\n",
            "Test_loss:5.90945,Test_accuracy:1300.00000\n",
            "Test_loss:6.24171,Test_accuracy:1390.62500\n",
            "Test_loss:6.62198,Test_accuracy:1471.87500\n",
            "Test_loss:7.12851,Test_accuracy:1550.00000\n",
            "Test_loss:7.60699,Test_accuracy:1631.25000\n",
            "Test_loss:8.17833,Test_accuracy:1718.75000\n",
            "Test_loss:8.66386,Test_accuracy:1800.00000\n",
            "Test_loss:9.20669,Test_accuracy:1881.25000\n",
            "Test_loss:9.92387,Test_accuracy:1965.62500\n",
            "Test_loss:10.29459,Test_accuracy:2053.12500\n",
            "Test_loss:10.58514,Test_accuracy:2146.87500\n",
            "Test_loss:10.96738,Test_accuracy:2234.37500\n",
            "Test_loss:11.21106,Test_accuracy:2328.12500\n",
            "Test_loss:11.42597,Test_accuracy:2418.75000\n",
            "Test_loss:11.87009,Test_accuracy:2503.12500\n",
            "Test_loss:12.20923,Test_accuracy:2590.62500\n",
            "Test_loss:13.00355,Test_accuracy:2662.50000\n",
            "Test_loss:13.48848,Test_accuracy:2746.87500\n",
            "Test_loss:13.80536,Test_accuracy:2834.37500\n",
            "Test_loss:14.21837,Test_accuracy:2912.50000\n",
            "Test_loss:14.79541,Test_accuracy:2996.87500\n",
            "Test_loss:15.14494,Test_accuracy:3078.12500\n",
            "Test_loss:15.61899,Test_accuracy:3162.50000\n",
            "Test_loss:15.99016,Test_accuracy:3250.00000\n",
            "Test_loss:16.50259,Test_accuracy:3334.37500\n",
            "Test_loss:16.75070,Test_accuracy:3431.25000\n",
            "Test_loss:17.13046,Test_accuracy:3521.87500\n",
            "Test_loss:17.67489,Test_accuracy:3612.50000\n",
            "Test_loss:17.88773,Test_accuracy:3706.25000\n",
            "Test_loss:18.31576,Test_accuracy:3793.75000\n",
            "Test_loss:18.47807,Test_accuracy:3890.62500\n",
            "Test_loss:19.14254,Test_accuracy:3971.87500\n",
            "Test_loss:19.69969,Test_accuracy:4050.00000\n",
            "Test_loss:20.13740,Test_accuracy:4128.12500\n",
            "Test_loss:20.33294,Test_accuracy:4215.62500\n",
            "Test_loss:20.59788,Test_accuracy:4303.12500\n",
            "Test_loss:20.82961,Test_accuracy:4393.75000\n",
            "Test_loss:21.52670,Test_accuracy:4468.75000\n",
            "Test_loss:21.71513,Test_accuracy:4562.50000\n",
            "Test_loss:22.04139,Test_accuracy:4650.00000\n",
            "Test_loss:22.68241,Test_accuracy:4734.37500\n",
            "Test_loss:22.96730,Test_accuracy:4828.12500\n",
            "Test_loss:23.39897,Test_accuracy:4909.37500\n",
            "Test_loss:23.90111,Test_accuracy:4993.75000\n",
            "Test_loss:24.06032,Test_accuracy:5087.50000\n",
            "Test_loss:24.34641,Test_accuracy:5181.25000\n",
            "Test_loss:24.68841,Test_accuracy:5265.62500\n",
            "Test_loss:25.46173,Test_accuracy:5337.50000\n",
            "Test_loss:25.87214,Test_accuracy:5425.00000\n",
            "Test_loss:26.58776,Test_accuracy:5509.37500\n",
            "Test_loss:27.20461,Test_accuracy:5581.25000\n",
            "Test_loss:27.42956,Test_accuracy:5675.00000\n",
            "Test_loss:27.55829,Test_accuracy:5775.00000\n",
            "Test_loss:27.86393,Test_accuracy:5862.50000\n",
            "Test_loss:28.32058,Test_accuracy:5946.87500\n",
            "Test_loss:28.45427,Test_accuracy:6046.87500\n",
            "Test_loss:28.62132,Test_accuracy:6140.62500\n",
            "Test_loss:28.95885,Test_accuracy:6231.25000\n",
            "Test_loss:29.34384,Test_accuracy:6315.62500\n",
            "Test_loss:29.67543,Test_accuracy:6400.00000\n",
            "Test_loss:30.20339,Test_accuracy:6484.37500\n",
            "Test_loss:30.66884,Test_accuracy:6562.50000\n",
            "Test_loss:30.92862,Test_accuracy:6653.12500\n",
            "Test_loss:31.84626,Test_accuracy:6731.25000\n",
            "Test_loss:32.35702,Test_accuracy:6812.50000\n",
            "Test_loss:32.81178,Test_accuracy:6900.00000\n",
            "Test_loss:33.12058,Test_accuracy:6984.37500\n",
            "Test_loss:33.59281,Test_accuracy:7075.00000\n",
            "Test_loss:34.00347,Test_accuracy:7165.62500\n",
            "Test_loss:34.50517,Test_accuracy:7250.00000\n",
            "Test_loss:34.91824,Test_accuracy:7331.25000\n",
            "Test_loss:35.37947,Test_accuracy:7412.50000\n",
            "Test_loss:35.78854,Test_accuracy:7487.50000\n",
            "Test_loss:35.99789,Test_accuracy:7578.12500\n",
            "Test_loss:36.58226,Test_accuracy:7656.25000\n",
            "Test_loss:37.01760,Test_accuracy:7743.75000\n",
            "Test_loss:37.91600,Test_accuracy:7809.37500\n",
            "Test_loss:38.56165,Test_accuracy:7884.37500\n",
            "Test_loss:39.10578,Test_accuracy:7959.37500\n",
            "Test_loss:39.70147,Test_accuracy:8034.37500\n",
            "Test_loss:40.18554,Test_accuracy:8118.75000\n",
            "Test_loss:40.65574,Test_accuracy:8203.12500\n",
            "Test_loss:41.20364,Test_accuracy:8275.00000\n",
            "Test_loss:41.80727,Test_accuracy:8359.37500\n",
            "Test_loss:42.00809,Test_accuracy:8456.25000\n",
            "Test_loss:42.54501,Test_accuracy:8537.50000\n",
            "Test_loss:42.99765,Test_accuracy:8615.62500\n",
            "Test_loss:44.05407,Test_accuracy:8690.62500\n",
            "Test_loss:44.48653,Test_accuracy:8775.00000\n",
            "Test_loss:44.97215,Test_accuracy:8856.25000\n",
            "Test_loss:45.40534,Test_accuracy:8946.87500\n",
            "Test_loss:45.59491,Test_accuracy:9040.62500\n",
            "Test_loss:45.87418,Test_accuracy:9131.25000\n",
            "Test_loss:46.26440,Test_accuracy:9218.75000\n",
            "Test_loss:47.22221,Test_accuracy:9290.62500\n",
            "Test_loss:47.67048,Test_accuracy:9375.00000\n",
            "Test_loss:48.30188,Test_accuracy:9453.12500\n",
            "Test_loss:48.63329,Test_accuracy:9543.75000\n",
            "Test_loss:48.97253,Test_accuracy:9637.50000\n",
            "Test_loss:49.27850,Test_accuracy:9725.00000\n",
            "Test_loss:49.76802,Test_accuracy:9812.50000\n",
            "Test_loss:50.19317,Test_accuracy:9896.87500\n",
            "Test_loss:50.55684,Test_accuracy:9984.37500\n",
            "Test_loss:51.05005,Test_accuracy:10068.75000\n",
            "Test_loss:51.55769,Test_accuracy:10150.00000\n",
            "Test_loss:51.92954,Test_accuracy:10237.50000\n",
            "Test_loss:52.46946,Test_accuracy:10325.00000\n",
            "Test_loss:52.99374,Test_accuracy:10406.25000\n",
            "Test_loss:53.24371,Test_accuracy:10490.62500\n",
            "Test_loss:53.80247,Test_accuracy:10568.75000\n",
            "Test_loss:54.28886,Test_accuracy:10646.87500\n",
            "Test_loss:54.59135,Test_accuracy:10734.37500\n",
            "Test_loss:55.05204,Test_accuracy:10818.75000\n",
            "Test_loss:55.86303,Test_accuracy:10896.87500\n",
            "Test_loss:56.28194,Test_accuracy:10981.25000\n",
            "Test_loss:57.30099,Test_accuracy:11043.75000\n",
            "Test_loss:57.60482,Test_accuracy:11131.25000\n",
            "Test_loss:57.93441,Test_accuracy:11218.75000\n",
            "Test_loss:58.44529,Test_accuracy:11300.00000\n",
            "Test_loss:58.88985,Test_accuracy:11381.25000\n",
            "Test_loss:59.18761,Test_accuracy:11471.87500\n",
            "Test_loss:59.42488,Test_accuracy:11565.62500\n",
            "Test_loss:59.62296,Test_accuracy:11656.25000\n",
            "Test_loss:60.10624,Test_accuracy:11737.50000\n",
            "Test_loss:60.44013,Test_accuracy:11825.00000\n",
            "Test_loss:60.67366,Test_accuracy:11918.75000\n",
            "Test_loss:61.21520,Test_accuracy:11996.87500\n",
            "Test_loss:61.44651,Test_accuracy:12084.37500\n",
            "Test_loss:61.67348,Test_accuracy:12175.00000\n",
            "Test_loss:61.88999,Test_accuracy:12265.62500\n",
            "Test_loss:62.07754,Test_accuracy:12356.25000\n",
            "Test_loss:62.74693,Test_accuracy:12437.50000\n",
            "Test_loss:63.42719,Test_accuracy:12518.75000\n",
            "Test_loss:63.81022,Test_accuracy:12609.37500\n",
            "Test_loss:64.22608,Test_accuracy:12687.50000\n",
            "Test_loss:64.47913,Test_accuracy:12781.25000\n",
            "Test_loss:65.21734,Test_accuracy:12862.50000\n",
            "Test_loss:65.76095,Test_accuracy:12934.37500\n",
            "Test_loss:66.24049,Test_accuracy:13015.62500\n",
            "Test_loss:66.89995,Test_accuracy:13093.75000\n",
            "Test_loss:67.26217,Test_accuracy:13181.25000\n",
            "Test_loss:67.44648,Test_accuracy:13275.00000\n",
            "Test_loss:68.10721,Test_accuracy:13350.00000\n",
            "Test_loss:68.76670,Test_accuracy:13428.12500\n",
            "Test_loss:69.43206,Test_accuracy:13500.00000\n",
            "Test_loss:69.77563,Test_accuracy:13590.62500\n",
            "Test_loss:70.28119,Test_accuracy:13678.12500\n",
            "Test_loss:70.94958,Test_accuracy:13753.12500\n",
            "Test_loss:71.64171,Test_accuracy:13834.37500\n",
            "Test_loss:71.97613,Test_accuracy:13925.00000\n",
            "Test_loss:72.75757,Test_accuracy:14003.12500\n",
            "Test_loss:73.01880,Test_accuracy:14087.50000\n",
            "Test_loss:73.72491,Test_accuracy:14171.87500\n",
            "Test_loss:73.86858,Test_accuracy:14271.87500\n",
            "Test_loss:74.33086,Test_accuracy:14353.12500\n",
            "Test_loss:74.73611,Test_accuracy:14440.62500\n",
            "Test_loss:75.54849,Test_accuracy:14515.62500\n",
            "Test_loss:76.09678,Test_accuracy:14596.87500\n",
            "Test_loss:77.21309,Test_accuracy:14665.62500\n",
            "Test_loss:77.73419,Test_accuracy:14746.87500\n",
            "Test_loss:78.74065,Test_accuracy:14821.87500\n",
            "Test_loss:79.10209,Test_accuracy:14912.50000\n",
            "Test_loss:79.45661,Test_accuracy:14993.75000\n",
            "Test_loss:79.81483,Test_accuracy:15078.12500\n",
            "Test_loss:80.37943,Test_accuracy:15159.37500\n",
            "Test_loss:80.75796,Test_accuracy:15240.62500\n",
            "Test_loss:81.19918,Test_accuracy:15331.25000\n",
            "Test_loss:81.77820,Test_accuracy:15415.62500\n",
            "Test_loss:82.12928,Test_accuracy:15503.12500\n",
            "Test_loss:82.39519,Test_accuracy:15596.87500\n",
            "Test_loss:82.74564,Test_accuracy:15684.37500\n",
            "Test_loss:83.01392,Test_accuracy:15775.00000\n",
            "Test_loss:83.35329,Test_accuracy:15859.37500\n",
            "Test_loss:83.91900,Test_accuracy:15943.75000\n",
            "Test_loss:84.47230,Test_accuracy:16025.00000\n",
            "Test_loss:84.79003,Test_accuracy:16115.62500\n",
            "Test_loss:85.26441,Test_accuracy:16200.00000\n",
            "Test_loss:85.59578,Test_accuracy:16287.50000\n",
            "Test_loss:86.13717,Test_accuracy:16362.50000\n",
            "Test_loss:86.38015,Test_accuracy:16456.25000\n",
            "Test_loss:86.59000,Test_accuracy:16550.00000\n",
            "Test_loss:87.06060,Test_accuracy:16631.25000\n",
            "Test_loss:87.46321,Test_accuracy:16721.87500\n",
            "Test_loss:88.01794,Test_accuracy:16806.25000\n",
            "Test_loss:88.45509,Test_accuracy:16890.62500\n",
            "Test_loss:88.72080,Test_accuracy:16978.12500\n",
            "Test_loss:89.27551,Test_accuracy:17059.37500\n",
            "Test_loss:89.82925,Test_accuracy:17137.50000\n",
            "Test_loss:90.45443,Test_accuracy:17215.62500\n",
            "Test_loss:90.86307,Test_accuracy:17296.87500\n",
            "Test_loss:91.42257,Test_accuracy:17375.00000\n",
            "Test_loss:92.21749,Test_accuracy:17453.12500\n",
            "Test_loss:92.69761,Test_accuracy:17540.62500\n",
            "Test_loss:93.29925,Test_accuracy:17612.50000\n",
            "Test_loss:93.77675,Test_accuracy:17696.87500\n",
            "Test_loss:94.48781,Test_accuracy:17775.00000\n",
            "Test_loss:94.77927,Test_accuracy:17859.37500\n",
            "Test_loss:95.00143,Test_accuracy:17953.12500\n",
            "Test_loss:95.36024,Test_accuracy:18037.50000\n",
            "Test_loss:95.87794,Test_accuracy:18125.00000\n",
            "Test_loss:96.37609,Test_accuracy:18206.25000\n",
            "Test_loss:96.62294,Test_accuracy:18296.87500\n",
            "Test_loss:97.05654,Test_accuracy:18381.25000\n",
            "Test_loss:97.31639,Test_accuracy:18475.00000\n",
            "Test_loss:97.82084,Test_accuracy:18553.12500\n",
            "Test_loss:98.13081,Test_accuracy:18637.50000\n",
            "Test_loss:98.52731,Test_accuracy:18718.75000\n",
            "Test_loss:98.90086,Test_accuracy:18806.25000\n",
            "Test_loss:99.49908,Test_accuracy:18887.50000\n",
            "Test_loss:99.76208,Test_accuracy:18971.87500\n",
            "Test_loss:100.41860,Test_accuracy:19050.00000\n",
            "Test_loss:100.92873,Test_accuracy:19137.50000\n",
            "Test_loss:101.43958,Test_accuracy:19225.00000\n",
            "Test_loss:101.73430,Test_accuracy:19312.50000\n",
            "Test_loss:102.04752,Test_accuracy:19403.12500\n",
            "Test_loss:102.62307,Test_accuracy:19478.12500\n",
            "Test_loss:102.94828,Test_accuracy:19568.75000\n",
            "Test_loss:103.18728,Test_accuracy:19662.50000\n",
            "Test_loss:103.63779,Test_accuracy:19743.75000\n",
            "Test_loss:103.71991,Test_accuracy:19843.75000\n",
            "Test_loss:104.05183,Test_accuracy:19934.37500\n",
            "Test_loss:104.44275,Test_accuracy:20018.75000\n",
            "Test_loss:104.74504,Test_accuracy:20106.25000\n",
            "Test_loss:105.16459,Test_accuracy:20190.62500\n",
            "Test_loss:105.60798,Test_accuracy:20278.12500\n",
            "Test_loss:105.91772,Test_accuracy:20359.37500\n",
            "Test_loss:106.20375,Test_accuracy:20443.75000\n",
            "Test_loss:106.53729,Test_accuracy:20528.12500\n",
            "Test_loss:106.70109,Test_accuracy:20625.00000\n",
            "Test_loss:107.07897,Test_accuracy:20706.25000\n",
            "Test_loss:107.58701,Test_accuracy:20790.62500\n",
            "Test_loss:107.95515,Test_accuracy:20871.87500\n",
            "Test_loss:108.52621,Test_accuracy:20953.12500\n",
            "Test_loss:108.84029,Test_accuracy:21043.75000\n",
            "Test_loss:109.42968,Test_accuracy:21125.00000\n",
            "Test_loss:109.97090,Test_accuracy:21209.37500\n",
            "Test_loss:110.34344,Test_accuracy:21293.75000\n",
            "Test_loss:110.81795,Test_accuracy:21371.87500\n",
            "Test_loss:111.34425,Test_accuracy:21456.25000\n",
            "Test_loss:111.64009,Test_accuracy:21543.75000\n",
            "Test_loss:112.67289,Test_accuracy:21618.75000\n",
            "Test_loss:112.95853,Test_accuracy:21706.25000\n",
            "Test_loss:113.38643,Test_accuracy:21793.75000\n",
            "Test_loss:113.67627,Test_accuracy:21887.50000\n",
            "Test_loss:114.27081,Test_accuracy:21965.62500\n",
            "Test_loss:114.75469,Test_accuracy:22050.00000\n",
            "Test_loss:115.06920,Test_accuracy:22140.62500\n",
            "Test_loss:115.38410,Test_accuracy:22228.12500\n",
            "Test_loss:115.64343,Test_accuracy:22321.87500\n",
            "Test_loss:115.95291,Test_accuracy:22409.37500\n",
            "Test_loss:116.44756,Test_accuracy:22487.50000\n",
            "Test_loss:116.64820,Test_accuracy:22581.25000\n",
            "Test_loss:117.16788,Test_accuracy:22659.37500\n",
            "Test_loss:117.58595,Test_accuracy:22743.75000\n",
            "Test_loss:118.20065,Test_accuracy:22828.12500\n",
            "Test_loss:118.79752,Test_accuracy:22909.37500\n",
            "Test_loss:119.29801,Test_accuracy:22996.87500\n",
            "Test_loss:119.46307,Test_accuracy:23093.75000\n",
            "Test_loss:120.10954,Test_accuracy:23165.62500\n",
            "Test_loss:120.99331,Test_accuracy:23234.37500\n",
            "Test_loss:121.48022,Test_accuracy:23312.50000\n",
            "Test_loss:121.84190,Test_accuracy:23393.75000\n",
            "Test_loss:122.17718,Test_accuracy:23478.12500\n",
            "Test_loss:122.65665,Test_accuracy:23562.50000\n",
            "Test_loss:123.19188,Test_accuracy:23650.00000\n",
            "Test_loss:123.75484,Test_accuracy:23731.25000\n",
            "Test_loss:124.02614,Test_accuracy:23821.87500\n",
            "Test_loss:124.47686,Test_accuracy:23906.25000\n",
            "Test_loss:124.81735,Test_accuracy:23993.75000\n",
            "Test_loss:125.41084,Test_accuracy:24065.62500\n",
            "Test_loss:125.76220,Test_accuracy:24153.12500\n",
            "Test_loss:126.40445,Test_accuracy:24231.25000\n",
            "Test_loss:126.91338,Test_accuracy:24309.37500\n",
            "Test_loss:127.39142,Test_accuracy:24390.62500\n",
            "Test_loss:127.75906,Test_accuracy:24475.00000\n",
            "Test_loss:128.21739,Test_accuracy:24562.50000\n",
            "Test_loss:128.67578,Test_accuracy:24643.75000\n",
            "Test_loss:128.94530,Test_accuracy:24737.50000\n",
            "Test_loss:129.62677,Test_accuracy:24809.37500\n",
            "Test_loss:129.77415,Test_accuracy:24903.12500\n",
            "Test_loss:129.91434,Test_accuracy:25000.00000\n",
            "Test_loss:130.27655,Test_accuracy:25084.37500\n",
            "Test_loss:131.08087,Test_accuracy:25153.12500\n",
            "Test_loss:131.54353,Test_accuracy:25240.62500\n",
            "Test_loss:132.00288,Test_accuracy:25325.00000\n",
            "Test_loss:132.68939,Test_accuracy:25406.25000\n",
            "Test_loss:133.04549,Test_accuracy:25490.62500\n",
            "Test_loss:133.46220,Test_accuracy:25571.87500\n",
            "Test_loss:133.98421,Test_accuracy:25653.12500\n",
            "Test_loss:134.21123,Test_accuracy:25743.75000\n",
            "Test_loss:134.45975,Test_accuracy:25837.50000\n",
            "Test_loss:134.63924,Test_accuracy:25934.37500\n",
            "Test_loss:134.93970,Test_accuracy:26021.87500\n",
            "Test_loss:135.13025,Test_accuracy:26112.50000\n",
            "Test_loss:135.73120,Test_accuracy:26190.62500\n",
            "Test_loss:136.06866,Test_accuracy:26271.87500\n",
            "Test_loss:136.60597,Test_accuracy:26356.25000\n",
            "Test_loss:137.42780,Test_accuracy:26428.12500\n",
            "Test_loss:137.82784,Test_accuracy:26515.62500\n",
            "Epoch: 8\n",
            "-------\n",
            "Train Loss: 0.28659 | Train Acc: 87.50000\n",
            "Train Loss: 0.54651 | Train Acc: 178.12500\n",
            "Train Loss: 0.90316 | Train Acc: 265.62500\n",
            "Train Loss: 1.26169 | Train Acc: 353.12500\n",
            "Train Loss: 1.56160 | Train Acc: 446.87500\n",
            "Train Loss: 1.75342 | Train Acc: 537.50000\n",
            "Train Loss: 2.03491 | Train Acc: 625.00000\n",
            "Train Loss: 2.26804 | Train Acc: 715.62500\n",
            "Train Loss: 2.73745 | Train Acc: 793.75000\n",
            "Train Loss: 3.07420 | Train Acc: 881.25000\n",
            "Train Loss: 3.50149 | Train Acc: 965.62500\n",
            "Train Loss: 3.93839 | Train Acc: 1046.87500\n",
            "Train Loss: 4.30122 | Train Acc: 1131.25000\n",
            "Train Loss: 4.74533 | Train Acc: 1218.75000\n",
            "Train Loss: 5.17503 | Train Acc: 1303.12500\n",
            "Train Loss: 5.68483 | Train Acc: 1384.37500\n",
            "Train Loss: 5.92860 | Train Acc: 1478.12500\n",
            "Train Loss: 6.35743 | Train Acc: 1568.75000\n",
            "Train Loss: 6.87679 | Train Acc: 1650.00000\n",
            "Train Loss: 7.16214 | Train Acc: 1740.62500\n",
            "Train Loss: 7.38879 | Train Acc: 1837.50000\n",
            "Train Loss: 7.85070 | Train Acc: 1928.12500\n",
            "Train Loss: 8.34944 | Train Acc: 2012.50000\n",
            "Train Loss: 8.59596 | Train Acc: 2103.12500\n",
            "Train Loss: 8.80598 | Train Acc: 2196.87500\n",
            "Train Loss: 9.22875 | Train Acc: 2287.50000\n",
            "Train Loss: 9.93146 | Train Acc: 2362.50000\n",
            "Train Loss: 10.20388 | Train Acc: 2453.12500\n",
            "Train Loss: 10.81172 | Train Acc: 2543.75000\n",
            "Train Loss: 11.37362 | Train Acc: 2625.00000\n",
            "Train Loss: 11.72169 | Train Acc: 2709.37500\n",
            "Train Loss: 11.96286 | Train Acc: 2796.87500\n",
            "Train Loss: 12.37867 | Train Acc: 2884.37500\n",
            "Train Loss: 12.70590 | Train Acc: 2971.87500\n",
            "Train Loss: 13.13424 | Train Acc: 3059.37500\n",
            "Train Loss: 13.36499 | Train Acc: 3150.00000\n",
            "Train Loss: 13.76686 | Train Acc: 3234.37500\n",
            "Train Loss: 14.05135 | Train Acc: 3318.75000\n",
            "Train Loss: 14.37624 | Train Acc: 3409.37500\n",
            "Train Loss: 14.72094 | Train Acc: 3503.12500\n",
            "Train Loss: 15.20463 | Train Acc: 3587.50000\n",
            "Train Loss: 15.48866 | Train Acc: 3678.12500\n",
            "Train Loss: 15.98883 | Train Acc: 3762.50000\n",
            "Train Loss: 16.23639 | Train Acc: 3850.00000\n",
            "Train Loss: 16.98004 | Train Acc: 3931.25000\n",
            "Train Loss: 17.22657 | Train Acc: 4021.87500\n",
            "Train Loss: 17.55218 | Train Acc: 4109.37500\n",
            "Train Loss: 17.66723 | Train Acc: 4206.25000\n",
            "Train Loss: 17.97185 | Train Acc: 4296.87500\n",
            "Train Loss: 18.39466 | Train Acc: 4378.12500\n",
            "Train Loss: 18.59573 | Train Acc: 4471.87500\n",
            "Train Loss: 19.09057 | Train Acc: 4550.00000\n",
            "Train Loss: 19.30734 | Train Acc: 4640.62500\n",
            "Train Loss: 19.72509 | Train Acc: 4725.00000\n",
            "Train Loss: 19.98557 | Train Acc: 4809.37500\n",
            "Train Loss: 20.20999 | Train Acc: 4903.12500\n",
            "Train Loss: 20.50698 | Train Acc: 4990.62500\n",
            "Train Loss: 20.66296 | Train Acc: 5087.50000\n",
            "Train Loss: 21.02806 | Train Acc: 5175.00000\n",
            "Train Loss: 21.80963 | Train Acc: 5240.62500\n",
            "Train Loss: 22.19228 | Train Acc: 5325.00000\n",
            "Train Loss: 22.78675 | Train Acc: 5409.37500\n",
            "Train Loss: 23.26454 | Train Acc: 5490.62500\n",
            "Train Loss: 23.41096 | Train Acc: 5584.37500\n",
            "Train Loss: 23.74175 | Train Acc: 5671.87500\n",
            "Train Loss: 24.07351 | Train Acc: 5756.25000\n",
            "Train Loss: 24.70743 | Train Acc: 5837.50000\n",
            "Train Loss: 25.08366 | Train Acc: 5928.12500\n",
            "Train Loss: 25.48097 | Train Acc: 6015.62500\n",
            "Train Loss: 26.00037 | Train Acc: 6096.87500\n",
            "Train Loss: 26.39144 | Train Acc: 6178.12500\n",
            "Train Loss: 26.81586 | Train Acc: 6265.62500\n",
            "Train Loss: 27.06810 | Train Acc: 6359.37500\n",
            "Train Loss: 27.57915 | Train Acc: 6440.62500\n",
            "Train Loss: 27.85425 | Train Acc: 6534.37500\n",
            "Train Loss: 28.26447 | Train Acc: 6621.87500\n",
            "Train Loss: 28.51355 | Train Acc: 6712.50000\n",
            "Train Loss: 28.93593 | Train Acc: 6796.87500\n",
            "Train Loss: 29.21722 | Train Acc: 6884.37500\n",
            "Train Loss: 29.49192 | Train Acc: 6971.87500\n",
            "Train Loss: 29.63460 | Train Acc: 7065.62500\n",
            "Train Loss: 29.87134 | Train Acc: 7159.37500\n",
            "Train Loss: 30.20367 | Train Acc: 7246.87500\n",
            "Train Loss: 30.73646 | Train Acc: 7334.37500\n",
            "Train Loss: 31.22312 | Train Acc: 7412.50000\n",
            "Train Loss: 31.67210 | Train Acc: 7496.87500\n",
            "Train Loss: 32.11147 | Train Acc: 7578.12500\n",
            "Train Loss: 32.42281 | Train Acc: 7668.75000\n",
            "Train Loss: 32.80287 | Train Acc: 7753.12500\n",
            "Train Loss: 33.05026 | Train Acc: 7840.62500\n",
            "Train Loss: 33.34294 | Train Acc: 7931.25000\n",
            "Train Loss: 33.58210 | Train Acc: 8025.00000\n",
            "Train Loss: 33.82725 | Train Acc: 8118.75000\n",
            "Train Loss: 34.52539 | Train Acc: 8200.00000\n",
            "Train Loss: 34.69476 | Train Acc: 8293.75000\n",
            "Train Loss: 35.05401 | Train Acc: 8384.37500\n",
            "Train Loss: 35.28854 | Train Acc: 8475.00000\n",
            "Train Loss: 36.03009 | Train Acc: 8556.25000\n",
            "Train Loss: 36.22057 | Train Acc: 8646.87500\n",
            "Train Loss: 36.60232 | Train Acc: 8734.37500\n",
            "Train Loss: 36.95977 | Train Acc: 8825.00000\n",
            "Train Loss: 37.19310 | Train Acc: 8918.75000\n",
            "Train Loss: 37.71177 | Train Acc: 8996.87500\n",
            "Train Loss: 37.93489 | Train Acc: 9084.37500\n",
            "Train Loss: 38.28114 | Train Acc: 9168.75000\n",
            "Train Loss: 38.55643 | Train Acc: 9256.25000\n",
            "Train Loss: 38.86072 | Train Acc: 9343.75000\n",
            "Train Loss: 39.16090 | Train Acc: 9431.25000\n",
            "Train Loss: 39.55076 | Train Acc: 9518.75000\n",
            "Train Loss: 39.91643 | Train Acc: 9603.12500\n",
            "Train Loss: 40.34102 | Train Acc: 9690.62500\n",
            "Train Loss: 40.95945 | Train Acc: 9765.62500\n",
            "Train Loss: 41.39945 | Train Acc: 9843.75000\n",
            "Train Loss: 41.74828 | Train Acc: 9931.25000\n",
            "Train Loss: 42.28258 | Train Acc: 10006.25000\n",
            "Train Loss: 42.46971 | Train Acc: 10096.87500\n",
            "Train Loss: 42.95108 | Train Acc: 10181.25000\n",
            "Train Loss: 43.31379 | Train Acc: 10268.75000\n",
            "Train Loss: 43.79977 | Train Acc: 10350.00000\n",
            "Train Loss: 44.16715 | Train Acc: 10437.50000\n",
            "Train Loss: 44.78639 | Train Acc: 10515.62500\n",
            "Train Loss: 45.29429 | Train Acc: 10593.75000\n",
            "Train Loss: 45.69447 | Train Acc: 10675.00000\n",
            "Train Loss: 46.00766 | Train Acc: 10759.37500\n",
            "Train Loss: 46.18380 | Train Acc: 10856.25000\n",
            "Train Loss: 46.56903 | Train Acc: 10940.62500\n",
            "Train Loss: 46.90840 | Train Acc: 11031.25000\n",
            "Train Loss: 48.06746 | Train Acc: 11115.62500\n",
            "Train Loss: 48.48040 | Train Acc: 11200.00000\n",
            "Train Loss: 49.00503 | Train Acc: 11278.12500\n",
            "Train Loss: 49.42184 | Train Acc: 11359.37500\n",
            "Train Loss: 50.02800 | Train Acc: 11437.50000\n",
            "Train Loss: 50.38304 | Train Acc: 11525.00000\n",
            "Train Loss: 50.61993 | Train Acc: 11621.87500\n",
            "Train Loss: 51.19861 | Train Acc: 11706.25000\n",
            "Train Loss: 51.59693 | Train Acc: 11784.37500\n",
            "Train Loss: 51.72814 | Train Acc: 11884.37500\n",
            "Train Loss: 52.13925 | Train Acc: 11968.75000\n",
            "Train Loss: 52.47768 | Train Acc: 12056.25000\n",
            "Train Loss: 52.65374 | Train Acc: 12150.00000\n",
            "Train Loss: 53.06554 | Train Acc: 12243.75000\n",
            "Train Loss: 53.46796 | Train Acc: 12328.12500\n",
            "Train Loss: 53.57823 | Train Acc: 12425.00000\n",
            "Train Loss: 53.76572 | Train Acc: 12515.62500\n",
            "Train Loss: 54.05068 | Train Acc: 12606.25000\n",
            "Train Loss: 54.31948 | Train Acc: 12700.00000\n",
            "Train Loss: 54.71745 | Train Acc: 12790.62500\n",
            "Train Loss: 55.16591 | Train Acc: 12884.37500\n",
            "Train Loss: 55.37859 | Train Acc: 12978.12500\n",
            "Train Loss: 55.71893 | Train Acc: 13065.62500\n",
            "Train Loss: 56.24817 | Train Acc: 13146.87500\n",
            "Train Loss: 56.55165 | Train Acc: 13231.25000\n",
            "Train Loss: 56.98874 | Train Acc: 13325.00000\n",
            "Train Loss: 57.19482 | Train Acc: 13415.62500\n",
            "Train Loss: 57.61363 | Train Acc: 13500.00000\n",
            "Train Loss: 57.85536 | Train Acc: 13590.62500\n",
            "Train Loss: 58.03594 | Train Acc: 13687.50000\n",
            "Train Loss: 58.65536 | Train Acc: 13771.87500\n",
            "Train Loss: 58.92947 | Train Acc: 13862.50000\n",
            "Train Loss: 59.79708 | Train Acc: 13934.37500\n",
            "Train Loss: 60.10378 | Train Acc: 14025.00000\n",
            "Train Loss: 60.48707 | Train Acc: 14112.50000\n",
            "Train Loss: 60.87345 | Train Acc: 14196.87500\n",
            "Train Loss: 61.04772 | Train Acc: 14290.62500\n",
            "Train Loss: 61.29437 | Train Acc: 14381.25000\n",
            "Train Loss: 61.68244 | Train Acc: 14462.50000\n",
            "Train Loss: 61.83906 | Train Acc: 14562.50000\n",
            "Train Loss: 62.38123 | Train Acc: 14646.87500\n",
            "Train Loss: 63.02230 | Train Acc: 14728.12500\n",
            "Train Loss: 63.36621 | Train Acc: 14812.50000\n",
            "Train Loss: 63.69190 | Train Acc: 14900.00000\n",
            "Train Loss: 63.87051 | Train Acc: 14996.87500\n",
            "Train Loss: 64.51730 | Train Acc: 15078.12500\n",
            "Train Loss: 65.16227 | Train Acc: 15146.87500\n",
            "Train Loss: 65.51902 | Train Acc: 15228.12500\n",
            "Train Loss: 65.76279 | Train Acc: 15318.75000\n",
            "Train Loss: 66.02054 | Train Acc: 15406.25000\n",
            "Train Loss: 66.34424 | Train Acc: 15490.62500\n",
            "Train Loss: 66.75058 | Train Acc: 15575.00000\n",
            "Train Loss: 67.07613 | Train Acc: 15665.62500\n",
            "Train Loss: 67.53184 | Train Acc: 15750.00000\n",
            "Train Loss: 68.09145 | Train Acc: 15837.50000\n",
            "Train Loss: 68.41485 | Train Acc: 15921.87500\n",
            "Train Loss: 68.78005 | Train Acc: 16006.25000\n",
            "Train Loss: 69.12395 | Train Acc: 16093.75000\n",
            "Train Loss: 69.38646 | Train Acc: 16187.50000\n",
            "Train Loss: 69.48954 | Train Acc: 16284.37500\n",
            "Train Loss: 69.59187 | Train Acc: 16384.37500\n",
            "Train Loss: 69.80813 | Train Acc: 16475.00000\n",
            "Train Loss: 70.07314 | Train Acc: 16571.87500\n",
            "Train Loss: 70.54370 | Train Acc: 16650.00000\n",
            "Train Loss: 71.11023 | Train Acc: 16728.12500\n",
            "Train Loss: 71.39652 | Train Acc: 16815.62500\n",
            "Train Loss: 71.59316 | Train Acc: 16903.12500\n",
            "Train Loss: 72.02696 | Train Acc: 16984.37500\n",
            "Train Loss: 72.28483 | Train Acc: 17075.00000\n",
            "Train Loss: 72.70817 | Train Acc: 17159.37500\n",
            "Train Loss: 73.15518 | Train Acc: 17237.50000\n",
            "Train Loss: 74.04159 | Train Acc: 17318.75000\n",
            "Train Loss: 74.29141 | Train Acc: 17409.37500\n",
            "Train Loss: 74.82471 | Train Acc: 17490.62500\n",
            "Train Loss: 75.26418 | Train Acc: 17578.12500\n",
            "Train Loss: 75.83147 | Train Acc: 17650.00000\n",
            "Train Loss: 75.97940 | Train Acc: 17746.87500\n",
            "Train Loss: 76.62395 | Train Acc: 17834.37500\n",
            "Train Loss: 76.82439 | Train Acc: 17931.25000\n",
            "Train Loss: 77.11127 | Train Acc: 18025.00000\n",
            "Train Loss: 77.44543 | Train Acc: 18109.37500\n",
            "Train Loss: 77.86311 | Train Acc: 18200.00000\n",
            "Train Loss: 78.63170 | Train Acc: 18281.25000\n",
            "Train Loss: 79.03315 | Train Acc: 18362.50000\n",
            "Train Loss: 79.62220 | Train Acc: 18440.62500\n",
            "Train Loss: 80.02275 | Train Acc: 18528.12500\n",
            "Train Loss: 80.77237 | Train Acc: 18612.50000\n",
            "Train Loss: 81.42047 | Train Acc: 18690.62500\n",
            "Train Loss: 81.88958 | Train Acc: 18771.87500\n",
            "Train Loss: 82.04676 | Train Acc: 18868.75000\n",
            "Train Loss: 82.40845 | Train Acc: 18956.25000\n",
            "Train Loss: 82.98311 | Train Acc: 19043.75000\n",
            "Train Loss: 83.32914 | Train Acc: 19128.12500\n",
            "Train Loss: 83.70471 | Train Acc: 19215.62500\n",
            "Train Loss: 83.89872 | Train Acc: 19306.25000\n",
            "Train Loss: 84.20716 | Train Acc: 19396.87500\n",
            "Train Loss: 84.60027 | Train Acc: 19487.50000\n",
            "Train Loss: 84.83530 | Train Acc: 19575.00000\n",
            "Train Loss: 85.06863 | Train Acc: 19668.75000\n",
            "Train Loss: 85.42579 | Train Acc: 19756.25000\n",
            "Train Loss: 85.81367 | Train Acc: 19840.62500\n",
            "Train Loss: 86.07023 | Train Acc: 19934.37500\n",
            "Train Loss: 86.53713 | Train Acc: 20015.62500\n",
            "Train Loss: 86.93942 | Train Acc: 20106.25000\n",
            "Train Loss: 87.03530 | Train Acc: 20203.12500\n",
            "Train Loss: 87.28599 | Train Acc: 20296.87500\n",
            "Train Loss: 87.74629 | Train Acc: 20375.00000\n",
            "Train Loss: 88.02922 | Train Acc: 20465.62500\n",
            "Train Loss: 88.83487 | Train Acc: 20546.87500\n",
            "Train Loss: 89.20989 | Train Acc: 20631.25000\n",
            "Train Loss: 89.44021 | Train Acc: 20725.00000\n",
            "Train Loss: 89.62343 | Train Acc: 20815.62500\n",
            "Train Loss: 90.26305 | Train Acc: 20900.00000\n",
            "Train Loss: 90.81383 | Train Acc: 20981.25000\n",
            "Train Loss: 91.36346 | Train Acc: 21059.37500\n",
            "Train Loss: 91.53627 | Train Acc: 21150.00000\n",
            "Train Loss: 92.13685 | Train Acc: 21234.37500\n",
            "Train Loss: 92.53948 | Train Acc: 21321.87500\n",
            "Train Loss: 92.87939 | Train Acc: 21409.37500\n",
            "Train Loss: 93.10882 | Train Acc: 21500.00000\n",
            "Train Loss: 93.35392 | Train Acc: 21593.75000\n",
            "Train Loss: 93.62357 | Train Acc: 21687.50000\n",
            "Train Loss: 93.96765 | Train Acc: 21778.12500\n",
            "Train Loss: 94.27198 | Train Acc: 21868.75000\n",
            "Train Loss: 94.60617 | Train Acc: 21953.12500\n",
            "Train Loss: 95.16947 | Train Acc: 22034.37500\n",
            "Train Loss: 95.32470 | Train Acc: 22125.00000\n",
            "Train Loss: 96.08306 | Train Acc: 22206.25000\n",
            "Train Loss: 96.41235 | Train Acc: 22293.75000\n",
            "Train Loss: 96.64149 | Train Acc: 22378.12500\n",
            "Train Loss: 96.81515 | Train Acc: 22471.87500\n",
            "Train Loss: 97.16715 | Train Acc: 22556.25000\n",
            "Train Loss: 97.53461 | Train Acc: 22643.75000\n",
            "Train Loss: 97.69921 | Train Acc: 22737.50000\n",
            "Train Loss: 97.89299 | Train Acc: 22828.12500\n",
            "Train Loss: 98.21433 | Train Acc: 22921.87500\n",
            "Train Loss: 98.75009 | Train Acc: 23003.12500\n",
            "Train Loss: 99.04828 | Train Acc: 23096.87500\n",
            "Train Loss: 99.49863 | Train Acc: 23178.12500\n",
            "Train Loss: 99.82975 | Train Acc: 23265.62500\n",
            "Train Loss: 100.31840 | Train Acc: 23346.87500\n",
            "Train Loss: 100.42746 | Train Acc: 23443.75000\n",
            "Train Loss: 100.89669 | Train Acc: 23525.00000\n",
            "Train Loss: 101.25588 | Train Acc: 23612.50000\n",
            "Train Loss: 101.58141 | Train Acc: 23696.87500\n",
            "Train Loss: 102.01258 | Train Acc: 23784.37500\n",
            "Train Loss: 102.30977 | Train Acc: 23868.75000\n",
            "Train Loss: 102.87415 | Train Acc: 23959.37500\n",
            "Train Loss: 103.03981 | Train Acc: 24056.25000\n",
            "Train Loss: 103.34216 | Train Acc: 24150.00000\n",
            "Train Loss: 103.69658 | Train Acc: 24237.50000\n",
            "Train Loss: 104.14043 | Train Acc: 24315.62500\n",
            "Train Loss: 104.66692 | Train Acc: 24393.75000\n",
            "Train Loss: 105.23344 | Train Acc: 24481.25000\n",
            "Train Loss: 105.77302 | Train Acc: 24562.50000\n",
            "Train Loss: 106.09636 | Train Acc: 24653.12500\n",
            "Train Loss: 106.53297 | Train Acc: 24737.50000\n",
            "Train Loss: 106.86762 | Train Acc: 24821.87500\n",
            "Train Loss: 107.31372 | Train Acc: 24906.25000\n",
            "Train Loss: 107.60133 | Train Acc: 25000.00000\n",
            "Train Loss: 107.94915 | Train Acc: 25090.62500\n",
            "Train Loss: 108.33910 | Train Acc: 25178.12500\n",
            "Train Loss: 108.61009 | Train Acc: 25268.75000\n",
            "Train Loss: 109.11653 | Train Acc: 25350.00000\n",
            "Train Loss: 109.65765 | Train Acc: 25425.00000\n",
            "Train Loss: 110.16004 | Train Acc: 25512.50000\n",
            "Train Loss: 110.48492 | Train Acc: 25593.75000\n",
            "Train Loss: 110.63122 | Train Acc: 25690.62500\n",
            "Train Loss: 111.21947 | Train Acc: 25765.62500\n",
            "Train Loss: 111.45258 | Train Acc: 25859.37500\n",
            "Train Loss: 111.79622 | Train Acc: 25943.75000\n",
            "Train Loss: 112.11936 | Train Acc: 26028.12500\n",
            "Train Loss: 112.52260 | Train Acc: 26109.37500\n",
            "Train Loss: 112.80086 | Train Acc: 26200.00000\n",
            "Train Loss: 113.43021 | Train Acc: 26271.87500\n",
            "Train Loss: 113.61879 | Train Acc: 26365.62500\n",
            "Train Loss: 114.00243 | Train Acc: 26453.12500\n",
            "Train Loss: 114.70783 | Train Acc: 26531.25000\n",
            "Train Loss: 115.07071 | Train Acc: 26615.62500\n",
            "Train Loss: 115.46257 | Train Acc: 26703.12500\n",
            "Train Loss: 115.62288 | Train Acc: 26796.87500\n",
            "Train Loss: 115.86650 | Train Acc: 26890.62500\n",
            "Train Loss: 116.41268 | Train Acc: 26975.00000\n",
            "Train Loss: 116.58158 | Train Acc: 27065.62500\n",
            "Train Loss: 116.77451 | Train Acc: 27159.37500\n",
            "Train Loss: 116.95258 | Train Acc: 27256.25000\n",
            "Train Loss: 117.19088 | Train Acc: 27343.75000\n",
            "Train Loss: 117.68563 | Train Acc: 27421.87500\n",
            "Train Loss: 118.11753 | Train Acc: 27503.12500\n",
            "Train Loss: 118.44671 | Train Acc: 27593.75000\n",
            "Train Loss: 118.94388 | Train Acc: 27671.87500\n",
            "Train Loss: 119.35698 | Train Acc: 27750.00000\n",
            "Train Loss: 119.78947 | Train Acc: 27843.75000\n",
            "Train Loss: 120.21759 | Train Acc: 27925.00000\n",
            "Train Loss: 120.67428 | Train Acc: 28006.25000\n",
            "Train Loss: 121.61851 | Train Acc: 28081.25000\n",
            "Train Loss: 121.99418 | Train Acc: 28171.87500\n",
            "Train Loss: 122.52559 | Train Acc: 28243.75000\n",
            "Train Loss: 122.94394 | Train Acc: 28337.50000\n",
            "Train Loss: 123.29710 | Train Acc: 28421.87500\n",
            "Train Loss: 123.67270 | Train Acc: 28506.25000\n",
            "Train Loss: 123.86248 | Train Acc: 28603.12500\n",
            "Train Loss: 124.16298 | Train Acc: 28690.62500\n",
            "Train Loss: 124.66993 | Train Acc: 28765.62500\n",
            "Train Loss: 124.93706 | Train Acc: 28850.00000\n",
            "Train Loss: 125.23816 | Train Acc: 28937.50000\n",
            "Train Loss: 125.59566 | Train Acc: 29021.87500\n",
            "Train Loss: 125.88404 | Train Acc: 29109.37500\n",
            "Train Loss: 126.26110 | Train Acc: 29193.75000\n",
            "Train Loss: 126.44227 | Train Acc: 29287.50000\n",
            "Train Loss: 126.87807 | Train Acc: 29368.75000\n",
            "Train Loss: 127.18988 | Train Acc: 29456.25000\n",
            "Train Loss: 127.63239 | Train Acc: 29543.75000\n",
            "Train Loss: 127.84171 | Train Acc: 29634.37500\n",
            "Train Loss: 128.13951 | Train Acc: 29721.87500\n",
            "Train Loss: 128.47406 | Train Acc: 29815.62500\n",
            "Train Loss: 128.79812 | Train Acc: 29906.25000\n",
            "Train Loss: 129.06189 | Train Acc: 29993.75000\n",
            "Train Loss: 129.24420 | Train Acc: 30090.62500\n",
            "Train Loss: 129.73728 | Train Acc: 30178.12500\n",
            "Train Loss: 130.37405 | Train Acc: 30256.25000\n",
            "Train Loss: 130.70243 | Train Acc: 30350.00000\n",
            "Train Loss: 131.06656 | Train Acc: 30431.25000\n",
            "Train Loss: 131.29464 | Train Acc: 30518.75000\n",
            "Train Loss: 131.70549 | Train Acc: 30603.12500\n",
            "Train Loss: 132.63345 | Train Acc: 30671.87500\n",
            "Train Loss: 132.99710 | Train Acc: 30762.50000\n",
            "Train Loss: 133.27968 | Train Acc: 30859.37500\n",
            "Train Loss: 133.92484 | Train Acc: 30937.50000\n",
            "Train Loss: 134.02460 | Train Acc: 31037.50000\n",
            "Train Loss: 134.44248 | Train Acc: 31118.75000\n",
            "Train Loss: 134.71580 | Train Acc: 31209.37500\n",
            "Train Loss: 135.05005 | Train Acc: 31296.87500\n",
            "Train Loss: 135.44898 | Train Acc: 31381.25000\n",
            "Train Loss: 135.80402 | Train Acc: 31468.75000\n",
            "Train Loss: 136.36854 | Train Acc: 31540.62500\n",
            "Train Loss: 136.79415 | Train Acc: 31634.37500\n",
            "Train Loss: 136.95814 | Train Acc: 31728.12500\n",
            "Train Loss: 137.35553 | Train Acc: 31812.50000\n",
            "Train Loss: 137.65785 | Train Acc: 31900.00000\n",
            "Train Loss: 138.06189 | Train Acc: 31987.50000\n",
            "Train Loss: 138.26665 | Train Acc: 32078.12500\n",
            "Train Loss: 138.55121 | Train Acc: 32168.75000\n",
            "Train Loss: 138.90678 | Train Acc: 32256.25000\n",
            "Train Loss: 139.28115 | Train Acc: 32343.75000\n",
            "Train Loss: 139.59972 | Train Acc: 32437.50000\n",
            "Train Loss: 140.10550 | Train Acc: 32525.00000\n",
            "Train Loss: 140.46008 | Train Acc: 32609.37500\n",
            "Train Loss: 140.76487 | Train Acc: 32700.00000\n",
            "Train Loss: 141.05132 | Train Acc: 32781.25000\n",
            "Train Loss: 141.19771 | Train Acc: 32878.12500\n",
            "Train Loss: 141.52504 | Train Acc: 32968.75000\n",
            "Train Loss: 141.70798 | Train Acc: 33059.37500\n",
            "Train Loss: 141.90566 | Train Acc: 33153.12500\n",
            "Train Loss: 142.46680 | Train Acc: 33234.37500\n",
            "Train Loss: 142.61574 | Train Acc: 33328.12500\n",
            "Train Loss: 142.82112 | Train Acc: 33425.00000\n",
            "Train Loss: 142.94729 | Train Acc: 33521.87500\n",
            "Train Loss: 143.26776 | Train Acc: 33612.50000\n",
            "Train Loss: 143.60798 | Train Acc: 33693.75000\n",
            "Train Loss: 143.86346 | Train Acc: 33784.37500\n",
            "Train Loss: 144.19552 | Train Acc: 33871.87500\n",
            "Train Loss: 144.62286 | Train Acc: 33956.25000\n",
            "Train Loss: 144.85370 | Train Acc: 34053.12500\n",
            "Train Loss: 145.16004 | Train Acc: 34143.75000\n",
            "Train Loss: 145.59937 | Train Acc: 34231.25000\n",
            "Train Loss: 146.09486 | Train Acc: 34312.50000\n",
            "Train Loss: 146.41893 | Train Acc: 34403.12500\n",
            "Train Loss: 147.19264 | Train Acc: 34481.25000\n",
            "Train Loss: 147.83958 | Train Acc: 34565.62500\n",
            "Train Loss: 148.24644 | Train Acc: 34643.75000\n",
            "Train Loss: 148.54035 | Train Acc: 34728.12500\n",
            "Train Loss: 149.16275 | Train Acc: 34806.25000\n",
            "Train Loss: 149.46466 | Train Acc: 34893.75000\n",
            "Train Loss: 149.80592 | Train Acc: 34981.25000\n",
            "Train Loss: 150.44980 | Train Acc: 35065.62500\n",
            "Train Loss: 151.11459 | Train Acc: 35137.50000\n",
            "Train Loss: 151.47360 | Train Acc: 35221.87500\n",
            "Train Loss: 151.98080 | Train Acc: 35303.12500\n",
            "Train Loss: 152.33974 | Train Acc: 35381.25000\n",
            "Train Loss: 152.56850 | Train Acc: 35468.75000\n",
            "Train Loss: 153.01131 | Train Acc: 35553.12500\n",
            "Train Loss: 153.46180 | Train Acc: 35637.50000\n",
            "Train Loss: 153.75623 | Train Acc: 35725.00000\n",
            "Train Loss: 154.04305 | Train Acc: 35815.62500\n",
            "Train Loss: 154.43959 | Train Acc: 35900.00000\n",
            "Train Loss: 154.72335 | Train Acc: 35990.62500\n",
            "Train Loss: 155.30172 | Train Acc: 36078.12500\n",
            "Train Loss: 156.02853 | Train Acc: 36153.12500\n",
            "Train Loss: 156.42487 | Train Acc: 36237.50000\n",
            "Train Loss: 156.85222 | Train Acc: 36328.12500\n",
            "Train Loss: 157.21296 | Train Acc: 36418.75000\n",
            "Train Loss: 157.34864 | Train Acc: 36515.62500\n",
            "Train Loss: 157.51398 | Train Acc: 36609.37500\n",
            "Train Loss: 157.76377 | Train Acc: 36703.12500\n",
            "Train Loss: 158.01788 | Train Acc: 36793.75000\n",
            "Train Loss: 158.24615 | Train Acc: 36887.50000\n",
            "Train Loss: 158.66889 | Train Acc: 36971.87500\n",
            "Train Loss: 159.16981 | Train Acc: 37059.37500\n",
            "Train Loss: 159.55176 | Train Acc: 37146.87500\n",
            "Train Loss: 159.89521 | Train Acc: 37231.25000\n",
            "Train Loss: 160.54777 | Train Acc: 37306.25000\n",
            "Train Loss: 160.86079 | Train Acc: 37390.62500\n",
            "Train Loss: 161.41919 | Train Acc: 37478.12500\n",
            "Train Loss: 161.90358 | Train Acc: 37565.62500\n",
            "Train Loss: 162.30809 | Train Acc: 37650.00000\n",
            "Train Loss: 162.54043 | Train Acc: 37737.50000\n",
            "Train Loss: 162.90407 | Train Acc: 37828.12500\n",
            "Train Loss: 163.30916 | Train Acc: 37915.62500\n",
            "Train Loss: 163.91398 | Train Acc: 37996.87500\n",
            "Train Loss: 164.10774 | Train Acc: 38090.62500\n",
            "Train Loss: 164.50061 | Train Acc: 38168.75000\n",
            "Train Loss: 164.93172 | Train Acc: 38259.37500\n",
            "Train Loss: 165.17818 | Train Acc: 38353.12500\n",
            "Train Loss: 165.48477 | Train Acc: 38443.75000\n",
            "Train Loss: 165.73060 | Train Acc: 38528.12500\n",
            "Train Loss: 166.22164 | Train Acc: 38609.37500\n",
            "Train Loss: 166.74444 | Train Acc: 38696.87500\n",
            "Train Loss: 166.99871 | Train Acc: 38790.62500\n",
            "Train Loss: 167.51337 | Train Acc: 38865.62500\n",
            "Train Loss: 168.09831 | Train Acc: 38943.75000\n",
            "Train Loss: 168.39502 | Train Acc: 39031.25000\n",
            "Train Loss: 169.07536 | Train Acc: 39106.25000\n",
            "Train Loss: 169.33273 | Train Acc: 39196.87500\n",
            "Train Loss: 169.79064 | Train Acc: 39275.00000\n",
            "Train Loss: 170.37229 | Train Acc: 39346.87500\n",
            "Train Loss: 170.93933 | Train Acc: 39428.12500\n",
            "Train Loss: 171.23736 | Train Acc: 39521.87500\n",
            "Train Loss: 171.53743 | Train Acc: 39612.50000\n",
            "Train Loss: 171.93017 | Train Acc: 39700.00000\n",
            "Train Loss: 172.34345 | Train Acc: 39790.62500\n",
            "Train Loss: 172.78449 | Train Acc: 39871.87500\n",
            "Train Loss: 173.04854 | Train Acc: 39962.50000\n",
            "Train Loss: 173.38443 | Train Acc: 40046.87500\n",
            "Train Loss: 173.78258 | Train Acc: 40134.37500\n",
            "Train Loss: 174.20232 | Train Acc: 40218.75000\n",
            "Train Loss: 174.43235 | Train Acc: 40309.37500\n",
            "Train Loss: 174.84846 | Train Acc: 40390.62500\n",
            "Train Loss: 175.06792 | Train Acc: 40481.25000\n",
            "Train Loss: 175.42278 | Train Acc: 40565.62500\n",
            "Train Loss: 175.67877 | Train Acc: 40653.12500\n",
            "Train Loss: 176.03767 | Train Acc: 40737.50000\n",
            "Train Loss: 176.73356 | Train Acc: 40815.62500\n",
            "Train Loss: 177.27021 | Train Acc: 40893.75000\n",
            "Train Loss: 177.65405 | Train Acc: 40984.37500\n",
            "Train Loss: 177.85336 | Train Acc: 41078.12500\n",
            "Train Loss: 178.08125 | Train Acc: 41165.62500\n",
            "Train Loss: 178.43389 | Train Acc: 41250.00000\n",
            "Train Loss: 178.81798 | Train Acc: 41334.37500\n",
            "Train Loss: 179.15201 | Train Acc: 41421.87500\n",
            "Train Loss: 179.65278 | Train Acc: 41503.12500\n",
            "Train Loss: 179.81163 | Train Acc: 41600.00000\n",
            "Train Loss: 180.21846 | Train Acc: 41687.50000\n",
            "Train Loss: 180.49661 | Train Acc: 41781.25000\n",
            "Train Loss: 180.94611 | Train Acc: 41859.37500\n",
            "Train Loss: 181.50592 | Train Acc: 41937.50000\n",
            "Train Loss: 181.61258 | Train Acc: 42037.50000\n",
            "Train Loss: 181.97221 | Train Acc: 42125.00000\n",
            "Train Loss: 182.47539 | Train Acc: 42206.25000\n",
            "Train Loss: 182.82411 | Train Acc: 42293.75000\n",
            "Train Loss: 183.28171 | Train Acc: 42387.50000\n",
            "Train Loss: 183.68061 | Train Acc: 42475.00000\n",
            "Train Loss: 184.01628 | Train Acc: 42559.37500\n",
            "Train Loss: 184.48077 | Train Acc: 42634.37500\n",
            "Train Loss: 184.61084 | Train Acc: 42731.25000\n",
            "Train Loss: 185.06560 | Train Acc: 42815.62500\n",
            "Train Loss: 185.65388 | Train Acc: 42906.25000\n",
            "Train Loss: 186.15056 | Train Acc: 42987.50000\n",
            "Train Loss: 186.34180 | Train Acc: 43081.25000\n",
            "Train Loss: 186.47372 | Train Acc: 43178.12500\n",
            "Train Loss: 186.77727 | Train Acc: 43262.50000\n",
            "Train Loss: 187.03858 | Train Acc: 43356.25000\n",
            "Train Loss: 187.34450 | Train Acc: 43443.75000\n",
            "Train Loss: 187.82549 | Train Acc: 43525.00000\n",
            "Train Loss: 188.31232 | Train Acc: 43612.50000\n",
            "Train Loss: 188.73250 | Train Acc: 43693.75000\n",
            "Train Loss: 189.01497 | Train Acc: 43787.50000\n",
            "Train Loss: 189.33066 | Train Acc: 43875.00000\n",
            "Train Loss: 189.91693 | Train Acc: 43962.50000\n",
            "Train Loss: 190.20174 | Train Acc: 44050.00000\n",
            "Train Loss: 190.78483 | Train Acc: 44128.12500\n",
            "Train Loss: 191.01069 | Train Acc: 44221.87500\n",
            "Train Loss: 191.28076 | Train Acc: 44312.50000\n",
            "Train Loss: 191.66390 | Train Acc: 44400.00000\n",
            "Train Loss: 192.15703 | Train Acc: 44478.12500\n",
            "Train Loss: 192.55270 | Train Acc: 44565.62500\n",
            "Train Loss: 192.65610 | Train Acc: 44665.62500\n",
            "Train Loss: 193.02402 | Train Acc: 44746.87500\n",
            "Train Loss: 193.42528 | Train Acc: 44828.12500\n",
            "Train Loss: 193.79807 | Train Acc: 44918.75000\n",
            "Train Loss: 194.25112 | Train Acc: 45003.12500\n",
            "Train Loss: 194.60000 | Train Acc: 45087.50000\n",
            "Train Loss: 194.81464 | Train Acc: 45181.25000\n",
            "Train Loss: 195.22050 | Train Acc: 45262.50000\n",
            "Train Loss: 195.53932 | Train Acc: 45350.00000\n",
            "Train Loss: 195.83654 | Train Acc: 45440.62500\n",
            "Train Loss: 196.21257 | Train Acc: 45525.00000\n",
            "Train Loss: 196.70771 | Train Acc: 45606.25000\n",
            "Train Loss: 197.12955 | Train Acc: 45690.62500\n",
            "Train Loss: 197.42635 | Train Acc: 45775.00000\n",
            "Train Loss: 197.78747 | Train Acc: 45862.50000\n",
            "Train Loss: 198.21078 | Train Acc: 45950.00000\n",
            "Train Loss: 198.56468 | Train Acc: 46037.50000\n",
            "Train Loss: 199.13847 | Train Acc: 46112.50000\n",
            "Train Loss: 199.40028 | Train Acc: 46203.12500\n",
            "Train Loss: 199.76748 | Train Acc: 46284.37500\n",
            "Train Loss: 200.11195 | Train Acc: 46368.75000\n",
            "Train Loss: 200.34326 | Train Acc: 46459.37500\n",
            "Train Loss: 200.60658 | Train Acc: 46550.00000\n",
            "Train Loss: 200.94368 | Train Acc: 46640.62500\n",
            "Train Loss: 201.12294 | Train Acc: 46734.37500\n",
            "Train Loss: 201.49756 | Train Acc: 46825.00000\n",
            "Train Loss: 201.71720 | Train Acc: 46921.87500\n",
            "Train Loss: 202.02817 | Train Acc: 47015.62500\n",
            "Train Loss: 202.53333 | Train Acc: 47103.12500\n",
            "Train Loss: 202.94262 | Train Acc: 47184.37500\n",
            "Train Loss: 203.41831 | Train Acc: 47275.00000\n",
            "Train Loss: 203.66536 | Train Acc: 47368.75000\n",
            "Train Loss: 204.14477 | Train Acc: 47446.87500\n",
            "Train Loss: 204.81921 | Train Acc: 47521.87500\n",
            "Train Loss: 205.10245 | Train Acc: 47615.62500\n",
            "Train Loss: 205.35320 | Train Acc: 47709.37500\n",
            "Train Loss: 205.84709 | Train Acc: 47790.62500\n",
            "Train Loss: 206.13988 | Train Acc: 47884.37500\n",
            "Train Loss: 206.51629 | Train Acc: 47971.87500\n",
            "Train Loss: 206.97910 | Train Acc: 48050.00000\n",
            "Train Loss: 207.24513 | Train Acc: 48140.62500\n",
            "Train Loss: 207.57480 | Train Acc: 48228.12500\n",
            "Train Loss: 207.82428 | Train Acc: 48318.75000\n",
            "Train Loss: 208.08750 | Train Acc: 48415.62500\n",
            "Train Loss: 208.38737 | Train Acc: 48503.12500\n",
            "Train Loss: 208.63675 | Train Acc: 48593.75000\n",
            "Train Loss: 209.31453 | Train Acc: 48665.62500\n",
            "Train Loss: 209.81059 | Train Acc: 48746.87500\n",
            "Train Loss: 210.16201 | Train Acc: 48831.25000\n",
            "Train Loss: 210.56476 | Train Acc: 48915.62500\n",
            "Train Loss: 210.95729 | Train Acc: 49006.25000\n",
            "Train Loss: 211.26082 | Train Acc: 49093.75000\n",
            "Train Loss: 212.08507 | Train Acc: 49159.37500\n",
            "Train Loss: 212.27871 | Train Acc: 49253.12500\n",
            "Train Loss: 212.61961 | Train Acc: 49337.50000\n",
            "Train Loss: 213.31650 | Train Acc: 49412.50000\n",
            "Train Loss: 213.85037 | Train Acc: 49493.75000\n",
            "Train Loss: 214.32590 | Train Acc: 49571.87500\n",
            "Train Loss: 214.65241 | Train Acc: 49659.37500\n",
            "Train Loss: 214.96518 | Train Acc: 49750.00000\n",
            "Train Loss: 215.33990 | Train Acc: 49837.50000\n",
            "Train Loss: 215.54911 | Train Acc: 49931.25000\n",
            "Train Loss: 215.68941 | Train Acc: 50028.12500\n",
            "Train Loss: 216.08104 | Train Acc: 50112.50000\n",
            "Train Loss: 216.50885 | Train Acc: 50193.75000\n",
            "Train Loss: 216.66676 | Train Acc: 50287.50000\n",
            "Train Loss: 216.97723 | Train Acc: 50381.25000\n",
            "Train Loss: 217.25625 | Train Acc: 50475.00000\n",
            "Train Loss: 217.67190 | Train Acc: 50559.37500\n",
            "Train Loss: 218.15164 | Train Acc: 50634.37500\n",
            "Train Loss: 218.41470 | Train Acc: 50721.87500\n",
            "Train Loss: 218.73098 | Train Acc: 50809.37500\n",
            "Train Loss: 218.97141 | Train Acc: 50906.25000\n",
            "Train Loss: 219.54826 | Train Acc: 50993.75000\n",
            "Train Loss: 220.28082 | Train Acc: 51071.87500\n",
            "Train Loss: 220.59593 | Train Acc: 51159.37500\n",
            "Train Loss: 220.91218 | Train Acc: 51246.87500\n",
            "Train Loss: 221.08340 | Train Acc: 51343.75000\n",
            "Train Loss: 221.46197 | Train Acc: 51434.37500\n",
            "Train Loss: 221.75277 | Train Acc: 51528.12500\n",
            "Train Loss: 222.05459 | Train Acc: 51618.75000\n",
            "Train Loss: 222.18402 | Train Acc: 51712.50000\n",
            "Train Loss: 222.51798 | Train Acc: 51800.00000\n",
            "Train Loss: 222.85760 | Train Acc: 51881.25000\n",
            "Train Loss: 223.23963 | Train Acc: 51968.75000\n",
            "Train Loss: 223.91098 | Train Acc: 52043.75000\n",
            "Train Loss: 224.33158 | Train Acc: 52131.25000\n",
            "Train Loss: 224.63611 | Train Acc: 52218.75000\n",
            "Train Loss: 225.02532 | Train Acc: 52306.25000\n",
            "Train Loss: 225.35979 | Train Acc: 52396.87500\n",
            "Train Loss: 225.76047 | Train Acc: 52487.50000\n",
            "Train Loss: 226.00055 | Train Acc: 52575.00000\n",
            "Train Loss: 226.19218 | Train Acc: 52671.87500\n",
            "Train Loss: 226.45123 | Train Acc: 52762.50000\n",
            "Train Loss: 226.78355 | Train Acc: 52850.00000\n",
            "Train Loss: 226.94410 | Train Acc: 52943.75000\n",
            "Train Loss: 227.38695 | Train Acc: 53028.12500\n",
            "Train Loss: 227.70537 | Train Acc: 53115.62500\n",
            "Train Loss: 228.11082 | Train Acc: 53203.12500\n",
            "Train Loss: 228.72087 | Train Acc: 53284.37500\n",
            "Train Loss: 229.47593 | Train Acc: 53362.50000\n",
            "Train Loss: 229.86799 | Train Acc: 53440.62500\n",
            "Train Loss: 230.24064 | Train Acc: 53528.12500\n",
            "Train Loss: 230.55180 | Train Acc: 53615.62500\n",
            "Train Loss: 230.81307 | Train Acc: 53706.25000\n",
            "Train Loss: 231.10670 | Train Acc: 53793.75000\n",
            "Train Loss: 231.44024 | Train Acc: 53884.37500\n",
            "Train Loss: 231.74526 | Train Acc: 53981.25000\n",
            "Train Loss: 232.11657 | Train Acc: 54065.62500\n",
            "Train Loss: 232.25454 | Train Acc: 54159.37500\n",
            "Train Loss: 232.83532 | Train Acc: 54234.37500\n",
            "Train Loss: 233.00903 | Train Acc: 54325.00000\n",
            "Train Loss: 233.45192 | Train Acc: 54409.37500\n",
            "Train Loss: 233.74857 | Train Acc: 54500.00000\n",
            "Train Loss: 234.06817 | Train Acc: 54578.12500\n",
            "Train Loss: 234.33124 | Train Acc: 54671.87500\n",
            "Train Loss: 234.63506 | Train Acc: 54756.25000\n",
            "Train Loss: 234.88234 | Train Acc: 54846.87500\n",
            "Train Loss: 235.53624 | Train Acc: 54921.87500\n",
            "Train Loss: 235.92902 | Train Acc: 55003.12500\n",
            "Train Loss: 236.48503 | Train Acc: 55087.50000\n",
            "Train Loss: 236.86374 | Train Acc: 55178.12500\n",
            "Train Loss: 237.26942 | Train Acc: 55262.50000\n",
            "Train Loss: 237.67119 | Train Acc: 55340.62500\n",
            "Train Loss: 238.06091 | Train Acc: 55425.00000\n",
            "Train Loss: 238.43779 | Train Acc: 55512.50000\n",
            "Train Loss: 238.94592 | Train Acc: 55600.00000\n",
            "Train Loss: 239.13465 | Train Acc: 55696.87500\n",
            "Train Loss: 239.42693 | Train Acc: 55781.25000\n",
            "Train Loss: 239.83040 | Train Acc: 55868.75000\n",
            "Train Loss: 240.16358 | Train Acc: 55956.25000\n",
            "Train Loss: 240.44029 | Train Acc: 56046.87500\n",
            "Train Loss: 240.90890 | Train Acc: 56128.12500\n",
            "Train Loss: 241.15684 | Train Acc: 56215.62500\n",
            "Train Loss: 241.67427 | Train Acc: 56290.62500\n",
            "Train Loss: 241.93709 | Train Acc: 56384.37500\n",
            "Train Loss: 242.15906 | Train Acc: 56478.12500\n",
            "Train Loss: 242.52031 | Train Acc: 56565.62500\n",
            "Train Loss: 242.88433 | Train Acc: 56653.12500\n",
            "Train Loss: 243.17484 | Train Acc: 56746.87500\n",
            "Train Loss: 243.59449 | Train Acc: 56831.25000\n",
            "Train Loss: 243.79343 | Train Acc: 56925.00000\n",
            "Train Loss: 244.14879 | Train Acc: 57009.37500\n",
            "Train Loss: 244.74097 | Train Acc: 57081.25000\n",
            "Train Loss: 244.98745 | Train Acc: 57175.00000\n",
            "Train Loss: 245.28577 | Train Acc: 57259.37500\n",
            "Train Loss: 245.46780 | Train Acc: 57356.25000\n",
            "Train Loss: 245.80379 | Train Acc: 57443.75000\n",
            "Train Loss: 246.21140 | Train Acc: 57528.12500\n",
            "Train Loss: 246.52445 | Train Acc: 57618.75000\n",
            "Train Loss: 246.97469 | Train Acc: 57706.25000\n",
            "Train Loss: 247.27529 | Train Acc: 57793.75000\n",
            "Train Loss: 247.82594 | Train Acc: 57875.00000\n",
            "Train Loss: 248.22651 | Train Acc: 57962.50000\n",
            "Train Loss: 248.58330 | Train Acc: 58046.87500\n",
            "Train Loss: 248.97279 | Train Acc: 58137.50000\n",
            "Train Loss: 249.29860 | Train Acc: 58225.00000\n",
            "Train Loss: 249.84820 | Train Acc: 58315.62500\n",
            "Train Loss: 250.18329 | Train Acc: 58396.87500\n",
            "Train Loss: 250.63909 | Train Acc: 58481.25000\n",
            "Train Loss: 250.80370 | Train Acc: 58575.00000\n",
            "Train Loss: 251.15944 | Train Acc: 58668.75000\n",
            "Train Loss: 251.69235 | Train Acc: 58753.12500\n",
            "Train Loss: 251.97417 | Train Acc: 58840.62500\n",
            "Train Loss: 252.45847 | Train Acc: 58915.62500\n",
            "Train Loss: 252.90484 | Train Acc: 59003.12500\n",
            "Train Loss: 253.24570 | Train Acc: 59093.75000\n",
            "Train Loss: 253.58088 | Train Acc: 59184.37500\n",
            "Train Loss: 253.83435 | Train Acc: 59271.87500\n",
            "Train Loss: 254.10079 | Train Acc: 59362.50000\n",
            "Train Loss: 254.45991 | Train Acc: 59456.25000\n",
            "Train Loss: 254.88788 | Train Acc: 59540.62500\n",
            "Train Loss: 255.16257 | Train Acc: 59628.12500\n",
            "Train Loss: 255.62207 | Train Acc: 59725.00000\n",
            "Train Loss: 255.97146 | Train Acc: 59809.37500\n",
            "Train Loss: 256.28760 | Train Acc: 59893.75000\n",
            "Train Loss: 256.60888 | Train Acc: 59975.00000\n",
            "Train Loss: 257.02070 | Train Acc: 60062.50000\n",
            "Train Loss: 257.45273 | Train Acc: 60143.75000\n",
            "Train Loss: 257.72730 | Train Acc: 60231.25000\n",
            "Train Loss: 258.18021 | Train Acc: 60312.50000\n",
            "Train Loss: 258.57357 | Train Acc: 60400.00000\n",
            "Train Loss: 258.86693 | Train Acc: 60490.62500\n",
            "Train Loss: 259.27879 | Train Acc: 60578.12500\n",
            "Train Loss: 259.85054 | Train Acc: 60659.37500\n",
            "Train Loss: 260.01277 | Train Acc: 60756.25000\n",
            "Train Loss: 260.64547 | Train Acc: 60825.00000\n",
            "Train Loss: 260.97041 | Train Acc: 60915.62500\n",
            "Train Loss: 261.32741 | Train Acc: 60993.75000\n",
            "Train Loss: 261.48482 | Train Acc: 61090.62500\n",
            "Train Loss: 261.84478 | Train Acc: 61178.12500\n",
            "Train Loss: 262.45534 | Train Acc: 61256.25000\n",
            "Train Loss: 263.02408 | Train Acc: 61337.50000\n",
            "Train Loss: 263.38501 | Train Acc: 61425.00000\n",
            "Train Loss: 263.73535 | Train Acc: 61515.62500\n",
            "Train Loss: 264.02324 | Train Acc: 61606.25000\n",
            "Train Loss: 264.35266 | Train Acc: 61687.50000\n",
            "Train Loss: 265.17576 | Train Acc: 61756.25000\n",
            "Train Loss: 265.55573 | Train Acc: 61846.87500\n",
            "Train Loss: 265.99400 | Train Acc: 61925.00000\n",
            "Train Loss: 266.34502 | Train Acc: 62006.25000\n",
            "Train Loss: 266.89095 | Train Acc: 62087.50000\n",
            "Train Loss: 267.45159 | Train Acc: 62171.87500\n",
            "Train Loss: 267.82155 | Train Acc: 62262.50000\n",
            "Train Loss: 268.24357 | Train Acc: 62346.87500\n",
            "Train Loss: 268.50941 | Train Acc: 62440.62500\n",
            "Train Loss: 268.88420 | Train Acc: 62528.12500\n",
            "Train Loss: 269.26246 | Train Acc: 62615.62500\n",
            "Train Loss: 269.79916 | Train Acc: 62700.00000\n",
            "Train Loss: 270.83669 | Train Acc: 62768.75000\n",
            "Train Loss: 271.46307 | Train Acc: 62846.87500\n",
            "Train Loss: 271.86025 | Train Acc: 62931.25000\n",
            "Train Loss: 272.03927 | Train Acc: 63025.00000\n",
            "Train Loss: 272.40147 | Train Acc: 63112.50000\n",
            "Train Loss: 273.17747 | Train Acc: 63196.87500\n",
            "Train Loss: 273.60325 | Train Acc: 63284.37500\n",
            "Train Loss: 273.96598 | Train Acc: 63371.87500\n",
            "Train Loss: 274.38545 | Train Acc: 63456.25000\n",
            "Train Loss: 274.78466 | Train Acc: 63543.75000\n",
            "Train Loss: 275.09965 | Train Acc: 63625.00000\n",
            "Train Loss: 275.43057 | Train Acc: 63712.50000\n",
            "Train Loss: 275.83933 | Train Acc: 63790.62500\n",
            "Train Loss: 276.08172 | Train Acc: 63881.25000\n",
            "Train Loss: 276.59663 | Train Acc: 63959.37500\n",
            "Train Loss: 277.21084 | Train Acc: 64040.62500\n",
            "Train Loss: 277.42190 | Train Acc: 64131.25000\n",
            "Train Loss: 277.60848 | Train Acc: 64221.87500\n",
            "Train Loss: 277.76536 | Train Acc: 64321.87500\n",
            "Train Loss: 278.51636 | Train Acc: 64403.12500\n",
            "Train Loss: 278.86849 | Train Acc: 64490.62500\n",
            "Train Loss: 279.44958 | Train Acc: 64571.87500\n",
            "Train Loss: 279.72144 | Train Acc: 64662.50000\n",
            "Train Loss: 279.77527 | Train Acc: 64762.50000\n",
            "Train Loss: 280.14670 | Train Acc: 64850.00000\n",
            "Train Loss: 280.50506 | Train Acc: 64934.37500\n",
            "Train Loss: 280.92068 | Train Acc: 65018.75000\n",
            "Train Loss: 281.17423 | Train Acc: 65106.25000\n",
            "Train Loss: 281.77249 | Train Acc: 65187.50000\n",
            "Train Loss: 282.14776 | Train Acc: 65275.00000\n",
            "Train Loss: 282.27286 | Train Acc: 65375.00000\n",
            "Train Loss: 282.66407 | Train Acc: 65468.75000\n",
            "Train Loss: 283.08101 | Train Acc: 65550.00000\n",
            "Train Loss: 283.43268 | Train Acc: 65634.37500\n",
            "Train Loss: 283.75710 | Train Acc: 65725.00000\n",
            "Train Loss: 284.21543 | Train Acc: 65803.12500\n",
            "Train Loss: 284.52220 | Train Acc: 65890.62500\n",
            "Train Loss: 284.84392 | Train Acc: 65975.00000\n",
            "Train Loss: 285.53439 | Train Acc: 66046.87500\n",
            "Train Loss: 286.11650 | Train Acc: 66128.12500\n",
            "Train Loss: 286.40061 | Train Acc: 66221.87500\n",
            "Train Loss: 286.66616 | Train Acc: 66312.50000\n",
            "Train Loss: 287.15414 | Train Acc: 66403.12500\n",
            "Train Loss: 287.59332 | Train Acc: 66490.62500\n",
            "Train Loss: 288.42403 | Train Acc: 66571.87500\n",
            "Train Loss: 288.78328 | Train Acc: 66659.37500\n",
            "Train Loss: 289.02540 | Train Acc: 66750.00000\n",
            "Train Loss: 289.61137 | Train Acc: 66831.25000\n",
            "Train Loss: 290.04340 | Train Acc: 66921.87500\n",
            "Train Loss: 290.52475 | Train Acc: 67000.00000\n",
            "Train Loss: 291.20909 | Train Acc: 67078.12500\n",
            "Train Loss: 291.61166 | Train Acc: 67162.50000\n",
            "Train Loss: 291.77027 | Train Acc: 67259.37500\n",
            "Train Loss: 291.95929 | Train Acc: 67353.12500\n",
            "Train Loss: 292.05479 | Train Acc: 67453.12500\n",
            "Train Loss: 292.21078 | Train Acc: 67546.87500\n",
            "Train Loss: 292.61965 | Train Acc: 67631.25000\n",
            "Train Loss: 292.97977 | Train Acc: 67721.87500\n",
            "Train Loss: 293.42231 | Train Acc: 67809.37500\n",
            "Train Loss: 293.69413 | Train Acc: 67896.87500\n",
            "Train Loss: 294.00013 | Train Acc: 67981.25000\n",
            "Train Loss: 294.26037 | Train Acc: 68071.87500\n",
            "Train Loss: 294.64657 | Train Acc: 68159.37500\n",
            "Train Loss: 295.06045 | Train Acc: 68246.87500\n",
            "Train Loss: 295.64916 | Train Acc: 68328.12500\n",
            "Train Loss: 296.08276 | Train Acc: 68409.37500\n",
            "Train Loss: 296.70952 | Train Acc: 68484.37500\n",
            "Train Loss: 297.32521 | Train Acc: 68565.62500\n",
            "Train Loss: 297.67266 | Train Acc: 68656.25000\n",
            "Train Loss: 297.90839 | Train Acc: 68750.00000\n",
            "Train Loss: 298.23581 | Train Acc: 68834.37500\n",
            "Train Loss: 298.79489 | Train Acc: 68915.62500\n",
            "Train Loss: 299.08199 | Train Acc: 69006.25000\n",
            "Train Loss: 299.35101 | Train Acc: 69096.87500\n",
            "Train Loss: 299.67397 | Train Acc: 69187.50000\n",
            "Train Loss: 300.11642 | Train Acc: 69268.75000\n",
            "Train Loss: 300.30562 | Train Acc: 69365.62500\n",
            "Train Loss: 300.54324 | Train Acc: 69456.25000\n",
            "Train Loss: 300.86027 | Train Acc: 69546.87500\n",
            "Train Loss: 301.28710 | Train Acc: 69625.00000\n",
            "Train Loss: 301.57384 | Train Acc: 69718.75000\n",
            "Train Loss: 301.92592 | Train Acc: 69806.25000\n",
            "Train Loss: 302.23544 | Train Acc: 69890.62500\n",
            "Train Loss: 302.57235 | Train Acc: 69978.12500\n",
            "Train Loss: 302.86898 | Train Acc: 70065.62500\n",
            "Train Loss: 303.19160 | Train Acc: 70153.12500\n",
            "Train Loss: 303.80488 | Train Acc: 70231.25000\n",
            "Train Loss: 304.20747 | Train Acc: 70318.75000\n",
            "Train Loss: 304.35442 | Train Acc: 70412.50000\n",
            "Train Loss: 304.50794 | Train Acc: 70509.37500\n",
            "Train Loss: 305.00396 | Train Acc: 70587.50000\n",
            "Train Loss: 305.51486 | Train Acc: 70671.87500\n",
            "Train Loss: 306.03451 | Train Acc: 70753.12500\n",
            "Train Loss: 306.32867 | Train Acc: 70840.62500\n",
            "Train Loss: 306.75734 | Train Acc: 70928.12500\n",
            "Train Loss: 307.11078 | Train Acc: 71015.62500\n",
            "Train Loss: 307.31878 | Train Acc: 71112.50000\n",
            "Train Loss: 307.77107 | Train Acc: 71190.62500\n",
            "Train Loss: 308.12744 | Train Acc: 71275.00000\n",
            "Train Loss: 308.50222 | Train Acc: 71362.50000\n",
            "Train Loss: 309.01611 | Train Acc: 71440.62500\n",
            "Train Loss: 309.79260 | Train Acc: 71512.50000\n",
            "Train Loss: 310.17890 | Train Acc: 71596.87500\n",
            "Train Loss: 310.42693 | Train Acc: 71684.37500\n",
            "Train Loss: 310.54668 | Train Acc: 71784.37500\n",
            "Train Loss: 310.84638 | Train Acc: 71868.75000\n",
            "Train Loss: 311.14662 | Train Acc: 71959.37500\n",
            "Train Loss: 311.38165 | Train Acc: 72050.00000\n",
            "Train Loss: 311.67170 | Train Acc: 72140.62500\n",
            "Train Loss: 311.91208 | Train Acc: 72231.25000\n",
            "Train Loss: 312.38260 | Train Acc: 72315.62500\n",
            "Train Loss: 312.79291 | Train Acc: 72403.12500\n",
            "Train Loss: 313.26715 | Train Acc: 72484.37500\n",
            "Train Loss: 313.51343 | Train Acc: 72575.00000\n",
            "Train Loss: 314.22258 | Train Acc: 72646.87500\n",
            "Train Loss: 314.48731 | Train Acc: 72737.50000\n",
            "Train Loss: 314.98813 | Train Acc: 72812.50000\n",
            "Train Loss: 315.15437 | Train Acc: 72906.25000\n",
            "Train Loss: 315.60624 | Train Acc: 72984.37500\n",
            "Train Loss: 315.87497 | Train Acc: 73071.87500\n",
            "Train Loss: 316.23666 | Train Acc: 73156.25000\n",
            "Train Loss: 316.37403 | Train Acc: 73256.25000\n",
            "Train Loss: 316.79043 | Train Acc: 73343.75000\n",
            "Train Loss: 317.09977 | Train Acc: 73434.37500\n",
            "Train Loss: 317.38175 | Train Acc: 73525.00000\n",
            "Train Loss: 318.24019 | Train Acc: 73603.12500\n",
            "Train Loss: 318.54555 | Train Acc: 73693.75000\n",
            "Train Loss: 318.92388 | Train Acc: 73784.37500\n",
            "Train Loss: 319.32551 | Train Acc: 73865.62500\n",
            "Train Loss: 319.84031 | Train Acc: 73953.12500\n",
            "Train Loss: 320.13828 | Train Acc: 74043.75000\n",
            "Train Loss: 320.35929 | Train Acc: 74134.37500\n",
            "Train Loss: 320.64067 | Train Acc: 74225.00000\n",
            "Train Loss: 321.13909 | Train Acc: 74306.25000\n",
            "Train Loss: 321.44998 | Train Acc: 74393.75000\n",
            "Train Loss: 321.76122 | Train Acc: 74481.25000\n",
            "Train Loss: 322.19422 | Train Acc: 74568.75000\n",
            "Train Loss: 322.54252 | Train Acc: 74653.12500\n",
            "Train Loss: 322.87871 | Train Acc: 74743.75000\n",
            "Train Loss: 323.07736 | Train Acc: 74837.50000\n",
            "Train Loss: 323.33583 | Train Acc: 74921.87500\n",
            "Train Loss: 323.77321 | Train Acc: 75003.12500\n",
            "Train Loss: 324.00997 | Train Acc: 75093.75000\n",
            "Train Loss: 324.40664 | Train Acc: 75175.00000\n",
            "Train Loss: 324.66526 | Train Acc: 75262.50000\n",
            "Train Loss: 324.85605 | Train Acc: 75353.12500\n",
            "Train Loss: 325.46227 | Train Acc: 75428.12500\n",
            "Train Loss: 325.67009 | Train Acc: 75521.87500\n",
            "Train Loss: 325.87379 | Train Acc: 75615.62500\n",
            "Train Loss: 326.33493 | Train Acc: 75706.25000\n",
            "Train Loss: 326.95013 | Train Acc: 75781.25000\n",
            "Train Loss: 327.20014 | Train Acc: 75871.87500\n",
            "Train Loss: 327.55598 | Train Acc: 75956.25000\n",
            "Train Loss: 328.03132 | Train Acc: 76034.37500\n",
            "Train Loss: 328.19383 | Train Acc: 76131.25000\n",
            "Train Loss: 328.66220 | Train Acc: 76212.50000\n",
            "Train Loss: 328.96178 | Train Acc: 76300.00000\n",
            "Train Loss: 329.15852 | Train Acc: 76393.75000\n",
            "Train Loss: 329.33322 | Train Acc: 76484.37500\n",
            "Train Loss: 329.45320 | Train Acc: 76581.25000\n",
            "Train Loss: 329.78189 | Train Acc: 76668.75000\n",
            "Train Loss: 329.99562 | Train Acc: 76759.37500\n",
            "Train Loss: 330.49343 | Train Acc: 76840.62500\n",
            "Train Loss: 330.84430 | Train Acc: 76925.00000\n",
            "Train Loss: 331.59695 | Train Acc: 77000.00000\n",
            "Train Loss: 331.95178 | Train Acc: 77087.50000\n",
            "Train Loss: 332.30305 | Train Acc: 77168.75000\n",
            "Train Loss: 332.82580 | Train Acc: 77246.87500\n",
            "Train Loss: 333.09377 | Train Acc: 77340.62500\n",
            "Train Loss: 333.31946 | Train Acc: 77434.37500\n",
            "Train Loss: 333.89963 | Train Acc: 77512.50000\n",
            "Train Loss: 334.17064 | Train Acc: 77600.00000\n",
            "Train Loss: 334.63689 | Train Acc: 77681.25000\n",
            "Train Loss: 335.08426 | Train Acc: 77768.75000\n",
            "Train Loss: 335.52927 | Train Acc: 77853.12500\n",
            "Train Loss: 336.14295 | Train Acc: 77937.50000\n",
            "Train Loss: 336.45473 | Train Acc: 78028.12500\n",
            "Train Loss: 336.68260 | Train Acc: 78121.87500\n",
            "Train Loss: 336.89199 | Train Acc: 78218.75000\n",
            "Train Loss: 337.28158 | Train Acc: 78309.37500\n",
            "Train Loss: 337.46952 | Train Acc: 78400.00000\n",
            "Train Loss: 337.88106 | Train Acc: 78487.50000\n",
            "Train Loss: 338.10755 | Train Acc: 78584.37500\n",
            "Train Loss: 338.77010 | Train Acc: 78665.62500\n",
            "Train Loss: 339.05697 | Train Acc: 78756.25000\n",
            "Train Loss: 339.36351 | Train Acc: 78840.62500\n",
            "Train Loss: 339.74729 | Train Acc: 78928.12500\n",
            "Train Loss: 339.85874 | Train Acc: 79021.87500\n",
            "Train Loss: 340.11531 | Train Acc: 79112.50000\n",
            "Train Loss: 340.47494 | Train Acc: 79200.00000\n",
            "Train Loss: 340.89371 | Train Acc: 79284.37500\n",
            "Train Loss: 341.44540 | Train Acc: 79359.37500\n",
            "Train Loss: 341.60259 | Train Acc: 79450.00000\n",
            "Train Loss: 341.96577 | Train Acc: 79537.50000\n",
            "Train Loss: 342.27530 | Train Acc: 79618.75000\n",
            "Train Loss: 342.55906 | Train Acc: 79706.25000\n",
            "Train Loss: 343.21464 | Train Acc: 79781.25000\n",
            "Train Loss: 343.52721 | Train Acc: 79868.75000\n",
            "Train Loss: 343.93025 | Train Acc: 79953.12500\n",
            "Train Loss: 344.12953 | Train Acc: 80046.87500\n",
            "Train Loss: 344.30372 | Train Acc: 80140.62500\n",
            "Train Loss: 344.71817 | Train Acc: 80231.25000\n",
            "Train Loss: 345.10724 | Train Acc: 80318.75000\n",
            "Train Loss: 345.43131 | Train Acc: 80403.12500\n",
            "Train Loss: 345.87424 | Train Acc: 80484.37500\n",
            "Train Loss: 346.20009 | Train Acc: 80571.87500\n",
            "Train Loss: 346.57506 | Train Acc: 80656.25000\n",
            "Train Loss: 346.82165 | Train Acc: 80750.00000\n",
            "Train Loss: 347.30689 | Train Acc: 80837.50000\n",
            "Train Loss: 347.75507 | Train Acc: 80912.50000\n",
            "Train Loss: 348.07556 | Train Acc: 80996.87500\n",
            "Train Loss: 348.27705 | Train Acc: 81090.62500\n",
            "Train Loss: 348.49858 | Train Acc: 81184.37500\n",
            "Train Loss: 348.99455 | Train Acc: 81268.75000\n",
            "Train Loss: 349.26547 | Train Acc: 81359.37500\n",
            "Train Loss: 349.57748 | Train Acc: 81450.00000\n",
            "Train Loss: 350.03819 | Train Acc: 81531.25000\n",
            "Train Loss: 350.18520 | Train Acc: 81625.00000\n",
            "Train Loss: 350.56523 | Train Acc: 81712.50000\n",
            "Train Loss: 351.05536 | Train Acc: 81796.87500\n",
            "Train Loss: 351.23009 | Train Acc: 81890.62500\n",
            "Train Loss: 351.68161 | Train Acc: 81968.75000\n",
            "Train Loss: 352.18234 | Train Acc: 82056.25000\n",
            "Train Loss: 352.78491 | Train Acc: 82137.50000\n",
            "Train Loss: 353.26145 | Train Acc: 82221.87500\n",
            "Train Loss: 353.39778 | Train Acc: 82318.75000\n",
            "Train Loss: 353.58447 | Train Acc: 82409.37500\n",
            "Train Loss: 354.16420 | Train Acc: 82487.50000\n",
            "Train Loss: 354.50186 | Train Acc: 82578.12500\n",
            "Train Loss: 354.94105 | Train Acc: 82656.25000\n",
            "Train Loss: 355.53409 | Train Acc: 82737.50000\n",
            "Train Loss: 355.72118 | Train Acc: 82831.25000\n",
            "Train Loss: 356.07045 | Train Acc: 82918.75000\n",
            "Train Loss: 356.31306 | Train Acc: 83012.50000\n",
            "Train Loss: 356.57139 | Train Acc: 83100.00000\n",
            "Train Loss: 356.94080 | Train Acc: 83184.37500\n",
            "Train Loss: 357.24007 | Train Acc: 83265.62500\n",
            "Train Loss: 357.46830 | Train Acc: 83356.25000\n",
            "Train Loss: 357.86873 | Train Acc: 83443.75000\n",
            "Train Loss: 358.38070 | Train Acc: 83521.87500\n",
            "Train Loss: 358.61830 | Train Acc: 83609.37500\n",
            "Train Loss: 358.85777 | Train Acc: 83700.00000\n",
            "Train Loss: 359.11611 | Train Acc: 83793.75000\n",
            "Train Loss: 359.73854 | Train Acc: 83865.62500\n",
            "Train Loss: 360.15150 | Train Acc: 83950.00000\n",
            "Train Loss: 360.55918 | Train Acc: 84040.62500\n",
            "Train Loss: 361.10702 | Train Acc: 84118.75000\n",
            "Train Loss: 361.31055 | Train Acc: 84209.37500\n",
            "Train Loss: 361.61214 | Train Acc: 84293.75000\n",
            "Train Loss: 361.87436 | Train Acc: 84384.37500\n",
            "Train Loss: 361.96298 | Train Acc: 84481.25000\n",
            "Train Loss: 362.24491 | Train Acc: 84568.75000\n",
            "Train Loss: 362.66084 | Train Acc: 84653.12500\n",
            "Train Loss: 362.92468 | Train Acc: 84746.87500\n",
            "Train Loss: 363.24273 | Train Acc: 84840.62500\n",
            "Train Loss: 363.63032 | Train Acc: 84931.25000\n",
            "Train Loss: 364.02530 | Train Acc: 85025.00000\n",
            "Train Loss: 364.30919 | Train Acc: 85115.62500\n",
            "Train Loss: 364.75835 | Train Acc: 85206.25000\n",
            "Train Loss: 364.94971 | Train Acc: 85303.12500\n",
            "Train Loss: 365.34005 | Train Acc: 85384.37500\n",
            "Train Loss: 365.79764 | Train Acc: 85475.00000\n",
            "Train Loss: 366.11792 | Train Acc: 85568.75000\n",
            "Train Loss: 366.38145 | Train Acc: 85659.37500\n",
            "Train Loss: 366.68615 | Train Acc: 85750.00000\n",
            "Train Loss: 367.08991 | Train Acc: 85837.50000\n",
            "Train Loss: 367.48638 | Train Acc: 85921.87500\n",
            "Train Loss: 367.75054 | Train Acc: 86009.37500\n",
            "Train Loss: 368.22882 | Train Acc: 86100.00000\n",
            "Train Loss: 368.96590 | Train Acc: 86175.00000\n",
            "Train Loss: 369.34920 | Train Acc: 86259.37500\n",
            "Train Loss: 369.79986 | Train Acc: 86343.75000\n",
            "Train Loss: 370.16769 | Train Acc: 86428.12500\n",
            "Train Loss: 370.90227 | Train Acc: 86500.00000\n",
            "Train Loss: 371.30865 | Train Acc: 86587.50000\n",
            "Train Loss: 372.05823 | Train Acc: 86671.87500\n",
            "Train Loss: 372.95529 | Train Acc: 86753.12500\n",
            "Train Loss: 373.56762 | Train Acc: 86828.12500\n",
            "Train Loss: 374.07502 | Train Acc: 86909.37500\n",
            "Train Loss: 374.60717 | Train Acc: 86987.50000\n",
            "Train Loss: 374.98880 | Train Acc: 87068.75000\n",
            "Train Loss: 375.18536 | Train Acc: 87159.37500\n",
            "Train Loss: 375.56770 | Train Acc: 87246.87500\n",
            "Train Loss: 376.17855 | Train Acc: 87325.00000\n",
            "Train Loss: 376.50027 | Train Acc: 87415.62500\n",
            "Train Loss: 377.02112 | Train Acc: 87493.75000\n",
            "Train Loss: 377.42551 | Train Acc: 87571.87500\n",
            "Train Loss: 377.90490 | Train Acc: 87656.25000\n",
            "Train Loss: 378.08999 | Train Acc: 87746.87500\n",
            "Train Loss: 378.55241 | Train Acc: 87834.37500\n",
            "Train Loss: 378.99542 | Train Acc: 87915.62500\n",
            "Train Loss: 379.36715 | Train Acc: 87996.87500\n",
            "Train Loss: 379.71559 | Train Acc: 88081.25000\n",
            "Train Loss: 380.20850 | Train Acc: 88168.75000\n",
            "Train Loss: 380.43893 | Train Acc: 88259.37500\n",
            "Train Loss: 380.66375 | Train Acc: 88350.00000\n",
            "Train Loss: 381.35859 | Train Acc: 88431.25000\n",
            "Train Loss: 382.19149 | Train Acc: 88509.37500\n",
            "Train Loss: 382.58577 | Train Acc: 88596.87500\n",
            "Train Loss: 383.20544 | Train Acc: 88668.75000\n",
            "Train Loss: 383.58888 | Train Acc: 88759.37500\n",
            "Train Loss: 383.99863 | Train Acc: 88840.62500\n",
            "Train Loss: 384.21661 | Train Acc: 88931.25000\n",
            "Train Loss: 384.61426 | Train Acc: 89018.75000\n",
            "Train Loss: 384.80460 | Train Acc: 89112.50000\n",
            "Train Loss: 385.02291 | Train Acc: 89203.12500\n",
            "Train Loss: 385.29701 | Train Acc: 89293.75000\n",
            "Train Loss: 385.57786 | Train Acc: 89384.37500\n",
            "Train Loss: 386.24463 | Train Acc: 89465.62500\n",
            "Train Loss: 386.54092 | Train Acc: 89553.12500\n",
            "Train Loss: 386.86379 | Train Acc: 89646.87500\n",
            "Train Loss: 387.24502 | Train Acc: 89731.25000\n",
            "Train Loss: 387.71961 | Train Acc: 89806.25000\n",
            "Train Loss: 387.98273 | Train Acc: 89896.87500\n",
            "Train Loss: 388.18190 | Train Acc: 89990.62500\n",
            "Train Loss: 388.41488 | Train Acc: 90087.50000\n",
            "Train Loss: 388.82155 | Train Acc: 90178.12500\n",
            "Train Loss: 389.06728 | Train Acc: 90265.62500\n",
            "Train Loss: 389.41209 | Train Acc: 90356.25000\n",
            "Train Loss: 389.87949 | Train Acc: 90437.50000\n",
            "Train Loss: 390.41644 | Train Acc: 90509.37500\n",
            "Train Loss: 390.60465 | Train Acc: 90606.25000\n",
            "Train Loss: 390.88047 | Train Acc: 90693.75000\n",
            "Train Loss: 391.38681 | Train Acc: 90787.50000\n",
            "Train Loss: 391.74615 | Train Acc: 90875.00000\n",
            "Train Loss: 392.06626 | Train Acc: 90965.62500\n",
            "Train Loss: 392.47131 | Train Acc: 91056.25000\n",
            "Train Loss: 392.89989 | Train Acc: 91140.62500\n",
            "Train Loss: 393.44656 | Train Acc: 91221.87500\n",
            "Train Loss: 393.77820 | Train Acc: 91309.37500\n",
            "Train Loss: 394.06043 | Train Acc: 91400.00000\n",
            "Train Loss: 394.46279 | Train Acc: 91490.62500\n",
            "Train Loss: 394.84383 | Train Acc: 91571.87500\n",
            "Train Loss: 395.31185 | Train Acc: 91662.50000\n",
            "Train Loss: 395.59883 | Train Acc: 91746.87500\n",
            "Train Loss: 396.15768 | Train Acc: 91828.12500\n",
            "Train Loss: 396.45276 | Train Acc: 91918.75000\n",
            "Train Loss: 396.78540 | Train Acc: 92003.12500\n",
            "Train Loss: 397.11797 | Train Acc: 92084.37500\n",
            "Train Loss: 397.45754 | Train Acc: 92162.50000\n",
            "Train Loss: 398.06938 | Train Acc: 92237.50000\n",
            "Train Loss: 398.43981 | Train Acc: 92328.12500\n",
            "Train Loss: 398.70818 | Train Acc: 92415.62500\n",
            "Train Loss: 398.95740 | Train Acc: 92503.12500\n",
            "Train Loss: 399.54656 | Train Acc: 92578.12500\n",
            "Train Loss: 399.93883 | Train Acc: 92665.62500\n",
            "Train Loss: 400.25132 | Train Acc: 92753.12500\n",
            "Train Loss: 400.92791 | Train Acc: 92828.12500\n",
            "Train Loss: 401.56647 | Train Acc: 92912.50000\n",
            "Train Loss: 401.83168 | Train Acc: 93000.00000\n",
            "Train Loss: 402.22255 | Train Acc: 93081.25000\n",
            "Train Loss: 402.98152 | Train Acc: 93159.37500\n",
            "Train Loss: 403.41494 | Train Acc: 93234.37500\n",
            "Train Loss: 403.98399 | Train Acc: 93312.50000\n",
            "Train Loss: 404.26019 | Train Acc: 93403.12500\n",
            "Train Loss: 404.74212 | Train Acc: 93481.25000\n",
            "Train Loss: 405.20034 | Train Acc: 93565.62500\n",
            "Train Loss: 405.49077 | Train Acc: 93656.25000\n",
            "Train Loss: 405.83056 | Train Acc: 93746.87500\n",
            "Train Loss: 406.26384 | Train Acc: 93837.50000\n",
            "Train Loss: 406.86350 | Train Acc: 93921.87500\n",
            "Train Loss: 407.17241 | Train Acc: 94009.37500\n",
            "Train Loss: 407.43684 | Train Acc: 94103.12500\n",
            "Train Loss: 407.71821 | Train Acc: 94193.75000\n",
            "Train Loss: 408.13491 | Train Acc: 94278.12500\n",
            "Train Loss: 408.77065 | Train Acc: 94362.50000\n",
            "Train Loss: 409.08444 | Train Acc: 94453.12500\n",
            "Train Loss: 409.50995 | Train Acc: 94534.37500\n",
            "Train Loss: 409.69392 | Train Acc: 94628.12500\n",
            "Train Loss: 409.98630 | Train Acc: 94718.75000\n",
            "Train Loss: 410.34747 | Train Acc: 94803.12500\n",
            "Train Loss: 410.81868 | Train Acc: 94884.37500\n",
            "Train Loss: 411.21294 | Train Acc: 94965.62500\n",
            "Train Loss: 412.22176 | Train Acc: 95034.37500\n",
            "Train Loss: 412.35879 | Train Acc: 95131.25000\n",
            "Train Loss: 412.74339 | Train Acc: 95218.75000\n",
            "Train Loss: 413.17851 | Train Acc: 95309.37500\n",
            "Train Loss: 413.57137 | Train Acc: 95393.75000\n",
            "Train Loss: 413.78409 | Train Acc: 95484.37500\n",
            "Train Loss: 413.97337 | Train Acc: 95575.00000\n",
            "Train Loss: 414.32003 | Train Acc: 95665.62500\n",
            "Train Loss: 414.50633 | Train Acc: 95759.37500\n",
            "Train Loss: 414.80498 | Train Acc: 95856.25000\n",
            "Train Loss: 415.09626 | Train Acc: 95940.62500\n",
            "Train Loss: 415.57834 | Train Acc: 96018.75000\n",
            "Train Loss: 415.94478 | Train Acc: 96103.12500\n",
            "Train Loss: 416.11647 | Train Acc: 96196.87500\n",
            "Train Loss: 416.37578 | Train Acc: 96284.37500\n",
            "Train Loss: 416.89899 | Train Acc: 96362.50000\n",
            "Train Loss: 417.43324 | Train Acc: 96440.62500\n",
            "Train Loss: 417.64267 | Train Acc: 96534.37500\n",
            "Train Loss: 417.98528 | Train Acc: 96615.62500\n",
            "Train Loss: 418.43920 | Train Acc: 96696.87500\n",
            "Train Loss: 419.06247 | Train Acc: 96771.87500\n",
            "Train Loss: 419.26868 | Train Acc: 96865.62500\n",
            "Train Loss: 419.63654 | Train Acc: 96956.25000\n",
            "Train Loss: 419.92840 | Train Acc: 97046.87500\n",
            "Train Loss: 420.32817 | Train Acc: 97131.25000\n",
            "Train Loss: 421.02745 | Train Acc: 97215.62500\n",
            "Train Loss: 421.37264 | Train Acc: 97306.25000\n",
            "Train Loss: 421.64856 | Train Acc: 97396.87500\n",
            "Train Loss: 421.98425 | Train Acc: 97481.25000\n",
            "Train Loss: 422.09436 | Train Acc: 97575.00000\n",
            "Train Loss: 422.81347 | Train Acc: 97653.12500\n",
            "Train Loss: 423.23315 | Train Acc: 97740.62500\n",
            "Train Loss: 423.56961 | Train Acc: 97821.87500\n",
            "Train Loss: 423.81021 | Train Acc: 97918.75000\n",
            "Train Loss: 424.20029 | Train Acc: 98006.25000\n",
            "Train Loss: 424.42568 | Train Acc: 98100.00000\n",
            "Train Loss: 424.72869 | Train Acc: 98193.75000\n",
            "Train Loss: 424.99553 | Train Acc: 98281.25000\n",
            "Train Loss: 425.46991 | Train Acc: 98365.62500\n",
            "Train Loss: 425.69346 | Train Acc: 98459.37500\n",
            "Train Loss: 425.90825 | Train Acc: 98550.00000\n",
            "Train Loss: 426.23120 | Train Acc: 98640.62500\n",
            "Train Loss: 426.62470 | Train Acc: 98728.12500\n",
            "Train Loss: 426.92427 | Train Acc: 98812.50000\n",
            "Train Loss: 427.17898 | Train Acc: 98903.12500\n",
            "Train Loss: 427.54707 | Train Acc: 98996.87500\n",
            "Train Loss: 427.89689 | Train Acc: 99084.37500\n",
            "Train Loss: 428.10251 | Train Acc: 99175.00000\n",
            "Train Loss: 428.27568 | Train Acc: 99271.87500\n",
            "Train Loss: 428.85134 | Train Acc: 99356.25000\n",
            "Train Loss: 429.32604 | Train Acc: 99443.75000\n",
            "Train Loss: 429.64466 | Train Acc: 99537.50000\n",
            "Train Loss: 429.95184 | Train Acc: 99628.12500\n",
            "Train Loss: 430.36872 | Train Acc: 99706.25000\n",
            "Train Loss: 430.85536 | Train Acc: 99787.50000\n",
            "Train Loss: 431.05569 | Train Acc: 99884.37500\n",
            "Train Loss: 431.48725 | Train Acc: 99965.62500\n",
            "Train Loss: 432.01512 | Train Acc: 100050.00000\n",
            "Train Loss: 432.25104 | Train Acc: 100140.62500\n",
            "Train Loss: 432.85228 | Train Acc: 100225.00000\n",
            "Train Loss: 433.20507 | Train Acc: 100321.87500\n",
            "Train Loss: 433.65402 | Train Acc: 100409.37500\n",
            "Train Loss: 434.04714 | Train Acc: 100496.87500\n",
            "Train Loss: 434.39005 | Train Acc: 100584.37500\n",
            "Train Loss: 434.71513 | Train Acc: 100668.75000\n",
            "Train Loss: 435.22807 | Train Acc: 100746.87500\n",
            "Train Loss: 435.75703 | Train Acc: 100825.00000\n",
            "Train Loss: 435.91672 | Train Acc: 100921.87500\n",
            "Train Loss: 436.43881 | Train Acc: 101000.00000\n",
            "Train Loss: 436.88695 | Train Acc: 101078.12500\n",
            "Train Loss: 437.16654 | Train Acc: 101168.75000\n",
            "Train Loss: 437.62255 | Train Acc: 101256.25000\n",
            "Train Loss: 438.25440 | Train Acc: 101334.37500\n",
            "Train Loss: 438.69041 | Train Acc: 101415.62500\n",
            "Train Loss: 439.03006 | Train Acc: 101506.25000\n",
            "Train Loss: 439.39031 | Train Acc: 101596.87500\n",
            "Train Loss: 439.74799 | Train Acc: 101687.50000\n",
            "Train Loss: 440.09180 | Train Acc: 101781.25000\n",
            "Train Loss: 440.37783 | Train Acc: 101865.62500\n",
            "Train Loss: 440.69631 | Train Acc: 101956.25000\n",
            "Train Loss: 441.06048 | Train Acc: 102037.50000\n",
            "Train Loss: 441.42597 | Train Acc: 102121.87500\n",
            "Train Loss: 441.98196 | Train Acc: 102200.00000\n",
            "Train Loss: 442.21742 | Train Acc: 102287.50000\n",
            "Train Loss: 442.50296 | Train Acc: 102375.00000\n",
            "Train Loss: 442.77114 | Train Acc: 102465.62500\n",
            "Train Loss: 443.00185 | Train Acc: 102559.37500\n",
            "Train Loss: 443.36053 | Train Acc: 102646.87500\n",
            "Train Loss: 443.63835 | Train Acc: 102734.37500\n",
            "Train Loss: 443.86755 | Train Acc: 102828.12500\n",
            "Train Loss: 444.17352 | Train Acc: 102915.62500\n",
            "Train Loss: 444.69280 | Train Acc: 102996.87500\n",
            "Train Loss: 445.10624 | Train Acc: 103081.25000\n",
            "Train Loss: 445.41999 | Train Acc: 103175.00000\n",
            "Train Loss: 445.75444 | Train Acc: 103259.37500\n",
            "Train Loss: 445.93673 | Train Acc: 103353.12500\n",
            "Train Loss: 446.20732 | Train Acc: 103443.75000\n",
            "Train Loss: 446.81801 | Train Acc: 103518.75000\n",
            "Train Loss: 447.36977 | Train Acc: 103593.75000\n",
            "Train Loss: 447.88424 | Train Acc: 103671.87500\n",
            "Train Loss: 448.40206 | Train Acc: 103753.12500\n",
            "Train Loss: 448.70441 | Train Acc: 103840.62500\n",
            "Train Loss: 449.12898 | Train Acc: 103918.75000\n",
            "Train Loss: 449.74130 | Train Acc: 104003.12500\n",
            "Train Loss: 449.95306 | Train Acc: 104096.87500\n",
            "Train Loss: 450.21355 | Train Acc: 104187.50000\n",
            "Train Loss: 450.37930 | Train Acc: 104281.25000\n",
            "Train Loss: 451.00676 | Train Acc: 104368.75000\n",
            "Train Loss: 451.52632 | Train Acc: 104450.00000\n",
            "Train Loss: 451.99218 | Train Acc: 104534.37500\n",
            "Train Loss: 452.13688 | Train Acc: 104634.37500\n",
            "Train Loss: 452.89638 | Train Acc: 104712.50000\n",
            "Train Loss: 453.46792 | Train Acc: 104800.00000\n",
            "Train Loss: 453.86240 | Train Acc: 104881.25000\n",
            "Train Loss: 454.07124 | Train Acc: 104975.00000\n",
            "Train Loss: 454.50446 | Train Acc: 105065.62500\n",
            "Train Loss: 454.97194 | Train Acc: 105143.75000\n",
            "Train Loss: 455.31901 | Train Acc: 105231.25000\n",
            "Train Loss: 455.61569 | Train Acc: 105315.62500\n",
            "Train Loss: 456.13501 | Train Acc: 105396.87500\n",
            "Train Loss: 456.48333 | Train Acc: 105487.50000\n",
            "Train Loss: 456.75391 | Train Acc: 105571.87500\n",
            "Train Loss: 457.38502 | Train Acc: 105643.75000\n",
            "Train Loss: 457.62557 | Train Acc: 105731.25000\n",
            "Train Loss: 457.98904 | Train Acc: 105815.62500\n",
            "Train Loss: 458.25117 | Train Acc: 105909.37500\n",
            "Train Loss: 458.59524 | Train Acc: 106000.00000\n",
            "Train Loss: 458.94415 | Train Acc: 106084.37500\n",
            "Train Loss: 459.21574 | Train Acc: 106168.75000\n",
            "Train Loss: 459.71983 | Train Acc: 106253.12500\n",
            "Train Loss: 460.17514 | Train Acc: 106340.62500\n",
            "Train Loss: 460.59799 | Train Acc: 106425.00000\n",
            "Train Loss: 460.92608 | Train Acc: 106509.37500\n",
            "Train Loss: 461.35397 | Train Acc: 106590.62500\n",
            "Train Loss: 461.69540 | Train Acc: 106675.00000\n",
            "Train Loss: 461.89793 | Train Acc: 106768.75000\n",
            "Train Loss: 462.28263 | Train Acc: 106856.25000\n",
            "Train Loss: 462.61598 | Train Acc: 106946.87500\n",
            "Train Loss: 462.94575 | Train Acc: 107028.12500\n",
            "Train Loss: 463.28611 | Train Acc: 107118.75000\n",
            "Train Loss: 463.51147 | Train Acc: 107215.62500\n",
            "Train Loss: 463.96326 | Train Acc: 107293.75000\n",
            "Train Loss: 464.57185 | Train Acc: 107375.00000\n",
            "Train Loss: 465.02022 | Train Acc: 107462.50000\n",
            "Train Loss: 465.40612 | Train Acc: 107556.25000\n",
            "Train Loss: 465.48309 | Train Acc: 107653.12500\n",
            "Train Loss: 465.73037 | Train Acc: 107746.87500\n",
            "Train Loss: 466.16194 | Train Acc: 107831.25000\n",
            "Train Loss: 466.40104 | Train Acc: 107921.87500\n",
            "Train Loss: 466.80957 | Train Acc: 108006.25000\n",
            "Train Loss: 467.21685 | Train Acc: 108096.87500\n",
            "Train Loss: 467.73062 | Train Acc: 108184.37500\n",
            "Train Loss: 467.94851 | Train Acc: 108275.00000\n",
            "Train Loss: 469.02103 | Train Acc: 108343.75000\n",
            "Train Loss: 469.17303 | Train Acc: 108440.62500\n",
            "Train Loss: 469.63879 | Train Acc: 108525.00000\n",
            "Train Loss: 469.94487 | Train Acc: 108612.50000\n",
            "Train Loss: 470.34791 | Train Acc: 108696.87500\n",
            "Train Loss: 470.81104 | Train Acc: 108781.25000\n",
            "Train Loss: 470.94922 | Train Acc: 108878.12500\n",
            "Train Loss: 471.10405 | Train Acc: 108975.00000\n",
            "Train Loss: 471.51861 | Train Acc: 109059.37500\n",
            "Train Loss: 471.79369 | Train Acc: 109153.12500\n",
            "Train Loss: 472.31046 | Train Acc: 109234.37500\n",
            "Train Loss: 472.53727 | Train Acc: 109325.00000\n",
            "Train Loss: 472.89472 | Train Acc: 109409.37500\n",
            "Train Loss: 473.02232 | Train Acc: 109506.25000\n",
            "Train Loss: 473.56015 | Train Acc: 109587.50000\n",
            "Train Loss: 473.96476 | Train Acc: 109678.12500\n",
            "Train Loss: 474.34264 | Train Acc: 109762.50000\n",
            "Train Loss: 474.89218 | Train Acc: 109837.50000\n",
            "Train Loss: 475.19607 | Train Acc: 109931.25000\n",
            "Train Loss: 475.59934 | Train Acc: 110015.62500\n",
            "Train Loss: 475.89176 | Train Acc: 110103.12500\n",
            "Train Loss: 476.05013 | Train Acc: 110200.00000\n",
            "Train Loss: 476.36600 | Train Acc: 110287.50000\n",
            "Train Loss: 476.88818 | Train Acc: 110371.87500\n",
            "Train Loss: 477.15742 | Train Acc: 110462.50000\n",
            "Train Loss: 477.36037 | Train Acc: 110553.12500\n",
            "Train Loss: 477.68208 | Train Acc: 110637.50000\n",
            "Train Loss: 478.08702 | Train Acc: 110725.00000\n",
            "Train Loss: 478.50158 | Train Acc: 110800.00000\n",
            "Train Loss: 478.85038 | Train Acc: 110887.50000\n",
            "Train Loss: 479.29475 | Train Acc: 110971.87500\n",
            "Train Loss: 479.83904 | Train Acc: 111062.50000\n",
            "Train Loss: 480.26592 | Train Acc: 111146.87500\n",
            "Train Loss: 480.76265 | Train Acc: 111225.00000\n",
            "Train Loss: 480.94353 | Train Acc: 111315.62500\n",
            "Train Loss: 481.19620 | Train Acc: 111412.50000\n",
            "Train Loss: 481.55202 | Train Acc: 111496.87500\n",
            "Train Loss: 481.73409 | Train Acc: 111593.75000\n",
            "Train Loss: 481.92240 | Train Acc: 111690.62500\n",
            "Train Loss: 482.21469 | Train Acc: 111778.12500\n",
            "Train Loss: 482.63113 | Train Acc: 111865.62500\n",
            "Train Loss: 482.98166 | Train Acc: 111956.25000\n",
            "Train Loss: 483.18743 | Train Acc: 112046.87500\n",
            "Train Loss: 483.34823 | Train Acc: 112143.75000\n",
            "Train Loss: 484.09121 | Train Acc: 112221.87500\n",
            "Train Loss: 484.58863 | Train Acc: 112303.12500\n",
            "Train Loss: 485.07258 | Train Acc: 112381.25000\n",
            "Train Loss: 485.46458 | Train Acc: 112465.62500\n",
            "Train Loss: 485.75327 | Train Acc: 112556.25000\n",
            "Train Loss: 486.24315 | Train Acc: 112637.50000\n",
            "Train Loss: 486.72517 | Train Acc: 112721.87500\n",
            "Train Loss: 487.42935 | Train Acc: 112809.37500\n",
            "Train Loss: 487.75461 | Train Acc: 112896.87500\n",
            "Train Loss: 488.02344 | Train Acc: 112984.37500\n",
            "Train Loss: 488.14130 | Train Acc: 113078.12500\n",
            "Train Loss: 488.66204 | Train Acc: 113153.12500\n",
            "Train Loss: 488.87759 | Train Acc: 113243.75000\n",
            "Train Loss: 489.13682 | Train Acc: 113334.37500\n",
            "Train Loss: 489.38264 | Train Acc: 113425.00000\n",
            "Train Loss: 489.98355 | Train Acc: 113503.12500\n",
            "Train Loss: 490.19311 | Train Acc: 113587.50000\n",
            "Train Loss: 490.45409 | Train Acc: 113675.00000\n",
            "Train Loss: 490.65274 | Train Acc: 113768.75000\n",
            "Train Loss: 491.14744 | Train Acc: 113850.00000\n",
            "Train Loss: 491.58960 | Train Acc: 113937.50000\n",
            "Train Loss: 492.06909 | Train Acc: 114015.62500\n",
            "Train Loss: 492.72944 | Train Acc: 114096.87500\n",
            "Train Loss: 493.12284 | Train Acc: 114184.37500\n",
            "Train Loss: 493.44672 | Train Acc: 114271.87500\n",
            "Train Loss: 493.69617 | Train Acc: 114365.62500\n",
            "Train Loss: 494.08378 | Train Acc: 114450.00000\n",
            "Train Loss: 494.58490 | Train Acc: 114528.12500\n",
            "Train Loss: 494.89204 | Train Acc: 114618.75000\n",
            "Train Loss: 495.16470 | Train Acc: 114715.62500\n",
            "Train Loss: 495.54374 | Train Acc: 114806.25000\n",
            "Train Loss: 496.14127 | Train Acc: 114890.62500\n",
            "Train Loss: 496.57921 | Train Acc: 114971.87500\n",
            "Train Loss: 496.82439 | Train Acc: 115065.62500\n",
            "Train Loss: 497.00092 | Train Acc: 115156.25000\n",
            "Train Loss: 497.39319 | Train Acc: 115243.75000\n",
            "Train Loss: 497.76705 | Train Acc: 115331.25000\n",
            "Train Loss: 497.90815 | Train Acc: 115428.12500\n",
            "Train Loss: 498.76705 | Train Acc: 115503.12500\n",
            "Train Loss: 499.07443 | Train Acc: 115593.75000\n",
            "Train Loss: 499.74634 | Train Acc: 115675.00000\n",
            "Train Loss: 499.97841 | Train Acc: 115765.62500\n",
            "Train Loss: 500.15402 | Train Acc: 115862.50000\n",
            "Train Loss: 500.40320 | Train Acc: 115953.12500\n",
            "Train Loss: 500.85135 | Train Acc: 116037.50000\n",
            "Train Loss: 501.43139 | Train Acc: 116109.37500\n",
            "Train Loss: 501.90268 | Train Acc: 116200.00000\n",
            "Train Loss: 502.08790 | Train Acc: 116293.75000\n",
            "Train Loss: 502.36947 | Train Acc: 116384.37500\n",
            "Train Loss: 502.53561 | Train Acc: 116481.25000\n",
            "Train Loss: 502.93891 | Train Acc: 116565.62500\n",
            "Train Loss: 503.39822 | Train Acc: 116643.75000\n",
            "Train Loss: 503.64981 | Train Acc: 116734.37500\n",
            "Train Loss: 504.23783 | Train Acc: 116812.50000\n",
            "Train Loss: 504.65073 | Train Acc: 116893.75000\n",
            "Train Loss: 505.02718 | Train Acc: 116975.00000\n",
            "Train Loss: 505.30448 | Train Acc: 117062.50000\n",
            "Train Loss: 505.57723 | Train Acc: 117146.87500\n",
            "Train Loss: 505.96057 | Train Acc: 117234.37500\n",
            "Train Loss: 506.35927 | Train Acc: 117321.87500\n",
            "Train Loss: 506.55851 | Train Acc: 117412.50000\n",
            "Train Loss: 507.06952 | Train Acc: 117487.50000\n",
            "Train Loss: 507.55969 | Train Acc: 117571.87500\n",
            "Train Loss: 508.38384 | Train Acc: 117643.75000\n",
            "Train Loss: 508.91973 | Train Acc: 117721.87500\n",
            "Train Loss: 509.23897 | Train Acc: 117806.25000\n",
            "Train Loss: 509.66038 | Train Acc: 117887.50000\n",
            "Train Loss: 510.06054 | Train Acc: 117978.12500\n",
            "Train Loss: 510.35944 | Train Acc: 118065.62500\n",
            "Train Loss: 510.65249 | Train Acc: 118159.37500\n",
            "Train Loss: 510.75281 | Train Acc: 118259.37500\n",
            "Train Loss: 510.99138 | Train Acc: 118353.12500\n",
            "Train Loss: 511.16681 | Train Acc: 118446.87500\n",
            "Train Loss: 511.49065 | Train Acc: 118534.37500\n",
            "Train Loss: 511.68003 | Train Acc: 118628.12500\n",
            "Train Loss: 511.92595 | Train Acc: 118721.87500\n",
            "Train Loss: 512.19821 | Train Acc: 118812.50000\n",
            "Train Loss: 512.48249 | Train Acc: 118903.12500\n",
            "Train Loss: 513.12140 | Train Acc: 118984.37500\n",
            "Train Loss: 513.38332 | Train Acc: 119075.00000\n",
            "Train Loss: 513.74524 | Train Acc: 119162.50000\n",
            "Train Loss: 514.25277 | Train Acc: 119240.62500\n",
            "Train Loss: 514.32329 | Train Acc: 119340.62500\n",
            "Train Loss: 515.03621 | Train Acc: 119421.87500\n",
            "Train Loss: 515.39257 | Train Acc: 119506.25000\n",
            "Train Loss: 516.10699 | Train Acc: 119581.25000\n",
            "Train Loss: 516.51586 | Train Acc: 119665.62500\n",
            "Train Loss: 516.82855 | Train Acc: 119753.12500\n",
            "Train Loss: 517.32558 | Train Acc: 119843.75000\n",
            "Train Loss: 517.74115 | Train Acc: 119921.87500\n",
            "Train Loss: 517.97938 | Train Acc: 120015.62500\n",
            "Train Loss: 518.18494 | Train Acc: 120109.37500\n",
            "Train Loss: 518.36975 | Train Acc: 120203.12500\n",
            "Train Loss: 518.53275 | Train Acc: 120300.00000\n",
            "Train Loss: 518.95321 | Train Acc: 120387.50000\n",
            "Train Loss: 519.21220 | Train Acc: 120475.00000\n",
            "Train Loss: 519.49821 | Train Acc: 120565.62500\n",
            "Train Loss: 519.76129 | Train Acc: 120659.37500\n",
            "Train Loss: 520.21733 | Train Acc: 120737.50000\n",
            "Train Loss: 520.51781 | Train Acc: 120828.12500\n",
            "Train Loss: 520.88864 | Train Acc: 120912.50000\n",
            "Train Loss: 521.11510 | Train Acc: 121003.12500\n",
            "Train Loss: 521.71621 | Train Acc: 121075.00000\n",
            "Train Loss: 522.16721 | Train Acc: 121159.37500\n",
            "Train Loss: 522.56616 | Train Acc: 121243.75000\n",
            "Train Loss: 522.62536 | Train Acc: 121343.75000\n",
            "Train Loss: 522.96864 | Train Acc: 121434.37500\n",
            "Train Loss: 523.53428 | Train Acc: 121506.25000\n",
            "Train Loss: 523.88776 | Train Acc: 121600.00000\n",
            "Train Loss: 524.24603 | Train Acc: 121687.50000\n",
            "Train Loss: 524.36711 | Train Acc: 121787.50000\n",
            "Train Loss: 524.61397 | Train Acc: 121875.00000\n",
            "Train Loss: 524.93619 | Train Acc: 121959.37500\n",
            "Train Loss: 525.26754 | Train Acc: 122046.87500\n",
            "Train Loss: 525.65104 | Train Acc: 122128.12500\n",
            "Train Loss: 525.82909 | Train Acc: 122221.87500\n",
            "Train Loss: 526.17732 | Train Acc: 122309.37500\n",
            "Train Loss: 526.39823 | Train Acc: 122403.12500\n",
            "Train Loss: 526.68481 | Train Acc: 122487.50000\n",
            "Train Loss: 527.02770 | Train Acc: 122571.87500\n",
            "Train Loss: 527.40343 | Train Acc: 122656.25000\n",
            "Train Loss: 527.84094 | Train Acc: 122734.37500\n",
            "Train Loss: 528.20803 | Train Acc: 122818.75000\n",
            "Train Loss: 528.38580 | Train Acc: 122912.50000\n",
            "Train Loss: 528.74076 | Train Acc: 123003.12500\n",
            "Train Loss: 528.93973 | Train Acc: 123096.87500\n",
            "Train Loss: 529.41553 | Train Acc: 123181.25000\n",
            "Train Loss: 529.77229 | Train Acc: 123265.62500\n",
            "Train Loss: 530.32585 | Train Acc: 123343.75000\n",
            "Train Loss: 530.60384 | Train Acc: 123434.37500\n",
            "Train Loss: 531.20586 | Train Acc: 123512.50000\n",
            "Train Loss: 531.49688 | Train Acc: 123600.00000\n",
            "Train Loss: 531.84070 | Train Acc: 123690.62500\n",
            "Train Loss: 532.12366 | Train Acc: 123784.37500\n",
            "Train Loss: 532.40965 | Train Acc: 123871.87500\n",
            "Train Loss: 532.90249 | Train Acc: 123953.12500\n",
            "Train Loss: 533.18885 | Train Acc: 124040.62500\n",
            "Train Loss: 533.46614 | Train Acc: 124125.00000\n",
            "Train Loss: 533.79122 | Train Acc: 124212.50000\n",
            "Train Loss: 534.12447 | Train Acc: 124300.00000\n",
            "Train Loss: 534.51320 | Train Acc: 124387.50000\n",
            "Train Loss: 535.36238 | Train Acc: 124459.37500\n",
            "Train Loss: 535.82300 | Train Acc: 124546.87500\n",
            "Train Loss: 536.19128 | Train Acc: 124634.37500\n",
            "Train Loss: 536.78642 | Train Acc: 124709.37500\n",
            "Train Loss: 537.16600 | Train Acc: 124800.00000\n",
            "Train Loss: 537.56164 | Train Acc: 124881.25000\n",
            "Train Loss: 537.87352 | Train Acc: 124965.62500\n",
            "Train Loss: 538.07102 | Train Acc: 125056.25000\n",
            "Train Loss: 538.29310 | Train Acc: 125150.00000\n",
            "Train Loss: 538.45819 | Train Acc: 125243.75000\n",
            "Train Loss: 539.04527 | Train Acc: 125321.87500\n",
            "Train Loss: 539.43443 | Train Acc: 125400.00000\n",
            "Train Loss: 539.72668 | Train Acc: 125487.50000\n",
            "Train Loss: 540.02865 | Train Acc: 125581.25000\n",
            "Train Loss: 540.41445 | Train Acc: 125665.62500\n",
            "Train Loss: 540.75787 | Train Acc: 125753.12500\n",
            "Train Loss: 541.03441 | Train Acc: 125843.75000\n",
            "Train Loss: 541.43163 | Train Acc: 125931.25000\n",
            "Train Loss: 541.90085 | Train Acc: 126018.75000\n",
            "Train Loss: 542.18773 | Train Acc: 126106.25000\n",
            "Train Loss: 542.78052 | Train Acc: 126190.62500\n",
            "Train Loss: 543.18896 | Train Acc: 126271.87500\n",
            "Train Loss: 543.43220 | Train Acc: 126365.62500\n",
            "Train Loss: 543.85959 | Train Acc: 126450.00000\n",
            "Train Loss: 544.13290 | Train Acc: 126534.37500\n",
            "Train Loss: 544.50563 | Train Acc: 126621.87500\n",
            "Train Loss: 545.08149 | Train Acc: 126703.12500\n",
            "Train Loss: 545.49780 | Train Acc: 126790.62500\n",
            "Train Loss: 546.02193 | Train Acc: 126868.75000\n",
            "Train Loss: 546.33170 | Train Acc: 126959.37500\n",
            "Train Loss: 546.62040 | Train Acc: 127050.00000\n",
            "Train Loss: 547.07371 | Train Acc: 127131.25000\n",
            "Train Loss: 547.25243 | Train Acc: 127225.00000\n",
            "Train Loss: 547.56204 | Train Acc: 127312.50000\n",
            "Train Loss: 547.95671 | Train Acc: 127396.87500\n",
            "Train Loss: 548.21552 | Train Acc: 127484.37500\n",
            "Train Loss: 548.60053 | Train Acc: 127571.87500\n",
            "Train Loss: 548.85898 | Train Acc: 127659.37500\n",
            "Train Loss: 549.27713 | Train Acc: 127740.62500\n",
            "Train Loss: 549.84286 | Train Acc: 127821.87500\n",
            "Train Loss: 550.32484 | Train Acc: 127909.37500\n",
            "Train Loss: 550.99273 | Train Acc: 127987.50000\n",
            "Train Loss: 551.15342 | Train Acc: 128081.25000\n",
            "Train Loss: 551.78419 | Train Acc: 128162.50000\n",
            "Train Loss: 552.15968 | Train Acc: 128253.12500\n",
            "Train Loss: 552.52207 | Train Acc: 128343.75000\n",
            "Train Loss: 552.72503 | Train Acc: 128437.50000\n",
            "Train Loss: 553.15505 | Train Acc: 128525.00000\n",
            "Train Loss: 553.38093 | Train Acc: 128615.62500\n",
            "Train Loss: 553.84850 | Train Acc: 128696.87500\n",
            "Train Loss: 554.23473 | Train Acc: 128778.12500\n",
            "Train Loss: 555.09948 | Train Acc: 128856.25000\n",
            "Train Loss: 555.58071 | Train Acc: 128940.62500\n",
            "Train Loss: 555.87726 | Train Acc: 129028.12500\n",
            "Train Loss: 556.21774 | Train Acc: 129118.75000\n",
            "Train Loss: 556.60133 | Train Acc: 129209.37500\n",
            "Train Loss: 556.86611 | Train Acc: 129293.75000\n",
            "Train Loss: 557.40622 | Train Acc: 129371.87500\n",
            "Train Loss: 557.78645 | Train Acc: 129459.37500\n",
            "Train Loss: 558.03722 | Train Acc: 129550.00000\n",
            "Train Loss: 558.25904 | Train Acc: 129643.75000\n",
            "Train Loss: 558.39308 | Train Acc: 129737.50000\n",
            "Train Loss: 558.81022 | Train Acc: 129825.00000\n",
            "Train Loss: 559.13199 | Train Acc: 129912.50000\n",
            "Train Loss: 559.58109 | Train Acc: 130000.00000\n",
            "Train Loss: 559.86481 | Train Acc: 130090.62500\n",
            "Train Loss: 560.20120 | Train Acc: 130178.12500\n",
            "Train Loss: 560.53912 | Train Acc: 130268.75000\n",
            "Train Loss: 560.92405 | Train Acc: 130359.37500\n",
            "Train Loss: 561.37679 | Train Acc: 130446.87500\n",
            "Train Loss: 561.77225 | Train Acc: 130534.37500\n",
            "Train Loss: 562.11598 | Train Acc: 130618.75000\n",
            "Train Loss: 562.42364 | Train Acc: 130709.37500\n",
            "Train Loss: 562.59925 | Train Acc: 130803.12500\n",
            "Train Loss: 563.14382 | Train Acc: 130887.50000\n",
            "Train Loss: 563.55564 | Train Acc: 130971.87500\n",
            "Train Loss: 564.00398 | Train Acc: 131046.87500\n",
            "Train Loss: 564.44218 | Train Acc: 131137.50000\n",
            "Train Loss: 565.05036 | Train Acc: 131209.37500\n",
            "Train Loss: 565.30954 | Train Acc: 131303.12500\n",
            "Train Loss: 565.58161 | Train Acc: 131393.75000\n",
            "Train Loss: 566.12225 | Train Acc: 131465.62500\n",
            "Train Loss: 566.40957 | Train Acc: 131553.12500\n",
            "Train Loss: 566.71818 | Train Acc: 131640.62500\n",
            "Train Loss: 567.08059 | Train Acc: 131725.00000\n",
            "Train Loss: 567.60133 | Train Acc: 131800.00000\n",
            "Train Loss: 568.48853 | Train Acc: 131871.87500\n",
            "Train Loss: 568.96610 | Train Acc: 131956.25000\n",
            "Train Loss: 569.65896 | Train Acc: 132031.25000\n",
            "Train Loss: 570.02552 | Train Acc: 132118.75000\n",
            "Train Loss: 570.44952 | Train Acc: 132209.37500\n",
            "Train Loss: 570.49513 | Train Acc: 132309.37500\n",
            "Train Loss: 570.79701 | Train Acc: 132393.75000\n",
            "Train Loss: 570.88848 | Train Acc: 132493.75000\n",
            "Train Loss: 571.23090 | Train Acc: 132584.37500\n",
            "Train Loss: 571.44670 | Train Acc: 132678.12500\n",
            "Train Loss: 571.91916 | Train Acc: 132765.62500\n",
            "Train Loss: 572.32854 | Train Acc: 132850.00000\n",
            "Train Loss: 572.74056 | Train Acc: 132937.50000\n",
            "Train Loss: 573.21951 | Train Acc: 133025.00000\n",
            "Train Loss: 573.49465 | Train Acc: 133115.62500\n",
            "Train Loss: 573.86055 | Train Acc: 133203.12500\n",
            "Train Loss: 574.57153 | Train Acc: 133271.87500\n",
            "Train Loss: 574.70981 | Train Acc: 133368.75000\n",
            "Train Loss: 574.89895 | Train Acc: 133459.37500\n",
            "Train Loss: 575.22213 | Train Acc: 133546.87500\n",
            "Train Loss: 575.68644 | Train Acc: 133631.25000\n",
            "Train Loss: 575.90516 | Train Acc: 133725.00000\n",
            "Train Loss: 576.21243 | Train Acc: 133818.75000\n",
            "Train Loss: 576.63240 | Train Acc: 133906.25000\n",
            "Train Loss: 577.30002 | Train Acc: 133993.75000\n",
            "Train Loss: 577.64575 | Train Acc: 134084.37500\n",
            "Train Loss: 577.88327 | Train Acc: 134178.12500\n",
            "Train Loss: 578.14814 | Train Acc: 134268.75000\n",
            "Train Loss: 578.51146 | Train Acc: 134353.12500\n",
            "Train Loss: 578.80727 | Train Acc: 134446.87500\n",
            "Train Loss: 579.04545 | Train Acc: 134540.62500\n",
            "Train Loss: 579.37471 | Train Acc: 134625.00000\n",
            "Train Loss: 579.67318 | Train Acc: 134709.37500\n",
            "Train Loss: 580.10292 | Train Acc: 134790.62500\n",
            "Train Loss: 580.81965 | Train Acc: 134871.87500\n",
            "Train Loss: 581.12820 | Train Acc: 134959.37500\n",
            "Train Loss: 581.37647 | Train Acc: 135053.12500\n",
            "Train Loss: 581.60526 | Train Acc: 135146.87500\n",
            "Train Loss: 581.92433 | Train Acc: 135231.25000\n",
            "Train Loss: 582.15258 | Train Acc: 135325.00000\n",
            "Train Loss: 582.49745 | Train Acc: 135415.62500\n",
            "Train Loss: 582.76644 | Train Acc: 135503.12500\n",
            "Train Loss: 583.01075 | Train Acc: 135593.75000\n",
            "Train Loss: 583.22849 | Train Acc: 135687.50000\n",
            "Train Loss: 583.58989 | Train Acc: 135775.00000\n",
            "Train Loss: 584.03055 | Train Acc: 135856.25000\n",
            "Train Loss: 584.43440 | Train Acc: 135946.87500\n",
            "Train Loss: 584.80822 | Train Acc: 136034.37500\n",
            "Train Loss: 585.50322 | Train Acc: 136112.50000\n",
            "Train Loss: 585.82659 | Train Acc: 136200.00000\n",
            "Train Loss: 586.00806 | Train Acc: 136296.87500\n",
            "Train Loss: 586.23013 | Train Acc: 136384.37500\n",
            "Train Loss: 586.55983 | Train Acc: 136468.75000\n",
            "Train Loss: 586.95808 | Train Acc: 136562.50000\n",
            "Train Loss: 587.25662 | Train Acc: 136650.00000\n",
            "Train Loss: 587.45217 | Train Acc: 136743.75000\n",
            "Train Loss: 587.73158 | Train Acc: 136831.25000\n",
            "Train Loss: 588.02309 | Train Acc: 136918.75000\n",
            "Train Loss: 588.50703 | Train Acc: 137003.12500\n",
            "Train Loss: 588.61938 | Train Acc: 137103.12500\n",
            "Train Loss: 589.47221 | Train Acc: 137171.87500\n",
            "Train Loss: 589.86770 | Train Acc: 137250.00000\n",
            "Train Loss: 590.56779 | Train Acc: 137328.12500\n",
            "Train Loss: 590.76131 | Train Acc: 137418.75000\n",
            "Train Loss: 591.05634 | Train Acc: 137506.25000\n",
            "Train Loss: 591.28825 | Train Acc: 137603.12500\n",
            "Train Loss: 591.83430 | Train Acc: 137681.25000\n",
            "Train Loss: 592.67384 | Train Acc: 137756.25000\n",
            "Train Loss: 592.92369 | Train Acc: 137846.87500\n",
            "Train Loss: 593.22123 | Train Acc: 137940.62500\n",
            "Train Loss: 593.41761 | Train Acc: 138034.37500\n",
            "Train Loss: 593.68095 | Train Acc: 138118.75000\n",
            "Train Loss: 594.19493 | Train Acc: 138196.87500\n",
            "Train Loss: 594.51255 | Train Acc: 138281.25000\n",
            "Train Loss: 594.90292 | Train Acc: 138365.62500\n",
            "Train Loss: 595.16393 | Train Acc: 138456.25000\n",
            "Train Loss: 595.43958 | Train Acc: 138546.87500\n",
            "Train Loss: 595.70340 | Train Acc: 138637.50000\n",
            "Train Loss: 596.03079 | Train Acc: 138725.00000\n",
            "Train Loss: 596.54990 | Train Acc: 138809.37500\n",
            "Train Loss: 596.84611 | Train Acc: 138896.87500\n",
            "Train Loss: 597.11839 | Train Acc: 138981.25000\n",
            "Train Loss: 597.46390 | Train Acc: 139071.87500\n",
            "Train Loss: 597.69718 | Train Acc: 139159.37500\n",
            "Train Loss: 598.25599 | Train Acc: 139237.50000\n",
            "Train Loss: 598.62156 | Train Acc: 139321.87500\n",
            "Train Loss: 599.03299 | Train Acc: 139406.25000\n",
            "Train Loss: 599.33541 | Train Acc: 139484.37500\n",
            "Train Loss: 599.55813 | Train Acc: 139581.25000\n",
            "Train Loss: 599.94467 | Train Acc: 139665.62500\n",
            "Train Loss: 600.13521 | Train Acc: 139762.50000\n",
            "Train Loss: 600.32722 | Train Acc: 139856.25000\n",
            "Train Loss: 600.54927 | Train Acc: 139946.87500\n",
            "Train Loss: 600.76707 | Train Acc: 140043.75000\n",
            "Train Loss: 600.89237 | Train Acc: 140140.62500\n",
            "Train Loss: 601.07168 | Train Acc: 140234.37500\n",
            "Train Loss: 601.34888 | Train Acc: 140321.87500\n",
            "Train Loss: 601.68649 | Train Acc: 140415.62500\n",
            "Train Loss: 602.10426 | Train Acc: 140493.75000\n",
            "Train Loss: 602.58911 | Train Acc: 140578.12500\n",
            "Train Loss: 603.06648 | Train Acc: 140662.50000\n",
            "Train Loss: 603.94305 | Train Acc: 140743.75000\n",
            "Train Loss: 604.33018 | Train Acc: 140834.37500\n",
            "Train Loss: 604.67944 | Train Acc: 140925.00000\n",
            "Train Loss: 604.88786 | Train Acc: 141018.75000\n",
            "Train Loss: 605.37532 | Train Acc: 141106.25000\n",
            "Train Loss: 605.70122 | Train Acc: 141190.62500\n",
            "Train Loss: 605.94418 | Train Acc: 141278.12500\n",
            "Train Loss: 606.31340 | Train Acc: 141368.75000\n",
            "Train Loss: 606.73693 | Train Acc: 141453.12500\n",
            "Train Loss: 606.84963 | Train Acc: 141553.12500\n",
            "Train Loss: 607.30055 | Train Acc: 141640.62500\n",
            "Train Loss: 607.66588 | Train Acc: 141725.00000\n",
            "Train Loss: 608.03044 | Train Acc: 141812.50000\n",
            "Train Loss: 608.23709 | Train Acc: 141903.12500\n",
            "Train Loss: 608.55907 | Train Acc: 141993.75000\n",
            "Train Loss: 609.01393 | Train Acc: 142081.25000\n",
            "Train Loss: 609.32775 | Train Acc: 142162.50000\n",
            "Train Loss: 609.58347 | Train Acc: 142253.12500\n",
            "Train Loss: 609.77630 | Train Acc: 142350.00000\n",
            "Train Loss: 610.45573 | Train Acc: 142425.00000\n",
            "Train Loss: 610.74051 | Train Acc: 142512.50000\n",
            "Train Loss: 611.10278 | Train Acc: 142600.00000\n",
            "Train Loss: 611.26452 | Train Acc: 142693.75000\n",
            "Train Loss: 611.66708 | Train Acc: 142778.12500\n",
            "Train Loss: 611.92686 | Train Acc: 142871.87500\n",
            "Train Loss: 612.07717 | Train Acc: 142971.87500\n",
            "Train Loss: 612.39587 | Train Acc: 143053.12500\n",
            "Train Loss: 612.53999 | Train Acc: 143150.00000\n",
            "Train Loss: 612.75323 | Train Acc: 143243.75000\n",
            "Train Loss: 613.10394 | Train Acc: 143331.25000\n",
            "Train Loss: 613.58237 | Train Acc: 143415.62500\n",
            "Train Loss: 613.99881 | Train Acc: 143503.12500\n",
            "Train Loss: 614.31039 | Train Acc: 143587.50000\n",
            "Train Loss: 614.58142 | Train Acc: 143678.12500\n",
            "Train Loss: 614.85337 | Train Acc: 143765.62500\n",
            "Train Loss: 615.24695 | Train Acc: 143853.12500\n",
            "Train Loss: 615.60264 | Train Acc: 143943.75000\n",
            "Train Loss: 615.83530 | Train Acc: 144034.37500\n",
            "Train Loss: 616.40843 | Train Acc: 144112.50000\n",
            "Train Loss: 616.82994 | Train Acc: 144193.75000\n",
            "Train Loss: 617.01551 | Train Acc: 144287.50000\n",
            "Train Loss: 617.42758 | Train Acc: 144365.62500\n",
            "Train Loss: 617.91516 | Train Acc: 144450.00000\n",
            "Train Loss: 618.57490 | Train Acc: 144531.25000\n",
            "Train Loss: 618.85938 | Train Acc: 144621.87500\n",
            "Train Loss: 619.11055 | Train Acc: 144706.25000\n",
            "Train Loss: 619.27499 | Train Acc: 144800.00000\n",
            "Train Loss: 619.47422 | Train Acc: 144893.75000\n",
            "Train Loss: 620.21669 | Train Acc: 144971.87500\n",
            "Train Loss: 620.55788 | Train Acc: 145059.37500\n",
            "Train Loss: 621.15665 | Train Acc: 145140.62500\n",
            "Train Loss: 621.47556 | Train Acc: 145228.12500\n",
            "Train Loss: 621.69506 | Train Acc: 145321.87500\n",
            "Train Loss: 622.22761 | Train Acc: 145403.12500\n",
            "Train Loss: 622.48641 | Train Acc: 145490.62500\n",
            "Train Loss: 622.78357 | Train Acc: 145584.37500\n",
            "Train Loss: 623.03486 | Train Acc: 145678.12500\n",
            "Train Loss: 623.70259 | Train Acc: 145759.37500\n",
            "Train Loss: 624.22340 | Train Acc: 145846.87500\n",
            "Train Loss: 624.76637 | Train Acc: 145937.50000\n",
            "Train Loss: 624.88981 | Train Acc: 146034.37500\n",
            "Train Loss: 625.00494 | Train Acc: 146128.12500\n",
            "Train Loss: 625.11617 | Train Acc: 146228.12500\n",
            "Train Loss: 625.34925 | Train Acc: 146312.50000\n",
            "Train Loss: 625.90320 | Train Acc: 146390.62500\n",
            "Train Loss: 626.30744 | Train Acc: 146475.00000\n",
            "Train Loss: 626.50235 | Train Acc: 146571.87500\n",
            "Train Loss: 626.85600 | Train Acc: 146659.37500\n",
            "Train Loss: 627.13415 | Train Acc: 146750.00000\n",
            "Train Loss: 627.62955 | Train Acc: 146828.12500\n",
            "Train Loss: 627.82837 | Train Acc: 146918.75000\n",
            "Train Loss: 628.40022 | Train Acc: 147003.12500\n",
            "Train Loss: 628.74331 | Train Acc: 147084.37500\n",
            "Train Loss: 629.08721 | Train Acc: 147168.75000\n",
            "Train Loss: 629.36082 | Train Acc: 147259.37500\n",
            "Train Loss: 629.58869 | Train Acc: 147356.25000\n",
            "Train Loss: 629.86432 | Train Acc: 147443.75000\n",
            "Train Loss: 630.36916 | Train Acc: 147518.75000\n",
            "Train Loss: 630.85070 | Train Acc: 147603.12500\n",
            "Train Loss: 631.29792 | Train Acc: 147687.50000\n",
            "Train Loss: 631.65953 | Train Acc: 147778.12500\n",
            "Train Loss: 632.09606 | Train Acc: 147856.25000\n",
            "Train Loss: 632.41084 | Train Acc: 147943.75000\n",
            "Train Loss: 632.70502 | Train Acc: 148031.25000\n",
            "Train Loss: 633.29828 | Train Acc: 148109.37500\n",
            "Train Loss: 633.55971 | Train Acc: 148203.12500\n",
            "Train Loss: 633.73867 | Train Acc: 148300.00000\n",
            "Train Loss: 633.94643 | Train Acc: 148393.75000\n",
            "Train Loss: 634.45372 | Train Acc: 148478.12500\n",
            "Train Loss: 634.79698 | Train Acc: 148568.75000\n",
            "Train Loss: 635.09058 | Train Acc: 148659.37500\n",
            "Train Loss: 635.47738 | Train Acc: 148743.75000\n",
            "Train Loss: 636.15047 | Train Acc: 148828.12500\n",
            "Train Loss: 636.57154 | Train Acc: 148912.50000\n",
            "Train Loss: 636.90113 | Train Acc: 149006.25000\n",
            "Train Loss: 637.33480 | Train Acc: 149093.75000\n",
            "Train Loss: 637.55353 | Train Acc: 149187.50000\n",
            "Train Loss: 638.00523 | Train Acc: 149275.00000\n",
            "Train Loss: 638.21391 | Train Acc: 149368.75000\n",
            "Train Loss: 638.80112 | Train Acc: 149450.00000\n",
            "Train Loss: 639.19547 | Train Acc: 149543.75000\n",
            "Train Loss: 639.38262 | Train Acc: 149637.50000\n",
            "Train Loss: 639.74987 | Train Acc: 149725.00000\n",
            "Train Loss: 640.30548 | Train Acc: 149809.37500\n",
            "Train Loss: 640.85147 | Train Acc: 149887.50000\n",
            "Train Loss: 641.06122 | Train Acc: 149981.25000\n",
            "Train Loss: 641.36397 | Train Acc: 150068.75000\n",
            "Train Loss: 641.66938 | Train Acc: 150153.12500\n",
            "Train Loss: 642.03629 | Train Acc: 150237.50000\n",
            "Train Loss: 642.37683 | Train Acc: 150321.87500\n",
            "Train Loss: 642.96644 | Train Acc: 150406.25000\n",
            "Train Loss: 643.28134 | Train Acc: 150496.87500\n",
            "Train Loss: 643.64690 | Train Acc: 150584.37500\n",
            "Train Loss: 643.79652 | Train Acc: 150681.25000\n",
            "Train Loss: 644.18419 | Train Acc: 150765.62500\n",
            "Train Loss: 644.56728 | Train Acc: 150853.12500\n",
            "Train Loss: 644.90121 | Train Acc: 150940.62500\n",
            "Train Loss: 645.29394 | Train Acc: 151028.12500\n",
            "Train Loss: 645.58856 | Train Acc: 151118.75000\n",
            "Train Loss: 645.96620 | Train Acc: 151206.25000\n",
            "Train Loss: 646.22254 | Train Acc: 151296.87500\n",
            "Train Loss: 646.45912 | Train Acc: 151387.50000\n",
            "Train Loss: 646.73456 | Train Acc: 151478.12500\n",
            "Train Loss: 647.04628 | Train Acc: 151565.62500\n",
            "Train Loss: 647.23101 | Train Acc: 151665.62500\n",
            "Train Loss: 647.39658 | Train Acc: 151762.50000\n",
            "Train Loss: 647.66560 | Train Acc: 151850.00000\n",
            "Train Loss: 648.03569 | Train Acc: 151934.37500\n",
            "Train Loss: 648.38343 | Train Acc: 152015.62500\n",
            "Train Loss: 648.64675 | Train Acc: 152109.37500\n",
            "Train Loss: 649.26687 | Train Acc: 152187.50000\n",
            "Train Loss: 649.52753 | Train Acc: 152278.12500\n",
            "Train Loss: 649.88860 | Train Acc: 152368.75000\n",
            "Train Loss: 650.19843 | Train Acc: 152450.00000\n",
            "Train Loss: 650.61992 | Train Acc: 152531.25000\n",
            "Train Loss: 650.91246 | Train Acc: 152618.75000\n",
            "Train Loss: 651.30810 | Train Acc: 152706.25000\n",
            "Train Loss: 651.67217 | Train Acc: 152793.75000\n",
            "Train Loss: 651.86965 | Train Acc: 152887.50000\n",
            "Train Loss: 652.27259 | Train Acc: 152971.87500\n",
            "Train Loss: 652.61997 | Train Acc: 153056.25000\n",
            "Train Loss: 653.02862 | Train Acc: 153137.50000\n",
            "Train Loss: 653.30042 | Train Acc: 153221.87500\n",
            "Train Loss: 653.81989 | Train Acc: 153306.25000\n",
            "Train Loss: 654.43751 | Train Acc: 153381.25000\n",
            "Train Loss: 654.79003 | Train Acc: 153462.50000\n",
            "Train Loss: 655.23352 | Train Acc: 153550.00000\n",
            "Train Loss: 655.51692 | Train Acc: 153637.50000\n",
            "Train Loss: 655.97130 | Train Acc: 153721.87500\n",
            "Train Loss: 656.35973 | Train Acc: 153812.50000\n",
            "Train Loss: 656.52293 | Train Acc: 153903.12500\n",
            "Train Loss: 657.29034 | Train Acc: 153971.87500\n",
            "Train Loss: 657.51143 | Train Acc: 154065.62500\n",
            "Train Loss: 657.95556 | Train Acc: 154156.25000\n",
            "Train Loss: 658.38390 | Train Acc: 154240.62500\n",
            "Train Loss: 658.69217 | Train Acc: 154331.25000\n",
            "Train Loss: 659.06143 | Train Acc: 154418.75000\n",
            "Train Loss: 659.40274 | Train Acc: 154500.00000\n",
            "Train Loss: 659.81910 | Train Acc: 154587.50000\n",
            "Train Loss: 659.99217 | Train Acc: 154678.12500\n",
            "Train Loss: 660.37847 | Train Acc: 154762.50000\n",
            "Train Loss: 660.94930 | Train Acc: 154846.87500\n",
            "Train Loss: 661.38980 | Train Acc: 154928.12500\n",
            "Train Loss: 661.53627 | Train Acc: 155021.87500\n",
            "Train Loss: 662.15326 | Train Acc: 155106.25000\n",
            "Train Loss: 662.59445 | Train Acc: 155187.50000\n",
            "Train Loss: 663.08825 | Train Acc: 155265.62500\n",
            "Train Loss: 663.38577 | Train Acc: 155356.25000\n",
            "Train Loss: 663.72559 | Train Acc: 155440.62500\n",
            "Train Loss: 663.94853 | Train Acc: 155531.25000\n",
            "Train Loss: 664.46977 | Train Acc: 155612.50000\n",
            "Train Loss: 664.82506 | Train Acc: 155696.87500\n",
            "Train Loss: 665.22786 | Train Acc: 155778.12500\n",
            "Train Loss: 665.40413 | Train Acc: 155875.00000\n",
            "Train Loss: 665.73207 | Train Acc: 155956.25000\n",
            "Train Loss: 666.15723 | Train Acc: 156037.50000\n",
            "Train Loss: 666.25389 | Train Acc: 156134.37500\n",
            "Train Loss: 666.52086 | Train Acc: 156221.87500\n",
            "Train Loss: 666.95185 | Train Acc: 156309.37500\n",
            "Train Loss: 667.31345 | Train Acc: 156393.75000\n",
            "Train Loss: 667.82086 | Train Acc: 156481.25000\n",
            "Train Loss: 668.05701 | Train Acc: 156571.87500\n",
            "Train Loss: 668.76496 | Train Acc: 156653.12500\n",
            "Train Loss: 669.12993 | Train Acc: 156746.87500\n",
            "Train Loss: 669.38658 | Train Acc: 156840.62500\n",
            "Train Loss: 669.60826 | Train Acc: 156928.12500\n",
            "Train Loss: 670.18503 | Train Acc: 157009.37500\n",
            "Train Loss: 670.45485 | Train Acc: 157093.75000\n",
            "Train Loss: 670.93877 | Train Acc: 157184.37500\n",
            "Train Loss: 671.42999 | Train Acc: 157259.37500\n",
            "Train Loss: 671.88174 | Train Acc: 157340.62500\n",
            "Train Loss: 672.23901 | Train Acc: 157425.00000\n",
            "Train Loss: 672.99722 | Train Acc: 157496.87500\n",
            "Train Loss: 673.57385 | Train Acc: 157578.12500\n",
            "Train Loss: 673.73501 | Train Acc: 157675.00000\n",
            "Train Loss: 673.92529 | Train Acc: 157768.75000\n",
            "Train Loss: 674.55304 | Train Acc: 157846.87500\n",
            "Train Loss: 674.82078 | Train Acc: 157937.50000\n",
            "Train Loss: 675.31829 | Train Acc: 158015.62500\n",
            "Train Loss: 675.62908 | Train Acc: 158103.12500\n",
            "Train Loss: 676.26476 | Train Acc: 158184.37500\n",
            "Train Loss: 676.52852 | Train Acc: 158268.75000\n",
            "Train Loss: 676.84907 | Train Acc: 158353.12500\n",
            "Train Loss: 677.20213 | Train Acc: 158437.50000\n",
            "Train Loss: 677.89536 | Train Acc: 158521.87500\n",
            "Train Loss: 678.32242 | Train Acc: 158603.12500\n",
            "Train Loss: 678.84007 | Train Acc: 158684.37500\n",
            "Train Loss: 679.12975 | Train Acc: 158771.87500\n",
            "Train Loss: 679.50100 | Train Acc: 158859.37500\n",
            "Train Loss: 679.97069 | Train Acc: 158937.50000\n",
            "Train Loss: 680.17189 | Train Acc: 159028.12500\n",
            "Train Loss: 680.56877 | Train Acc: 159115.62500\n",
            "Train Loss: 681.09399 | Train Acc: 159196.87500\n",
            "Train Loss: 681.41676 | Train Acc: 159290.62500\n",
            "Train Loss: 681.71234 | Train Acc: 159381.25000\n",
            "Train Loss: 682.11229 | Train Acc: 159465.62500\n",
            "Train Loss: 682.52660 | Train Acc: 159546.87500\n",
            "Train Loss: 682.65980 | Train Acc: 159643.75000\n",
            "Train Loss: 683.18820 | Train Acc: 159728.12500\n",
            "Train Loss: 683.56934 | Train Acc: 159815.62500\n",
            "Train Loss: 683.71729 | Train Acc: 159912.50000\n",
            "Train Loss: 684.24725 | Train Acc: 159993.75000\n",
            "Train Loss: 684.46463 | Train Acc: 160084.37500\n",
            "Train Loss: 684.66233 | Train Acc: 160181.25000\n",
            "Train Loss: 685.32237 | Train Acc: 160259.37500\n",
            "Train Loss: 685.57580 | Train Acc: 160350.00000\n",
            "Train Loss: 685.77285 | Train Acc: 160440.62500\n",
            "Train Loss: 686.12895 | Train Acc: 160525.00000\n",
            "Train Loss: 686.62525 | Train Acc: 160618.75000\n",
            "Train Loss: 687.23804 | Train Acc: 160696.87500\n",
            "Train Loss: 687.87608 | Train Acc: 160771.87500\n",
            "Train Loss: 688.01632 | Train Acc: 160868.75000\n",
            "Train Loss: 688.41006 | Train Acc: 160953.12500\n",
            "Train Loss: 688.96308 | Train Acc: 161037.50000\n",
            "Train Loss: 689.27508 | Train Acc: 161125.00000\n",
            "Train Loss: 689.96587 | Train Acc: 161203.12500\n",
            "Train Loss: 690.18695 | Train Acc: 161293.75000\n",
            "Train Loss: 690.58091 | Train Acc: 161378.12500\n",
            "Train Loss: 690.94273 | Train Acc: 161462.50000\n",
            "Train Loss: 691.17411 | Train Acc: 161550.00000\n",
            "Train Loss: 691.62348 | Train Acc: 161628.12500\n",
            "Train Loss: 692.02665 | Train Acc: 161712.50000\n",
            "Train Loss: 692.46540 | Train Acc: 161800.00000\n",
            "Train Loss: 692.75835 | Train Acc: 161890.62500\n",
            "Train Loss: 692.98103 | Train Acc: 161981.25000\n",
            "Train Loss: 693.32077 | Train Acc: 162068.75000\n",
            "Train Loss: 693.55379 | Train Acc: 162165.62500\n",
            "Train Loss: 693.92101 | Train Acc: 162246.87500\n",
            "Train Loss: 694.32856 | Train Acc: 162337.50000\n",
            "Train Loss: 694.64346 | Train Acc: 162425.00000\n",
            "Train Loss: 694.88142 | Train Acc: 162512.50000\n",
            "Train Loss: 695.25952 | Train Acc: 162600.00000\n",
            "Train Loss: 695.69512 | Train Acc: 162681.25000\n",
            "Train Loss: 696.05541 | Train Acc: 162771.87500\n",
            "Train Loss: 696.43522 | Train Acc: 162862.50000\n",
            "Test_loss:0.42114,Test_accuracy:84.37500\n",
            "Test_loss:0.69776,Test_accuracy:168.75000\n",
            "Test_loss:1.14897,Test_accuracy:256.25000\n",
            "Test_loss:1.39289,Test_accuracy:346.87500\n",
            "Test_loss:1.96420,Test_accuracy:428.12500\n",
            "Test_loss:2.08099,Test_accuracy:528.12500\n",
            "Test_loss:2.32961,Test_accuracy:618.75000\n",
            "Test_loss:2.90172,Test_accuracy:696.87500\n",
            "Test_loss:3.28720,Test_accuracy:787.50000\n",
            "Test_loss:3.64857,Test_accuracy:881.25000\n",
            "Test_loss:4.10357,Test_accuracy:968.75000\n",
            "Test_loss:4.57616,Test_accuracy:1053.12500\n",
            "Test_loss:4.96065,Test_accuracy:1143.75000\n",
            "Test_loss:5.22017,Test_accuracy:1237.50000\n",
            "Test_loss:5.84245,Test_accuracy:1315.62500\n",
            "Test_loss:6.16651,Test_accuracy:1406.25000\n",
            "Test_loss:6.54649,Test_accuracy:1487.50000\n",
            "Test_loss:7.00000,Test_accuracy:1565.62500\n",
            "Test_loss:7.41694,Test_accuracy:1650.00000\n",
            "Test_loss:7.96899,Test_accuracy:1734.37500\n",
            "Test_loss:8.44756,Test_accuracy:1818.75000\n",
            "Test_loss:8.95612,Test_accuracy:1903.12500\n",
            "Test_loss:9.68619,Test_accuracy:1984.37500\n",
            "Test_loss:10.05790,Test_accuracy:2071.87500\n",
            "Test_loss:10.34937,Test_accuracy:2168.75000\n",
            "Test_loss:10.64382,Test_accuracy:2256.25000\n",
            "Test_loss:10.89921,Test_accuracy:2346.87500\n",
            "Test_loss:11.11950,Test_accuracy:2437.50000\n",
            "Test_loss:11.54462,Test_accuracy:2518.75000\n",
            "Test_loss:11.91469,Test_accuracy:2600.00000\n",
            "Test_loss:12.72560,Test_accuracy:2681.25000\n",
            "Test_loss:13.21573,Test_accuracy:2765.62500\n",
            "Test_loss:13.52459,Test_accuracy:2853.12500\n",
            "Test_loss:13.81699,Test_accuracy:2940.62500\n",
            "Test_loss:14.33914,Test_accuracy:3021.87500\n",
            "Test_loss:14.69500,Test_accuracy:3100.00000\n",
            "Test_loss:15.17726,Test_accuracy:3184.37500\n",
            "Test_loss:15.57486,Test_accuracy:3271.87500\n",
            "Test_loss:16.01923,Test_accuracy:3359.37500\n",
            "Test_loss:16.27615,Test_accuracy:3456.25000\n",
            "Test_loss:16.72785,Test_accuracy:3540.62500\n",
            "Test_loss:17.29301,Test_accuracy:3618.75000\n",
            "Test_loss:17.52493,Test_accuracy:3709.37500\n",
            "Test_loss:17.85366,Test_accuracy:3796.87500\n",
            "Test_loss:18.00301,Test_accuracy:3893.75000\n",
            "Test_loss:18.65849,Test_accuracy:3975.00000\n",
            "Test_loss:19.23273,Test_accuracy:4053.12500\n",
            "Test_loss:19.67290,Test_accuracy:4131.25000\n",
            "Test_loss:19.86236,Test_accuracy:4225.00000\n",
            "Test_loss:20.12734,Test_accuracy:4315.62500\n",
            "Test_loss:20.33626,Test_accuracy:4406.25000\n",
            "Test_loss:21.03499,Test_accuracy:4478.12500\n",
            "Test_loss:21.21796,Test_accuracy:4575.00000\n",
            "Test_loss:21.53901,Test_accuracy:4668.75000\n",
            "Test_loss:22.21128,Test_accuracy:4756.25000\n",
            "Test_loss:22.47450,Test_accuracy:4846.87500\n",
            "Test_loss:22.84004,Test_accuracy:4925.00000\n",
            "Test_loss:23.40115,Test_accuracy:5003.12500\n",
            "Test_loss:23.57958,Test_accuracy:5096.87500\n",
            "Test_loss:23.85111,Test_accuracy:5190.62500\n",
            "Test_loss:24.25209,Test_accuracy:5278.12500\n",
            "Test_loss:25.01883,Test_accuracy:5353.12500\n",
            "Test_loss:25.40293,Test_accuracy:5440.62500\n",
            "Test_loss:26.16371,Test_accuracy:5521.87500\n",
            "Test_loss:26.73782,Test_accuracy:5590.62500\n",
            "Test_loss:27.01138,Test_accuracy:5684.37500\n",
            "Test_loss:27.14039,Test_accuracy:5781.25000\n",
            "Test_loss:27.46488,Test_accuracy:5868.75000\n",
            "Test_loss:27.86079,Test_accuracy:5959.37500\n",
            "Test_loss:28.00939,Test_accuracy:6059.37500\n",
            "Test_loss:28.14490,Test_accuracy:6156.25000\n",
            "Test_loss:28.47664,Test_accuracy:6240.62500\n",
            "Test_loss:28.86504,Test_accuracy:6321.87500\n",
            "Test_loss:29.24892,Test_accuracy:6406.25000\n",
            "Test_loss:29.75840,Test_accuracy:6493.75000\n",
            "Test_loss:30.19841,Test_accuracy:6578.12500\n",
            "Test_loss:30.41317,Test_accuracy:6675.00000\n",
            "Test_loss:31.27416,Test_accuracy:6753.12500\n",
            "Test_loss:31.82602,Test_accuracy:6828.12500\n",
            "Test_loss:32.33506,Test_accuracy:6909.37500\n",
            "Test_loss:32.60467,Test_accuracy:6996.87500\n",
            "Test_loss:33.00012,Test_accuracy:7087.50000\n",
            "Test_loss:33.46682,Test_accuracy:7171.87500\n",
            "Test_loss:33.90146,Test_accuracy:7256.25000\n",
            "Test_loss:34.31284,Test_accuracy:7340.62500\n",
            "Test_loss:34.80600,Test_accuracy:7418.75000\n",
            "Test_loss:35.13362,Test_accuracy:7500.00000\n",
            "Test_loss:35.30643,Test_accuracy:7596.87500\n",
            "Test_loss:35.86025,Test_accuracy:7671.87500\n",
            "Test_loss:36.29166,Test_accuracy:7762.50000\n",
            "Test_loss:37.26302,Test_accuracy:7828.12500\n",
            "Test_loss:37.90620,Test_accuracy:7896.87500\n",
            "Test_loss:38.40946,Test_accuracy:7971.87500\n",
            "Test_loss:38.98044,Test_accuracy:8046.87500\n",
            "Test_loss:39.47623,Test_accuracy:8131.25000\n",
            "Test_loss:39.96962,Test_accuracy:8218.75000\n",
            "Test_loss:40.44048,Test_accuracy:8293.75000\n",
            "Test_loss:41.07996,Test_accuracy:8384.37500\n",
            "Test_loss:41.26659,Test_accuracy:8481.25000\n",
            "Test_loss:41.77300,Test_accuracy:8562.50000\n",
            "Test_loss:42.21102,Test_accuracy:8640.62500\n",
            "Test_loss:43.35388,Test_accuracy:8709.37500\n",
            "Test_loss:43.76706,Test_accuracy:8790.62500\n",
            "Test_loss:44.24715,Test_accuracy:8875.00000\n",
            "Test_loss:44.71579,Test_accuracy:8962.50000\n",
            "Test_loss:44.93899,Test_accuracy:9053.12500\n",
            "Test_loss:45.23717,Test_accuracy:9143.75000\n",
            "Test_loss:45.68137,Test_accuracy:9215.62500\n",
            "Test_loss:46.61661,Test_accuracy:9290.62500\n",
            "Test_loss:47.02725,Test_accuracy:9375.00000\n",
            "Test_loss:47.76270,Test_accuracy:9446.87500\n",
            "Test_loss:48.11094,Test_accuracy:9537.50000\n",
            "Test_loss:48.45970,Test_accuracy:9628.12500\n",
            "Test_loss:48.68845,Test_accuracy:9721.87500\n",
            "Test_loss:49.25574,Test_accuracy:9806.25000\n",
            "Test_loss:49.64114,Test_accuracy:9890.62500\n",
            "Test_loss:49.96901,Test_accuracy:9978.12500\n",
            "Test_loss:50.43444,Test_accuracy:10062.50000\n",
            "Test_loss:50.96786,Test_accuracy:10143.75000\n",
            "Test_loss:51.34871,Test_accuracy:10231.25000\n",
            "Test_loss:51.98416,Test_accuracy:10315.62500\n",
            "Test_loss:52.48821,Test_accuracy:10396.87500\n",
            "Test_loss:52.79678,Test_accuracy:10484.37500\n",
            "Test_loss:53.42509,Test_accuracy:10559.37500\n",
            "Test_loss:53.87597,Test_accuracy:10640.62500\n",
            "Test_loss:54.15469,Test_accuracy:10728.12500\n",
            "Test_loss:54.67532,Test_accuracy:10812.50000\n",
            "Test_loss:55.53084,Test_accuracy:10893.75000\n",
            "Test_loss:55.97234,Test_accuracy:10975.00000\n",
            "Test_loss:56.97096,Test_accuracy:11043.75000\n",
            "Test_loss:57.24590,Test_accuracy:11134.37500\n",
            "Test_loss:57.60840,Test_accuracy:11221.87500\n",
            "Test_loss:58.07971,Test_accuracy:11306.25000\n",
            "Test_loss:58.46692,Test_accuracy:11390.62500\n",
            "Test_loss:58.76163,Test_accuracy:11478.12500\n",
            "Test_loss:58.99422,Test_accuracy:11568.75000\n",
            "Test_loss:59.19161,Test_accuracy:11659.37500\n",
            "Test_loss:59.74823,Test_accuracy:11743.75000\n",
            "Test_loss:60.02473,Test_accuracy:11834.37500\n",
            "Test_loss:60.23962,Test_accuracy:11925.00000\n",
            "Test_loss:60.71485,Test_accuracy:12009.37500\n",
            "Test_loss:60.91484,Test_accuracy:12103.12500\n",
            "Test_loss:61.12303,Test_accuracy:12196.87500\n",
            "Test_loss:61.28489,Test_accuracy:12293.75000\n",
            "Test_loss:61.48992,Test_accuracy:12384.37500\n",
            "Test_loss:62.15748,Test_accuracy:12462.50000\n",
            "Test_loss:62.84870,Test_accuracy:12540.62500\n",
            "Test_loss:63.21242,Test_accuracy:12628.12500\n",
            "Test_loss:63.62399,Test_accuracy:12712.50000\n",
            "Test_loss:63.81635,Test_accuracy:12809.37500\n",
            "Test_loss:64.64524,Test_accuracy:12890.62500\n",
            "Test_loss:65.18623,Test_accuracy:12962.50000\n",
            "Test_loss:65.61249,Test_accuracy:13040.62500\n",
            "Test_loss:66.22828,Test_accuracy:13118.75000\n",
            "Test_loss:66.56883,Test_accuracy:13209.37500\n",
            "Test_loss:66.80220,Test_accuracy:13300.00000\n",
            "Test_loss:67.46680,Test_accuracy:13384.37500\n",
            "Test_loss:68.19174,Test_accuracy:13456.25000\n",
            "Test_loss:68.91412,Test_accuracy:13534.37500\n",
            "Test_loss:69.28194,Test_accuracy:13621.87500\n",
            "Test_loss:69.85523,Test_accuracy:13703.12500\n",
            "Test_loss:70.52260,Test_accuracy:13778.12500\n",
            "Test_loss:71.15208,Test_accuracy:13859.37500\n",
            "Test_loss:71.44839,Test_accuracy:13953.12500\n",
            "Test_loss:72.19524,Test_accuracy:14031.25000\n",
            "Test_loss:72.47657,Test_accuracy:14118.75000\n",
            "Test_loss:73.21330,Test_accuracy:14203.12500\n",
            "Test_loss:73.36388,Test_accuracy:14293.75000\n",
            "Test_loss:73.75033,Test_accuracy:14378.12500\n",
            "Test_loss:74.09685,Test_accuracy:14465.62500\n",
            "Test_loss:74.87446,Test_accuracy:14543.75000\n",
            "Test_loss:75.44042,Test_accuracy:14628.12500\n",
            "Test_loss:76.53279,Test_accuracy:14696.87500\n",
            "Test_loss:77.01415,Test_accuracy:14778.12500\n",
            "Test_loss:78.08362,Test_accuracy:14846.87500\n",
            "Test_loss:78.46983,Test_accuracy:14937.50000\n",
            "Test_loss:78.83089,Test_accuracy:15015.62500\n",
            "Test_loss:79.12602,Test_accuracy:15106.25000\n",
            "Test_loss:79.71858,Test_accuracy:15181.25000\n",
            "Test_loss:80.13989,Test_accuracy:15262.50000\n",
            "Test_loss:80.52879,Test_accuracy:15353.12500\n",
            "Test_loss:81.11830,Test_accuracy:15434.37500\n",
            "Test_loss:81.47498,Test_accuracy:15521.87500\n",
            "Test_loss:81.75447,Test_accuracy:15615.62500\n",
            "Test_loss:82.16501,Test_accuracy:15703.12500\n",
            "Test_loss:82.51879,Test_accuracy:15793.75000\n",
            "Test_loss:82.96378,Test_accuracy:15865.62500\n",
            "Test_loss:83.52109,Test_accuracy:15950.00000\n",
            "Test_loss:84.04495,Test_accuracy:16028.12500\n",
            "Test_loss:84.38808,Test_accuracy:16118.75000\n",
            "Test_loss:84.95161,Test_accuracy:16200.00000\n",
            "Test_loss:85.27809,Test_accuracy:16287.50000\n",
            "Test_loss:85.84062,Test_accuracy:16362.50000\n",
            "Test_loss:86.12594,Test_accuracy:16453.12500\n",
            "Test_loss:86.30895,Test_accuracy:16550.00000\n",
            "Test_loss:86.72134,Test_accuracy:16631.25000\n",
            "Test_loss:87.12651,Test_accuracy:16718.75000\n",
            "Test_loss:87.66968,Test_accuracy:16806.25000\n",
            "Test_loss:88.07314,Test_accuracy:16896.87500\n",
            "Test_loss:88.34750,Test_accuracy:16987.50000\n",
            "Test_loss:88.83868,Test_accuracy:17068.75000\n",
            "Test_loss:89.38665,Test_accuracy:17150.00000\n",
            "Test_loss:89.96088,Test_accuracy:17228.12500\n",
            "Test_loss:90.35349,Test_accuracy:17312.50000\n",
            "Test_loss:90.98428,Test_accuracy:17393.75000\n",
            "Test_loss:91.80471,Test_accuracy:17471.87500\n",
            "Test_loss:92.29855,Test_accuracy:17553.12500\n",
            "Test_loss:92.94506,Test_accuracy:17621.87500\n",
            "Test_loss:93.42089,Test_accuracy:17709.37500\n",
            "Test_loss:94.13414,Test_accuracy:17787.50000\n",
            "Test_loss:94.45794,Test_accuracy:17878.12500\n",
            "Test_loss:94.71633,Test_accuracy:17968.75000\n",
            "Test_loss:95.03651,Test_accuracy:18053.12500\n",
            "Test_loss:95.54459,Test_accuracy:18137.50000\n",
            "Test_loss:96.04782,Test_accuracy:18221.87500\n",
            "Test_loss:96.27337,Test_accuracy:18312.50000\n",
            "Test_loss:96.79555,Test_accuracy:18390.62500\n",
            "Test_loss:97.00412,Test_accuracy:18481.25000\n",
            "Test_loss:97.52995,Test_accuracy:18559.37500\n",
            "Test_loss:97.87983,Test_accuracy:18637.50000\n",
            "Test_loss:98.29035,Test_accuracy:18715.62500\n",
            "Test_loss:98.63504,Test_accuracy:18803.12500\n",
            "Test_loss:99.28101,Test_accuracy:18881.25000\n",
            "Test_loss:99.55341,Test_accuracy:18968.75000\n",
            "Test_loss:100.16441,Test_accuracy:19046.87500\n",
            "Test_loss:100.62141,Test_accuracy:19134.37500\n",
            "Test_loss:101.15350,Test_accuracy:19221.87500\n",
            "Test_loss:101.44218,Test_accuracy:19309.37500\n",
            "Test_loss:101.73550,Test_accuracy:19400.00000\n",
            "Test_loss:102.29793,Test_accuracy:19478.12500\n",
            "Test_loss:102.59705,Test_accuracy:19571.87500\n",
            "Test_loss:102.85159,Test_accuracy:19665.62500\n",
            "Test_loss:103.28431,Test_accuracy:19743.75000\n",
            "Test_loss:103.39503,Test_accuracy:19840.62500\n",
            "Test_loss:103.64653,Test_accuracy:19937.50000\n",
            "Test_loss:104.03610,Test_accuracy:20028.12500\n",
            "Test_loss:104.34610,Test_accuracy:20118.75000\n",
            "Test_loss:104.76141,Test_accuracy:20206.25000\n",
            "Test_loss:105.21530,Test_accuracy:20293.75000\n",
            "Test_loss:105.51437,Test_accuracy:20375.00000\n",
            "Test_loss:105.78880,Test_accuracy:20456.25000\n",
            "Test_loss:106.13420,Test_accuracy:20537.50000\n",
            "Test_loss:106.29982,Test_accuracy:20634.37500\n",
            "Test_loss:106.61552,Test_accuracy:20721.87500\n",
            "Test_loss:107.14129,Test_accuracy:20800.00000\n",
            "Test_loss:107.49780,Test_accuracy:20881.25000\n",
            "Test_loss:108.12047,Test_accuracy:20965.62500\n",
            "Test_loss:108.39844,Test_accuracy:21050.00000\n",
            "Test_loss:108.97972,Test_accuracy:21128.12500\n",
            "Test_loss:109.55113,Test_accuracy:21209.37500\n",
            "Test_loss:110.02856,Test_accuracy:21284.37500\n",
            "Test_loss:110.48471,Test_accuracy:21368.75000\n",
            "Test_loss:110.97070,Test_accuracy:21450.00000\n",
            "Test_loss:111.23003,Test_accuracy:21534.37500\n",
            "Test_loss:112.22516,Test_accuracy:21609.37500\n",
            "Test_loss:112.47233,Test_accuracy:21700.00000\n",
            "Test_loss:112.94982,Test_accuracy:21781.25000\n",
            "Test_loss:113.22192,Test_accuracy:21871.87500\n",
            "Test_loss:113.76454,Test_accuracy:21956.25000\n",
            "Test_loss:114.26521,Test_accuracy:22037.50000\n",
            "Test_loss:114.61391,Test_accuracy:22128.12500\n",
            "Test_loss:114.93810,Test_accuracy:22215.62500\n",
            "Test_loss:115.16442,Test_accuracy:22309.37500\n",
            "Test_loss:115.49406,Test_accuracy:22393.75000\n",
            "Test_loss:115.95913,Test_accuracy:22478.12500\n",
            "Test_loss:116.11520,Test_accuracy:22571.87500\n",
            "Test_loss:116.59692,Test_accuracy:22646.87500\n",
            "Test_loss:116.99343,Test_accuracy:22734.37500\n",
            "Test_loss:117.56298,Test_accuracy:22818.75000\n",
            "Test_loss:118.09200,Test_accuracy:22900.00000\n",
            "Test_loss:118.58985,Test_accuracy:22984.37500\n",
            "Test_loss:118.76450,Test_accuracy:23078.12500\n",
            "Test_loss:119.41978,Test_accuracy:23146.87500\n",
            "Test_loss:120.30127,Test_accuracy:23221.87500\n",
            "Test_loss:120.81267,Test_accuracy:23306.25000\n",
            "Test_loss:121.13025,Test_accuracy:23390.62500\n",
            "Test_loss:121.43522,Test_accuracy:23478.12500\n",
            "Test_loss:121.87094,Test_accuracy:23565.62500\n",
            "Test_loss:122.38496,Test_accuracy:23650.00000\n",
            "Test_loss:122.99110,Test_accuracy:23725.00000\n",
            "Test_loss:123.23029,Test_accuracy:23815.62500\n",
            "Test_loss:123.66044,Test_accuracy:23903.12500\n",
            "Test_loss:124.02375,Test_accuracy:23993.75000\n",
            "Test_loss:124.57648,Test_accuracy:24071.87500\n",
            "Test_loss:124.95080,Test_accuracy:24159.37500\n",
            "Test_loss:125.51516,Test_accuracy:24237.50000\n",
            "Test_loss:125.99906,Test_accuracy:24315.62500\n",
            "Test_loss:126.49220,Test_accuracy:24393.75000\n",
            "Test_loss:126.84904,Test_accuracy:24478.12500\n",
            "Test_loss:127.35793,Test_accuracy:24568.75000\n",
            "Test_loss:127.80783,Test_accuracy:24653.12500\n",
            "Test_loss:128.09750,Test_accuracy:24743.75000\n",
            "Test_loss:128.76787,Test_accuracy:24815.62500\n",
            "Test_loss:128.95488,Test_accuracy:24909.37500\n",
            "Test_loss:129.07735,Test_accuracy:25006.25000\n",
            "Test_loss:129.51067,Test_accuracy:25087.50000\n",
            "Test_loss:130.32579,Test_accuracy:25162.50000\n",
            "Test_loss:130.75075,Test_accuracy:25246.87500\n",
            "Test_loss:131.18649,Test_accuracy:25334.37500\n",
            "Test_loss:131.93114,Test_accuracy:25412.50000\n",
            "Test_loss:132.26096,Test_accuracy:25500.00000\n",
            "Test_loss:132.60439,Test_accuracy:25581.25000\n",
            "Test_loss:133.11169,Test_accuracy:25665.62500\n",
            "Test_loss:133.34938,Test_accuracy:25756.25000\n",
            "Test_loss:133.63734,Test_accuracy:25853.12500\n",
            "Test_loss:133.84557,Test_accuracy:25946.87500\n",
            "Test_loss:134.13026,Test_accuracy:26040.62500\n",
            "Test_loss:134.34132,Test_accuracy:26134.37500\n",
            "Test_loss:134.88353,Test_accuracy:26215.62500\n",
            "Test_loss:135.23465,Test_accuracy:26300.00000\n",
            "Test_loss:135.84438,Test_accuracy:26387.50000\n",
            "Test_loss:136.57648,Test_accuracy:26459.37500\n",
            "Test_loss:136.93881,Test_accuracy:26546.87500\n",
            "Epoch: 9\n",
            "-------\n",
            "Train Loss: 0.45132 | Train Acc: 87.50000\n",
            "Train Loss: 0.83255 | Train Acc: 168.75000\n",
            "Train Loss: 1.23664 | Train Acc: 256.25000\n",
            "Train Loss: 1.74451 | Train Acc: 346.87500\n",
            "Train Loss: 2.01805 | Train Acc: 440.62500\n",
            "Train Loss: 3.08474 | Train Acc: 518.75000\n",
            "Train Loss: 3.57642 | Train Acc: 600.00000\n",
            "Train Loss: 4.09426 | Train Acc: 678.12500\n",
            "Train Loss: 4.48724 | Train Acc: 765.62500\n",
            "Train Loss: 4.81118 | Train Acc: 853.12500\n",
            "Train Loss: 5.13497 | Train Acc: 943.75000\n",
            "Train Loss: 5.43491 | Train Acc: 1034.37500\n",
            "Train Loss: 5.80487 | Train Acc: 1118.75000\n",
            "Train Loss: 6.14356 | Train Acc: 1209.37500\n",
            "Train Loss: 6.81346 | Train Acc: 1281.25000\n",
            "Train Loss: 7.05241 | Train Acc: 1375.00000\n",
            "Train Loss: 7.36216 | Train Acc: 1465.62500\n",
            "Train Loss: 7.73207 | Train Acc: 1550.00000\n",
            "Train Loss: 8.04339 | Train Acc: 1634.37500\n",
            "Train Loss: 8.52421 | Train Acc: 1712.50000\n",
            "Train Loss: 9.19801 | Train Acc: 1796.87500\n",
            "Train Loss: 9.86013 | Train Acc: 1862.50000\n",
            "Train Loss: 10.05181 | Train Acc: 1959.37500\n",
            "Train Loss: 10.52389 | Train Acc: 2037.50000\n",
            "Train Loss: 10.94285 | Train Acc: 2125.00000\n",
            "Train Loss: 11.24286 | Train Acc: 2212.50000\n",
            "Train Loss: 11.39601 | Train Acc: 2306.25000\n",
            "Train Loss: 11.56681 | Train Acc: 2400.00000\n",
            "Train Loss: 11.92455 | Train Acc: 2487.50000\n",
            "Train Loss: 12.17509 | Train Acc: 2587.50000\n",
            "Train Loss: 12.77612 | Train Acc: 2653.12500\n",
            "Train Loss: 13.35526 | Train Acc: 2731.25000\n",
            "Train Loss: 13.74459 | Train Acc: 2818.75000\n",
            "Train Loss: 13.86378 | Train Acc: 2915.62500\n",
            "Train Loss: 14.11636 | Train Acc: 3009.37500\n",
            "Train Loss: 14.28838 | Train Acc: 3106.25000\n",
            "Train Loss: 14.90941 | Train Acc: 3184.37500\n",
            "Train Loss: 15.31424 | Train Acc: 3281.25000\n",
            "Train Loss: 15.53220 | Train Acc: 3375.00000\n",
            "Train Loss: 16.03333 | Train Acc: 3456.25000\n",
            "Train Loss: 16.34825 | Train Acc: 3543.75000\n",
            "Train Loss: 16.68740 | Train Acc: 3634.37500\n",
            "Train Loss: 16.99014 | Train Acc: 3721.87500\n",
            "Train Loss: 17.36511 | Train Acc: 3809.37500\n",
            "Train Loss: 17.82761 | Train Acc: 3893.75000\n",
            "Train Loss: 17.94506 | Train Acc: 3990.62500\n",
            "Train Loss: 18.20562 | Train Acc: 4081.25000\n",
            "Train Loss: 18.55925 | Train Acc: 4168.75000\n",
            "Train Loss: 18.83036 | Train Acc: 4265.62500\n",
            "Train Loss: 19.20808 | Train Acc: 4350.00000\n",
            "Train Loss: 19.55791 | Train Acc: 4437.50000\n",
            "Train Loss: 20.02571 | Train Acc: 4515.62500\n",
            "Train Loss: 20.19548 | Train Acc: 4606.25000\n",
            "Train Loss: 20.47612 | Train Acc: 4696.87500\n",
            "Train Loss: 20.69737 | Train Acc: 4790.62500\n",
            "Train Loss: 20.96960 | Train Acc: 4881.25000\n",
            "Train Loss: 21.39476 | Train Acc: 4965.62500\n",
            "Train Loss: 21.85813 | Train Acc: 5043.75000\n",
            "Train Loss: 22.61961 | Train Acc: 5121.87500\n",
            "Train Loss: 22.96720 | Train Acc: 5209.37500\n",
            "Train Loss: 23.31903 | Train Acc: 5296.87500\n",
            "Train Loss: 23.62375 | Train Acc: 5387.50000\n",
            "Train Loss: 23.95748 | Train Acc: 5471.87500\n",
            "Train Loss: 24.47490 | Train Acc: 5559.37500\n",
            "Train Loss: 24.67089 | Train Acc: 5650.00000\n",
            "Train Loss: 25.08726 | Train Acc: 5731.25000\n",
            "Train Loss: 25.57441 | Train Acc: 5815.62500\n",
            "Train Loss: 25.85824 | Train Acc: 5900.00000\n",
            "Train Loss: 26.33381 | Train Acc: 5984.37500\n",
            "Train Loss: 26.74310 | Train Acc: 6075.00000\n",
            "Train Loss: 27.33728 | Train Acc: 6153.12500\n",
            "Train Loss: 27.79008 | Train Acc: 6234.37500\n",
            "Train Loss: 28.08269 | Train Acc: 6318.75000\n",
            "Train Loss: 28.27276 | Train Acc: 6409.37500\n",
            "Train Loss: 28.68754 | Train Acc: 6496.87500\n",
            "Train Loss: 28.85154 | Train Acc: 6593.75000\n",
            "Train Loss: 29.14868 | Train Acc: 6678.12500\n",
            "Train Loss: 29.47590 | Train Acc: 6768.75000\n",
            "Train Loss: 29.78384 | Train Acc: 6856.25000\n",
            "Train Loss: 30.23692 | Train Acc: 6940.62500\n",
            "Train Loss: 30.86068 | Train Acc: 7015.62500\n",
            "Train Loss: 31.12314 | Train Acc: 7103.12500\n",
            "Train Loss: 31.34897 | Train Acc: 7190.62500\n",
            "Train Loss: 31.77247 | Train Acc: 7278.12500\n",
            "Train Loss: 32.06209 | Train Acc: 7368.75000\n",
            "Train Loss: 32.38209 | Train Acc: 7456.25000\n",
            "Train Loss: 32.85435 | Train Acc: 7528.12500\n",
            "Train Loss: 33.57960 | Train Acc: 7600.00000\n",
            "Train Loss: 33.84465 | Train Acc: 7690.62500\n",
            "Train Loss: 34.22592 | Train Acc: 7771.87500\n",
            "Train Loss: 34.68715 | Train Acc: 7856.25000\n",
            "Train Loss: 35.19793 | Train Acc: 7934.37500\n",
            "Train Loss: 35.53784 | Train Acc: 8021.87500\n",
            "Train Loss: 36.12951 | Train Acc: 8109.37500\n",
            "Train Loss: 36.44906 | Train Acc: 8200.00000\n",
            "Train Loss: 36.72017 | Train Acc: 8290.62500\n",
            "Train Loss: 37.02137 | Train Acc: 8381.25000\n",
            "Train Loss: 37.25575 | Train Acc: 8471.87500\n",
            "Train Loss: 37.96224 | Train Acc: 8550.00000\n",
            "Train Loss: 38.28437 | Train Acc: 8637.50000\n",
            "Train Loss: 38.64134 | Train Acc: 8725.00000\n",
            "Train Loss: 39.16309 | Train Acc: 8800.00000\n",
            "Train Loss: 39.76998 | Train Acc: 8878.12500\n",
            "Train Loss: 40.15396 | Train Acc: 8959.37500\n",
            "Train Loss: 40.55758 | Train Acc: 9043.75000\n",
            "Train Loss: 40.79073 | Train Acc: 9134.37500\n",
            "Train Loss: 41.20869 | Train Acc: 9221.87500\n",
            "Train Loss: 42.07140 | Train Acc: 9296.87500\n",
            "Train Loss: 42.88789 | Train Acc: 9365.62500\n",
            "Train Loss: 43.18171 | Train Acc: 9456.25000\n",
            "Train Loss: 43.43805 | Train Acc: 9543.75000\n",
            "Train Loss: 43.89381 | Train Acc: 9625.00000\n",
            "Train Loss: 44.06659 | Train Acc: 9718.75000\n",
            "Train Loss: 44.46052 | Train Acc: 9809.37500\n",
            "Train Loss: 44.87781 | Train Acc: 9896.87500\n",
            "Train Loss: 45.51576 | Train Acc: 9978.12500\n",
            "Train Loss: 46.24025 | Train Acc: 10050.00000\n",
            "Train Loss: 46.64055 | Train Acc: 10134.37500\n",
            "Train Loss: 47.04751 | Train Acc: 10218.75000\n",
            "Train Loss: 47.30898 | Train Acc: 10309.37500\n",
            "Train Loss: 47.44812 | Train Acc: 10406.25000\n",
            "Train Loss: 47.80747 | Train Acc: 10493.75000\n",
            "Train Loss: 48.23610 | Train Acc: 10581.25000\n",
            "Train Loss: 48.67982 | Train Acc: 10665.62500\n",
            "Train Loss: 49.02849 | Train Acc: 10753.12500\n",
            "Train Loss: 49.47509 | Train Acc: 10837.50000\n",
            "Train Loss: 49.73360 | Train Acc: 10925.00000\n",
            "Train Loss: 50.05508 | Train Acc: 11018.75000\n",
            "Train Loss: 50.27218 | Train Acc: 11112.50000\n",
            "Train Loss: 50.48025 | Train Acc: 11203.12500\n",
            "Train Loss: 50.69695 | Train Acc: 11296.87500\n",
            "Train Loss: 50.96990 | Train Acc: 11387.50000\n",
            "Train Loss: 51.16311 | Train Acc: 11481.25000\n",
            "Train Loss: 51.58149 | Train Acc: 11571.87500\n",
            "Train Loss: 51.86241 | Train Acc: 11659.37500\n",
            "Train Loss: 52.21177 | Train Acc: 11740.62500\n",
            "Train Loss: 52.41703 | Train Acc: 11834.37500\n",
            "Train Loss: 52.64462 | Train Acc: 11931.25000\n",
            "Train Loss: 52.91207 | Train Acc: 12025.00000\n",
            "Train Loss: 53.46944 | Train Acc: 12109.37500\n",
            "Train Loss: 54.05641 | Train Acc: 12190.62500\n",
            "Train Loss: 54.46476 | Train Acc: 12281.25000\n",
            "Train Loss: 54.88021 | Train Acc: 12368.75000\n",
            "Train Loss: 55.19363 | Train Acc: 12453.12500\n",
            "Train Loss: 55.77820 | Train Acc: 12534.37500\n",
            "Train Loss: 56.08924 | Train Acc: 12618.75000\n",
            "Train Loss: 56.36038 | Train Acc: 12709.37500\n",
            "Train Loss: 57.01198 | Train Acc: 12781.25000\n",
            "Train Loss: 57.16371 | Train Acc: 12881.25000\n",
            "Train Loss: 57.35995 | Train Acc: 12971.87500\n",
            "Train Loss: 57.90823 | Train Acc: 13062.50000\n",
            "Train Loss: 58.12556 | Train Acc: 13156.25000\n",
            "Train Loss: 58.70658 | Train Acc: 13243.75000\n",
            "Train Loss: 59.00785 | Train Acc: 13334.37500\n",
            "Train Loss: 59.22913 | Train Acc: 13418.75000\n",
            "Train Loss: 59.61730 | Train Acc: 13503.12500\n",
            "Train Loss: 59.83861 | Train Acc: 13600.00000\n",
            "Train Loss: 60.08433 | Train Acc: 13690.62500\n",
            "Train Loss: 60.35360 | Train Acc: 13781.25000\n",
            "Train Loss: 60.97359 | Train Acc: 13856.25000\n",
            "Train Loss: 61.22096 | Train Acc: 13950.00000\n",
            "Train Loss: 61.67719 | Train Acc: 14034.37500\n",
            "Train Loss: 61.94584 | Train Acc: 14121.87500\n",
            "Train Loss: 62.27710 | Train Acc: 14212.50000\n",
            "Train Loss: 63.00878 | Train Acc: 14300.00000\n",
            "Train Loss: 63.17448 | Train Acc: 14396.87500\n",
            "Train Loss: 63.58041 | Train Acc: 14484.37500\n",
            "Train Loss: 64.06068 | Train Acc: 14565.62500\n",
            "Train Loss: 64.42253 | Train Acc: 14659.37500\n",
            "Train Loss: 65.02884 | Train Acc: 14743.75000\n",
            "Train Loss: 65.39036 | Train Acc: 14825.00000\n",
            "Train Loss: 65.67077 | Train Acc: 14912.50000\n",
            "Train Loss: 65.99867 | Train Acc: 15000.00000\n",
            "Train Loss: 66.42494 | Train Acc: 15081.25000\n",
            "Train Loss: 66.62118 | Train Acc: 15168.75000\n",
            "Train Loss: 66.82569 | Train Acc: 15262.50000\n",
            "Train Loss: 67.28873 | Train Acc: 15353.12500\n",
            "Train Loss: 67.58857 | Train Acc: 15446.87500\n",
            "Train Loss: 67.90655 | Train Acc: 15537.50000\n",
            "Train Loss: 68.32581 | Train Acc: 15621.87500\n",
            "Train Loss: 68.64053 | Train Acc: 15712.50000\n",
            "Train Loss: 68.84367 | Train Acc: 15806.25000\n",
            "Train Loss: 69.33408 | Train Acc: 15887.50000\n",
            "Train Loss: 69.62812 | Train Acc: 15975.00000\n",
            "Train Loss: 69.74033 | Train Acc: 16075.00000\n",
            "Train Loss: 69.94714 | Train Acc: 16162.50000\n",
            "Train Loss: 70.32927 | Train Acc: 16243.75000\n",
            "Train Loss: 70.61812 | Train Acc: 16328.12500\n",
            "Train Loss: 71.11264 | Train Acc: 16409.37500\n",
            "Train Loss: 71.56390 | Train Acc: 16493.75000\n",
            "Train Loss: 71.97420 | Train Acc: 16578.12500\n",
            "Train Loss: 72.22294 | Train Acc: 16668.75000\n",
            "Train Loss: 72.45022 | Train Acc: 16756.25000\n",
            "Train Loss: 72.63828 | Train Acc: 16850.00000\n",
            "Train Loss: 72.79291 | Train Acc: 16943.75000\n",
            "Train Loss: 73.23068 | Train Acc: 17015.62500\n",
            "Train Loss: 73.39253 | Train Acc: 17115.62500\n",
            "Train Loss: 73.83825 | Train Acc: 17196.87500\n",
            "Train Loss: 74.19252 | Train Acc: 17284.37500\n",
            "Train Loss: 74.39394 | Train Acc: 17375.00000\n",
            "Train Loss: 74.85890 | Train Acc: 17450.00000\n",
            "Train Loss: 75.22462 | Train Acc: 17534.37500\n",
            "Train Loss: 75.43961 | Train Acc: 17625.00000\n",
            "Train Loss: 75.71732 | Train Acc: 17718.75000\n",
            "Train Loss: 75.98674 | Train Acc: 17812.50000\n",
            "Train Loss: 76.11325 | Train Acc: 17909.37500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model:torch.nn.Module,\n",
        "                data_loader: torch.utils.data.DataLoader,\n",
        "                loss_fn: torch.nn.Module,\n",
        "                optimizer: torch.optim.Optimizer,\n",
        "                accuracy_fn,\n",
        "                device: torch.device = device):\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    # Put data on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # 2. Calculate the loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # 6. Train Accuracy\n",
        "    train_acc += accuracy_fn(y_true=y,\n",
        "                             y_pred=y_pred.argmax(dim=1))\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    if batch % 400 == 0:\n",
        "      print(f'Looked at {batch * len(X)}/{len(data_loader.dataset)} samples')\n",
        "      print(f'Train Loss: {train_loss:.5f}')\n",
        "      print(f'Train Acc: {train_acc:.5f}'\n",
        "\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "\n",
        "  return {'train_loss': train_loss,\n",
        "          'train_acc': train_acc}\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "  test_loss, test_acc = 0,0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      test_pred = model(X)\n",
        "      test_loss += loss_fn(test_pred, y)\n",
        "      test_acc += accuracy_fn(y_true=y,\n",
        "                              y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "      if batch % 400 == 0:\n",
        "        print(f'Looked at {batch * len(X)}/{len(data_loader.dataset)} samples')\n",
        "        print(f'Test Loss: {test_loss:.5f}')\n",
        "        print(f'Test Acc: {test_acc:.5f}'\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "\n",
        "    return {'test_loss': test_loss.item(),\n",
        "          'test_acc': test_acc}\n",
        ""
      ],
      "metadata": {
        "id": "_qoODgda_cY1"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}